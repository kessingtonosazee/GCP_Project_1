{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kessingtonosazee/GCP_Project_1/blob/master/Copy_of_aml_2324_w3_webinar_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to SHAP"
      ],
      "metadata": {
        "id": "_unsgc3dmo3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q/A"
      ],
      "metadata": {
        "id": "7EMo0r4Yp9ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I guess in the classification example, the base value was calculated from the bar graph of target feature. How can we calcuate the base value in a regression problem?\n",
        "\n",
        "    - In most cases, the base case is determined by the distribution of the target. If classification, it could be default probability of a class (e.g., diabetes=1); if regression, the mean value (e.g., the mean house price)."
      ],
      "metadata": {
        "id": "Hq8G-jzmp-zM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In a Data Science project, in which phase SHAP is normally calculated and why? In what way(s) it practically helps us in a project afer knowing which features have strong +ive or -ive relations?\n",
        "\n",
        "    - At any point that you are trying to make sense of how a model behaves, its predictive pattern, and so on; it could be in a first iteration, after a simple model has been built; at later stages, informing further feature selection.\n"
      ],
      "metadata": {
        "id": "tjIbuphtqD_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If I delete features which negatively affects the accuracy of a prediction, then will it increase the overall accuracy rates?\n",
        "\n",
        "    - firstly, we need to clarify that SHAP is about the effect on the target, and whether feature values increase or decrease the predictions and by how much (if classification, usually, impact on probability of class of interest; if regression, for example, reduction or increase of house prices).\n",
        "\n",
        "    - if we are talking permutation importance, then, yes, that reflects the drop in the model's score (e.g., $R^2$, MAE).\n",
        "\n",
        "    - No guarantee that removing a feature will increase performance as models behave in intricate ways usually; there might be interactions, for example; but I would experiment with it, guided by SHAP/PI values.\n"
      ],
      "metadata": {
        "id": "xQRgqDsnqGTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* In shap.plots.beeswarm plot, I observed that for some features there are clusters of SHAP points and for other features, I noticed that SHAP values are scattered. Does their different behaviour means anything if we talk about relation of features with predictions?\n",
        "\n",
        "    - The distribution and shape of SHAP values (e.g., spread, centrality, \"clumpiness\") is a reflection of the underlying model. One can use that for further exploratory model and data analysis."
      ],
      "metadata": {
        "id": "NUIzfJL4qJEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* so to clarify, the positive and negative SHAP values have an effect on the predicted values whereas the magnitude of these values indicate their importance?\n",
        "\n",
        "    - positive/negative: in which direction your target is affected/nudged/...; magnitude of effect can be seen a proxy for importance. Specially, the average mean absolute SHAP value."
      ],
      "metadata": {
        "id": "IhkvbsJfqBYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* So,  for example, would a feature with a SHAP value of -3 be more relevant than a feature with a -1 SHAP value?\n",
        "\n",
        "    - I am assuming we are talking about average values. I think we can say more \"important\" as understood in machine learning (associations). It does not necessarily mean, for example, if an intervention (causation) focusing on that feature will give us the best treatment results, for example."
      ],
      "metadata": {
        "id": "qFpBJJVAvmhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* My question is how we evaluate shap values when dealing with encoded categorical data ,especially when we encode each value as a different integer.What about One hot encoder technique?\n",
        "\n",
        "    - There are ways of \"hiding\" the encoding from SHAP, and then obtain the effects on the raw features. This could be much more computationally intensive...\n",
        "\n",
        "    - Otherwise, we have to observe the features in the new data space, and talk about \"port_S\", \"port_C\", instead of \"port_of_embarkment\".\n",
        "\n",
        "    - \"Sum the SHAP values of \"Color_Red\", \"Color_Green\", and \"Color_Blue\" for each instance to get the total impact of the \"Color\" feature.\" (from ChatGPT)\n",
        "\n",
        "        + From the author: https://github.com/shap/shap/issues/397#issuecomment-455728636"
      ],
      "metadata": {
        "id": "nXY3TuX7wVnU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FRtMTy5w5KU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}