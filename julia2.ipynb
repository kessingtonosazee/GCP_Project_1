{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUR2+KWdULbW4KrMyNvbtP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kessingtonosazee/GCP_Project_1/blob/master/julia2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgdW50taHzQl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This document is an automatically-generated file that contains all typeset code blocks from\n",
        "Algorithms for Decision Making by Mykel Kochenderfer, Tim Wheeler, and Kyle Wray. This book\n",
        "is available from the MIT Press. A PDF version is also available online at algorithmsbook.com.\n",
        "\n",
        "We share this content in the hopes that it helps you and makes the decision making algorithms\n",
        "more approachable and accessible. Thank you for reading!\n",
        "\n",
        "If you encounter any issues or have pressing comments, please file an issue at\n",
        "github.com/algorithmsbooks/decisionmaking\n",
        "\"\"\"\n",
        "\n",
        "#################### representation 1\n",
        "struct Variable\n",
        "\tname::Symbol\n",
        "\tr::Int # number of possible values\n",
        "end\n",
        "\n",
        "const Assignment = Dict{Symbol,Int}\n",
        "const FactorTable = Dict{Assignment,Float64}\n",
        "\n",
        "struct Factor\n",
        "\tvars::Vector{Variable}\n",
        "\ttable::FactorTable\n",
        "end\n",
        "\n",
        "variablenames(œï::Factor) = [var.name for var in œï.vars]\n",
        "\n",
        "select(a::Assignment, varnames::Vector{Symbol}) =\n",
        "\tAssignment(n=>a[n] for n in varnames)\n",
        "\n",
        "function assignments(vars::AbstractVector{Variable})\n",
        "    names = [var.name for var in vars]\n",
        "    return vec([Assignment(n=>v for (n,v) in zip(names, values))\n",
        "    \t\t\tfor values in product((1:v.r for v in vars)...)])\n",
        "end\n",
        "\n",
        "function normalize!(œï::Factor)\n",
        "\tz = sum(p for (a,p) in œï.table)\n",
        "\tfor (a,p) in œï.table\n",
        "\t\tœï.table[a] = p/z\n",
        "\tend\n",
        "\treturn œï\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### representation 2\n",
        "Dict{K,V}(a::NamedTuple) where K where V =\n",
        "    Dict{K,V}(n=>v for (n,v) in zip(keys(a), values(a)))\n",
        "Base.convert(::Type{Dict{K,V}}, a::NamedTuple) where K where V = Dict{K,V}(a)\n",
        "Base.isequal(a::Dict{<:Any,<:Any}, nt::NamedTuple) =\n",
        "    length(a) == length(nt) && all(a[n] == v for (n,v) in zip(keys(nt), values(nt)))\n",
        "####################\n",
        "\n",
        "#################### representation 3\n",
        "struct BayesianNetwork\n",
        "\tvars::Vector{Variable}\n",
        "    factors::Vector{Factor}\n",
        "\tgraph::SimpleDiGraph{Int64}\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### representation 4\n",
        "function probability(bn::BayesianNetwork, assignment)\n",
        "    subassignment(œï) = select(assignment, variablenames(œï))\n",
        "    probability(œï) = get(œï.table, subassignment(œï), 0.0)\n",
        "    return prod(probability(œï) for œï in bn.factors)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 1\n",
        "function Base.:*(œï::Factor, œà::Factor)\n",
        "    œïnames = variablenames(œï)\n",
        "    œànames = variablenames(œà)\n",
        "    œàonly = setdiff(œà.vars, œï.vars)\n",
        "    table = FactorTable()\n",
        "    for (œïa,œïp) in œï.table\n",
        "        for a in assignments(œàonly)\n",
        "            a = merge(œïa, a)\n",
        "            œàa = select(a, œànames)\n",
        "            table[a] = œïp * get(œà.table, œàa, 0.0)\n",
        "        end\n",
        "    end\n",
        "    vars = vcat(œï.vars, œàonly)\n",
        "    return Factor(vars, table)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 2\n",
        "function marginalize(œï::Factor, name)\n",
        "\ttable = FactorTable()\n",
        "\tfor (a, p) in œï.table\n",
        "\t\ta‚Ä≤ = delete!(copy(a), name)\n",
        "\t\ttable[a‚Ä≤] = get(table, a‚Ä≤, 0.0) + p\n",
        "\tend\n",
        "\tvars = filter(v -> v.name != name, œï.vars)\n",
        "\treturn Factor(vars, table)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 3\n",
        "in_scope(name, œï) = any(name == v.name for v in œï.vars)\n",
        "\n",
        "function condition(œï::Factor, name, value)\n",
        "\tif !in_scope(name, œï)\n",
        "\t\treturn œï\n",
        "\tend\n",
        "\ttable = FactorTable()\n",
        "\tfor (a, p) in œï.table\n",
        "\t\tif a[name] == value\n",
        "\t\t\ttable[delete!(copy(a), name)] = p\n",
        "\t\tend\n",
        "\tend\n",
        "\tvars = filter(v -> v.name != name, œï.vars)\n",
        "\treturn Factor(vars, table)\n",
        "end\n",
        "\n",
        "function condition(œï::Factor, evidence)\n",
        "\tfor (name, value) in pairs(evidence)\n",
        "\t\tœï = condition(œï, name, value)\n",
        "\tend\n",
        "\treturn œï\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 4\n",
        "struct ExactInference end\n",
        "\n",
        "function infer(M::ExactInference, bn, query, evidence)\n",
        "\tœï = prod(bn.factors)\n",
        "\tœï = condition(œï, evidence)\n",
        "\tfor name in setdiff(variablenames(œï), query)\n",
        "\t\tœï = marginalize(œï, name)\n",
        "\tend\n",
        "\treturn normalize!(œï)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 5\n",
        "struct VariableElimination\n",
        "\tordering # array of variable indices\n",
        "end\n",
        "\n",
        "function infer(M::VariableElimination, bn, query, evidence)\n",
        "\tŒ¶ = [condition(œï, evidence) for œï in bn.factors]\n",
        "\tfor i in M.ordering\n",
        "\t\tname = bn.vars[i].name\n",
        "\t\tif name ‚àâ query\n",
        "\t\t\tinds = findall(œï->in_scope(name, œï), Œ¶)\n",
        "\t\t\tif !isempty(inds)\n",
        "\t\t\t\tœï = prod(Œ¶[inds])\n",
        "\t\t\t\tdeleteat!(Œ¶, inds)\n",
        "\t\t\t\tœï = marginalize(œï, name)\n",
        "\t\t\t\tpush!(Œ¶, œï)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn normalize!(prod(Œ¶))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 6\n",
        "function topological_sort(G)\n",
        "\tG = deepcopy(G)\n",
        "\tordering = []\n",
        "\tparentless = filter(i -> isempty(inneighbors(G, i)), 1:nv(G))\n",
        "\twhile !isempty(parentless)\n",
        "\t\ti = pop!(parentless)\n",
        "\t\tpush!(ordering, i)\n",
        "\t\tfor j in copy(outneighbors(G, i))\n",
        "\t\t\trem_edge!(G, i, j)\n",
        "\t\t\tif isempty(inneighbors(G, j))\n",
        "\t\t\t\tpush!(parentless, j)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn ordering\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 7\n",
        "function Base.rand(œï::Factor)\n",
        "\ttot, p, w = 0.0, rand(), sum(values(œï.table))\n",
        "\tfor (a,v) in œï.table\n",
        "\t\ttot += v/w\n",
        "\t\tif tot >= p\n",
        "\t\t\treturn a\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn Assignment()\n",
        "end\n",
        "\n",
        "function Base.rand(bn::BayesianNetwork)\n",
        "\ta = Assignment()\n",
        "\tfor i in topological_sort(bn.graph)\n",
        "\t\tname, œï = bn.vars[i].name, bn.factors[i]\n",
        "\t\ta[name] = rand(condition(œï, a))[name]\n",
        "\tend\n",
        "\treturn a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 8\n",
        "struct DirectSampling\n",
        "\tm # number of samples\n",
        "end\n",
        "\n",
        "function infer(M::DirectSampling, bn, query, evidence)\n",
        "\ttable = FactorTable()\n",
        "\tfor i in 1:(M.m)\n",
        "\t\ta = rand(bn)\n",
        "\t\tif all(a[k] == v for (k,v) in pairs(evidence))\n",
        "\t\t\tb = select(a, query)\n",
        "\t\t\ttable[b] = get(table, b, 0) + 1\n",
        "\t\tend\n",
        "\tend\n",
        "\tvars = filter(v->v.name ‚àà query, bn.vars)\n",
        "\treturn normalize!(Factor(vars, table))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 9\n",
        "struct LikelihoodWeightedSampling\n",
        "\tm # number of samples\n",
        "end\n",
        "\n",
        "function infer(M::LikelihoodWeightedSampling, bn, query, evidence)\n",
        "\ttable = FactorTable()\n",
        "\tordering = topological_sort(bn.graph)\n",
        "\tfor i in 1:(M.m)\n",
        "\t\ta, w = Assignment(), 1.0\n",
        "\t\tfor j in ordering\n",
        "\t\t\tname, œï = bn.vars[j].name, bn.factors[j]\n",
        "\t\t\tif haskey(evidence, name)\n",
        "                a[name] = evidence[name]\n",
        "\t\t\t\tw *= œï.table[select(a, variablenames(œï))]\n",
        "\t\t\telse\n",
        "\t\t\t\ta[name] = rand(condition(œï, a))[name]\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "        b = select(a, query)\n",
        "\t\ttable[b] = get(table, b, 0) + w\n",
        "\tend\n",
        "\tvars = filter(v->v.name ‚àà query, bn.vars)\n",
        "\treturn normalize!(Factor(vars, table))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 10\n",
        "function blanket(bn, a, i)\n",
        "\tname = bn.vars[i].name\n",
        "\tval = a[name]\n",
        "\ta = delete!(copy(a), name)\n",
        "\tŒ¶ = filter(œï -> in_scope(name, œï), bn.factors)\n",
        "\tœï = prod(condition(œï, a) for œï in Œ¶)\n",
        "\treturn normalize!(œï)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 11\n",
        "function update_gibbs_sample!(a, bn, evidence, ordering)\n",
        "    for i in ordering\n",
        "\t\tname = bn.vars[i].name\n",
        "\t\tif !haskey(evidence, name)\n",
        "            b = blanket(bn, a, i)\n",
        "            a[name] = rand(b)[name]\n",
        "\t\tend\n",
        "\tend\n",
        "end\n",
        "\n",
        "function gibbs_sample!(a, bn, evidence, ordering, m)\n",
        "\tfor j in 1:m\n",
        "\t\tupdate_gibbs_sample!(a, bn, evidence, ordering)\n",
        "\tend\n",
        "end\n",
        "\n",
        "struct GibbsSampling\n",
        "\tm_samples # number of samples to use\n",
        "\tm_burnin  # number of samples to discard during burn-in\n",
        "\tm_skip    # number of samples to skip for thinning\n",
        "\tordering  # array of variable indices\n",
        "end\n",
        "\n",
        "function infer(M::GibbsSampling, bn, query, evidence)\n",
        "\ttable = FactorTable()\n",
        "\ta = merge(rand(bn), evidence)\n",
        "\tgibbs_sample!(a, bn, evidence, M.ordering, M.m_burnin)\n",
        "\tfor i in 1:(M.m_samples)\n",
        "\t\tgibbs_sample!(a, bn, evidence, M.ordering, M.m_skip)\n",
        "\t\tb = select(a, query)\n",
        "\t\ttable[b] = get(table, b, 0) + 1\n",
        "\tend\n",
        "\tvars = filter(v->v.name ‚àà query, bn.vars)\n",
        "\treturn normalize!(Factor(vars, table))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 12\n",
        "function infer(D::MvNormal, query, evidencevars, evidence)\n",
        "    Œº, Œ£ = D.Œº, D.Œ£.mat\n",
        "    b, Œºa, Œºb = evidence, Œº[query], Œº[evidencevars]\n",
        "    A = Œ£[query,query]\n",
        "    B = Œ£[evidencevars,evidencevars]\n",
        "    C = Œ£[query,evidencevars]\n",
        "    Œº = Œº[query] + C * (B\\(b - Œºb))\n",
        "    Œ£ = A - C * (B \\ C')\n",
        "    return MvNormal(Œº, Œ£)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### parameter-learning 1\n",
        "function sub2ind(siz, x)\n",
        "    k = vcat(1, cumprod(siz[1:end-1]))\n",
        "    return dot(k, x .- 1) + 1\n",
        "end\n",
        "\n",
        "function statistics(vars, G, D::Matrix{Int})\n",
        "    n = size(D, 1)\n",
        "    r = [vars[i].r for i in 1:n]\n",
        "    q = [prod([r[j] for j in inneighbors(G,i)]) for i in 1:n]\n",
        "    M = [zeros(q[i], r[i]) for i in 1:n]\n",
        "    for o in eachcol(D)\n",
        "        for i in 1:n\n",
        "            k = o[i]\n",
        "            parents = inneighbors(G,i)\n",
        "            j = 1\n",
        "            if !isempty(parents)\n",
        "                 j = sub2ind(r[parents], o[parents])\n",
        "            end\n",
        "            M[i][j,k] += 1.0\n",
        "        end\n",
        "    end\n",
        "    return M\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### parameter-learning 2\n",
        "function prior(vars, G)\n",
        "    n = length(vars)\n",
        "    r = [vars[i].r for i in 1:n]\n",
        "    q = [prod([r[j] for j in inneighbors(G,i)]) for i in 1:n]\n",
        "    return [ones(q[i], r[i]) for i in 1:n]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### parameter-learning 3\n",
        "gaussian_kernel(b) = x->pdf(Normal(0,b), x)\n",
        "\n",
        "function kernel_density_estimate(œï, O)\n",
        "\treturn x -> sum([œï(x - o) for o in O])/length(O)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 1\n",
        "function bayesian_score_component(M, Œ±)\n",
        "    p =  sum(loggamma.(Œ± + M))\n",
        "    p -= sum(loggamma.(Œ±))\n",
        "    p += sum(loggamma.(sum(Œ±,dims=2)))\n",
        "    p -= sum(loggamma.(sum(Œ±,dims=2) + sum(M,dims=2)))\n",
        "    return p\n",
        "end\n",
        "\n",
        "function bayesian_score(vars, G, D)\n",
        "    n = length(vars)\n",
        "    M = statistics(vars, G, D)\n",
        "    Œ± = prior(vars, G)\n",
        "    return sum(bayesian_score_component(M[i], Œ±[i]) for i in 1:n)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 2\n",
        "struct K2Search\n",
        "    ordering::Vector{Int} # variable ordering\n",
        "end\n",
        "\n",
        "function fit(method::K2Search, vars, D)\n",
        "    G = SimpleDiGraph(length(vars))\n",
        "    for (k,i) in enumerate(method.ordering[2:end])\n",
        "        y = bayesian_score(vars, G, D)\n",
        "        while true\n",
        "            y_best, j_best = -Inf, 0\n",
        "            for j in method.ordering[1:k]\n",
        "                if !has_edge(G, j, i)\n",
        "                    add_edge!(G, j, i)\n",
        "                    y‚Ä≤ = bayesian_score(vars, G, D)\n",
        "                    if y‚Ä≤ > y_best\n",
        "                        y_best, j_best = y‚Ä≤, j\n",
        "                    end\n",
        "                    rem_edge!(G, j, i)\n",
        "                end\n",
        "            end\n",
        "            if y_best > y\n",
        "                y = y_best\n",
        "                add_edge!(G, j_best, i)\n",
        "            else\n",
        "                break\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return G\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 3\n",
        "struct LocalDirectedGraphSearch\n",
        "    G     # initial graph\n",
        "    k_max # number of iterations\n",
        "end\n",
        "\n",
        "function rand_graph_neighbor(G)\n",
        "    n = nv(G)\n",
        "    i = rand(1:n)\n",
        "    j = mod1(i + rand(2:n)-1, n)\n",
        "    G‚Ä≤ = copy(G)\n",
        "    has_edge(G, i, j) ? rem_edge!(G‚Ä≤, i, j) : add_edge!(G‚Ä≤, i, j)\n",
        "    return G‚Ä≤\n",
        "end\n",
        "\n",
        "function fit(method::LocalDirectedGraphSearch, vars, D)\n",
        "    G = method.G\n",
        "    y = bayesian_score(vars, G, D)\n",
        "    for k in 1:method.k_max\n",
        "        G‚Ä≤ = rand_graph_neighbor(G)\n",
        "        y‚Ä≤ = is_cyclic(G‚Ä≤) ? -Inf : bayesian_score(vars, G‚Ä≤, D)\n",
        "        if y‚Ä≤ > y\n",
        "            y, G = y‚Ä≤, G‚Ä≤\n",
        "        end\n",
        "    end\n",
        "    return G\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 4\n",
        "function are_markov_equivalent(G, H)\n",
        "\tif nv(G) != nv(H) || ne(G) != ne(H) ||\n",
        "\t\t!all(has_edge(H, e) || has_edge(H, reverse(e))\n",
        "\t\t\t\t\t\t\t\t\t\tfor e in edges(G))\n",
        "\t\treturn false\n",
        "\tend\n",
        "\tfor (I, J) in [(G,H), (H,G)]\n",
        "\t\tfor c in 1:nv(I)\n",
        "\t\t\tparents = inneighbors(I, c)\n",
        "\t\t \tfor (a, b) in subsets(parents, 2)\n",
        "\t\t \t\tif !has_edge(I, a, b) && !has_edge(I, b, a) &&\n",
        "\t\t \t\t   !(has_edge(J, a, c) && has_edge(J, b, c))\n",
        "\t\t \t\t    return false\n",
        "\t\t \t\tend\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\n",
        "\treturn true\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 5\n",
        "is_clique(P, nodes) = all(has_edge(P, a, b) || has_edge(P, b, a)\n",
        "\t\t\t\t\t\t\t\tfor (a, b) in subsets(nodes, 2))\n",
        "\n",
        "function pdag_to_dag_node!(P, G, removed_nodes)\n",
        "\tfor i in 1:nv(P)\n",
        "\t\tif !removed_nodes[i]\n",
        "\t\t\tincoming = Set(inneighbors(P, i))\n",
        "\t\t\toutgoing = Set(outneighbors(P, i))\n",
        "\t\t\tdirected_in = setdiff(incoming, outgoing)\n",
        "\t\t\tundirected = incoming ‚à© outgoing\n",
        "\t\t\tdirected_out = setdiff(outgoing, incoming)\n",
        "\t\t\tif isempty(directed_out) && (isempty(undirected)\n",
        "\t\t\t      || is_clique(P, undirected ‚à™ directed_in))\n",
        "\t\t\t\tfor j in undirected\n",
        "\t\t\t\t\tadd_edge!(G, j, i)\n",
        "\t\t\t\tend\n",
        "\t\t\t\tfor j in incoming\n",
        "\t\t\t\t\trem_edge!(P, j, i)\n",
        "\t\t\t\t\trem_edge!(P, i, j)\n",
        "\t\t\t\tend\n",
        "\t\t\t\tremoved_nodes[i] = true\n",
        "\t\t\t\treturn true\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn false\n",
        "end\n",
        "\n",
        "function pdag_to_dag(P)\n",
        "\tG = SimpleDiGraph(nv(P))\n",
        "\tfor e in edges(P)\n",
        "\t\tif !has_edge(P, reverse(e))\n",
        "\t\t\tadd_edge!(G, e)\n",
        "\t\tend\n",
        "\tend\n",
        "\tremoved_nodes = falses(nv(P))\n",
        "\twhile !all(removed_nodes)\n",
        "\t\tif !pdag_to_dag_node!(P, G, removed_nodes)\n",
        "\t\t\terror(\"Cannot realize DAG for given PDAG\")\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn G\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### simple-decisions 1\n",
        "struct SimpleProblem\n",
        "    bn::BayesianNetwork\n",
        "    chance_vars::Vector{Variable}\n",
        "    decision_vars::Vector{Variable}\n",
        "    utility_vars::Vector{Variable}\n",
        "    utilities::Dict{Symbol, Vector{Float64}}\n",
        "end\n",
        "\n",
        "function solve(ùí´::SimpleProblem, evidence, M)\n",
        "    query = [var.name for var in ùí´.utility_vars]\n",
        "    U(a) = sum(ùí´.utilities[uname][a[uname]] for uname in query)\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for assignment in assignments(ùí´.decision_vars)\n",
        "        evidence = merge(evidence, assignment)\n",
        "        œï = infer(M, ùí´.bn, query, evidence)\n",
        "        u = sum(p*U(a) for (a, p) in œï.table)\n",
        "        if u > best.u\n",
        "            best = (a=assignment, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### simple-decisions 2\n",
        "function value_of_information(ùí´, query, evidence, M)\n",
        "    œï = infer(M, ùí´.bn, query, evidence)\n",
        "    voi = -solve(ùí´, evidence, M).u\n",
        "    query_vars = filter(v->v.name ‚àà query, ùí´.chance_vars)\n",
        "    for o‚Ä≤ in assignments(query_vars)\n",
        "        oo‚Ä≤ = merge(evidence, o‚Ä≤)\n",
        "        p = œï.table[o‚Ä≤]\n",
        "        voi += p*solve(ùí´, oo‚Ä≤, M).u\n",
        "    end\n",
        "    return voi\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 1\n",
        "struct MDP\n",
        "    Œ≥  # discount factor\n",
        "    ùíÆ  # state space\n",
        "    ùíú  # action space\n",
        "    T  # transition function\n",
        "    R  # reward function\n",
        "    TR # sample transition and reward\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 2\n",
        "function lookahead(ùí´::MDP, U, s, a)\n",
        "    ùíÆ, T, R, Œ≥ = ùí´.ùíÆ, ùí´.T, ùí´.R, ùí´.Œ≥\n",
        "    return R(s,a) + Œ≥*sum(T(s,a,s‚Ä≤)*U(s‚Ä≤) for s‚Ä≤ in ùíÆ)\n",
        "end\n",
        "function lookahead(ùí´::MDP, U::Vector, s, a)\n",
        "    ùíÆ, T, R, Œ≥ = ùí´.ùíÆ, ùí´.T, ùí´.R, ùí´.Œ≥\n",
        "    return R(s,a) + Œ≥*sum(T(s,a,s‚Ä≤)*U[i] for (i,s‚Ä≤) in enumerate(ùíÆ))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 3\n",
        "function iterative_policy_evaluation(ùí´::MDP, œÄ, k_max)\n",
        "    ùíÆ, T, R, Œ≥ = ùí´.ùíÆ, ùí´.T, ùí´.R, ùí´.Œ≥\n",
        "    U = [0.0 for s in ùíÆ]\n",
        "    for k in 1:k_max\n",
        "        U = [lookahead(ùí´, U, s, œÄ(s)) for s in ùíÆ]\n",
        "    end\n",
        "    return U\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 4\n",
        "function policy_evaluation(ùí´::MDP, œÄ)\n",
        "\tùíÆ, R, T, Œ≥ = ùí´.ùíÆ, ùí´.R, ùí´.T, ùí´.Œ≥\n",
        "\tR‚Ä≤ = [R(s, œÄ(s)) for s in ùíÆ]\n",
        "\tT‚Ä≤ = [T(s, œÄ(s), s‚Ä≤) for s in ùíÆ, s‚Ä≤ in ùíÆ]\n",
        "\treturn (I - Œ≥*T‚Ä≤)\\R‚Ä≤\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 5\n",
        "struct ValueFunctionPolicy\n",
        "\tùí´ # problem\n",
        "\tU # utility function\n",
        "end\n",
        "\n",
        "function greedy(ùí´::MDP, U, s)\n",
        "    u, a = findmax(a->lookahead(ùí´, U, s, a), ùí´.ùíú)\n",
        "    return (a=a, u=u)\n",
        "end\n",
        "\n",
        "(œÄ::ValueFunctionPolicy)(s) = greedy(œÄ.ùí´, œÄ.U, s).a\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 6\n",
        "struct PolicyIteration\n",
        "    œÄ # initial policy\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::PolicyIteration, ùí´::MDP)\n",
        "    œÄ, ùíÆ = M.œÄ, ùí´.ùíÆ\n",
        "    for k = 1:M.k_max\n",
        "        U = policy_evaluation(ùí´, œÄ)\n",
        "        œÄ‚Ä≤ = ValueFunctionPolicy(ùí´, U)\n",
        "        if all(œÄ(s) == œÄ‚Ä≤(s) for s in ùíÆ)\n",
        "            break\n",
        "        end\n",
        "        œÄ = œÄ‚Ä≤\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 7\n",
        "function backup(ùí´::MDP, U, s)\n",
        "\treturn maximum(lookahead(ùí´, U, s, a) for a in ùí´.ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 8\n",
        "struct ValueIteration\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::ValueIteration, ùí´::MDP)\n",
        "    U = [0.0 for s in ùí´.ùíÆ]\n",
        "    for k = 1:M.k_max\n",
        "        U = [backup(ùí´, U, s) for s in ùí´.ùíÆ]\n",
        "    end\n",
        "    return ValueFunctionPolicy(ùí´, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 9\n",
        "struct GaussSeidelValueIteration\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::GaussSeidelValueIteration, ùí´::MDP)\n",
        "    U = [0.0 for s in ùí´.ùíÆ]\n",
        "    for k = 1:M.k_max\n",
        "        for (i, s) in enumerate(ùí´.ùíÆ)\n",
        "            U[i] = backup(ùí´, U, s)\n",
        "        end\n",
        "    end\n",
        "    return ValueFunctionPolicy(ùí´, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 10\n",
        "struct LinearProgramFormulation end\n",
        "\n",
        "function tensorform(ùí´::MDP)\n",
        "    ùíÆ, ùíú, R, T = ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T\n",
        "    ùíÆ‚Ä≤ = eachindex(ùíÆ)\n",
        "    ùíú‚Ä≤ = eachindex(ùíú)\n",
        "    R‚Ä≤ = [R(s,a) for s in ùíÆ, a in ùíú]\n",
        "    T‚Ä≤ = [T(s,a,s‚Ä≤) for s in ùíÆ, a in ùíú, s‚Ä≤ in ùíÆ]\n",
        "    return ùíÆ‚Ä≤, ùíú‚Ä≤, R‚Ä≤, T‚Ä≤\n",
        "end\n",
        "\n",
        "solve(ùí´::MDP) = solve(LinearProgramFormulation(), ùí´)\n",
        "\n",
        "function solve(M::LinearProgramFormulation, ùí´::MDP)\n",
        "    ùíÆ, ùíú, R, T = tensorform(ùí´)\n",
        "    model = Model(GLPK.Optimizer)\n",
        "    @variable(model, U[ùíÆ])\n",
        "    @objective(model, Min, sum(U))\n",
        "    @constraint(model, [s=ùíÆ,a=ùíú], U[s] ‚â• R[s,a] + ùí´.Œ≥*T[s,a,:]‚ãÖU)\n",
        "    optimize!(model)\n",
        "    return ValueFunctionPolicy(ùí´, value.(U))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 11\n",
        "struct LinearQuadraticProblem\n",
        "    Ts # transition matrix with respect to state\n",
        "    Ta # transition matrix with respect to action\n",
        "    Rs # reward matrix with respect to state (negative semidefinite)\n",
        "    Ra # reward matrix with respect to action (negative definite)\n",
        "    h_max # horizon\n",
        "end\n",
        "\n",
        "function solve(ùí´::LinearQuadraticProblem)\n",
        "    Ts, Ta, Rs, Ra, h_max = ùí´.Ts, ùí´.Ta, ùí´.Rs, ùí´.Ra, ùí´.h_max\n",
        "    V = zeros(size(Rs))\n",
        "    œÄs = Any[s -> zeros(size(Ta, 2))]\n",
        "    for h in 2:h_max\n",
        "        V = Ts'*(V - V*Ta*((Ta'*V*Ta + Ra) \\ Ta'*V))*Ts + Rs\n",
        "        L = -(Ta'*V*Ta + Ra) \\ Ta' * V * Ts\n",
        "        push!(œÄs, s -> L*s)\n",
        "    end\n",
        "    return œÄs\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 1\n",
        "struct ApproximateValueIteration\n",
        "    UŒ∏    # initial parameterized value function that supports fit!\n",
        "    S     # set of discrete states for performing backups\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::ApproximateValueIteration, ùí´::MDP)\n",
        "    UŒ∏, S, k_max = M.UŒ∏, M.S, M.k_max\n",
        "    for k in 1:k_max\n",
        "        U = [backup(ùí´, UŒ∏, s) for s in S]\n",
        "        fit!(UŒ∏, S, U)\n",
        "    end\n",
        "    return ValueFunctionPolicy(ùí´, UŒ∏)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 2\n",
        "mutable struct NearestNeighborValueFunction\n",
        "    k # number of neighbors\n",
        "    d # distance function d(s, s‚Ä≤)\n",
        "    S # set of discrete states\n",
        "    Œ∏ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (UŒ∏::NearestNeighborValueFunction)(s)\n",
        "    dists = [UŒ∏.d(s,s‚Ä≤) for s‚Ä≤ in UŒ∏.S]\n",
        "    ind = sortperm(dists)[1:UŒ∏.k]\n",
        "    return mean(UŒ∏.Œ∏[i] for i in ind)\n",
        "end\n",
        "\n",
        "function fit!(UŒ∏::NearestNeighborValueFunction, S, U)\n",
        "    UŒ∏.Œ∏ = U\n",
        "    return UŒ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 3\n",
        "mutable struct LocallyWeightedValueFunction\n",
        "    k # kernel function k(s, s‚Ä≤)\n",
        "    S # set of discrete states\n",
        "    Œ∏ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (UŒ∏::LocallyWeightedValueFunction)(s)\n",
        "    w = normalize([UŒ∏.k(s,s‚Ä≤) for s‚Ä≤ in UŒ∏.S], 1)\n",
        "    return UŒ∏.Œ∏ ‚ãÖ w\n",
        "end\n",
        "\n",
        "function fit!(UŒ∏::LocallyWeightedValueFunction, S, U)\n",
        "    UŒ∏.Œ∏ = U\n",
        "    return UŒ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 4\n",
        "mutable struct MultilinearValueFunction\n",
        "    o # position of lower-left corner\n",
        "    Œ¥ # vector of widths\n",
        "    Œ∏ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (UŒ∏::MultilinearValueFunction)(s)\n",
        "\to, Œ¥, Œ∏ = UŒ∏.o, UŒ∏.Œ¥, UŒ∏.Œ∏\n",
        "    Œî = (s - o)./Œ¥\n",
        "    # Multidimensional index of lower-left cell\n",
        "    i = min.(floor.(Int, Œî) .+ 1, size(Œ∏) .- 1)\n",
        "    vertex_index = similar(i)\n",
        "    d = length(s)\n",
        "    u = 0.0\n",
        "    for vertex in 0:2^d-1\n",
        "        weight = 1.0\n",
        "        for j in 1:d\n",
        "            # Check whether jth bit is set\n",
        "            if vertex & (1 << (j-1)) > 0\n",
        "                vertex_index[j] = i[j] + 1\n",
        "                weight *= Œî[j] - i[j] + 1\n",
        "            else\n",
        "                vertex_index[j] = i[j]\n",
        "                weight *= i[j] - Œî[j]\n",
        "            end\n",
        "        end\n",
        "        u += Œ∏[vertex_index...]*weight\n",
        "    end\n",
        "    return u\n",
        "end\n",
        "\n",
        "function fit!(UŒ∏::MultilinearValueFunction, S, U)\n",
        "    UŒ∏.Œ∏ = U\n",
        "    return UŒ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 5\n",
        "mutable struct SimplexValueFunction\n",
        "    o # position of lower-left corner\n",
        "    Œ¥ # vector of widths\n",
        "    Œ∏ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (UŒ∏::SimplexValueFunction)(s)\n",
        "\tŒî = (s - UŒ∏.o)./UŒ∏.Œ¥\n",
        "\t# Multidimensional index of upper-right cell\n",
        "\ti = min.(floor.(Int, Œî) .+ 1, size(UŒ∏.Œ∏) .- 1) .+ 1\n",
        "\tu = 0.0\n",
        "\ts‚Ä≤ = (s - (UŒ∏.o + UŒ∏.Œ¥.*(i.-2))) ./ UŒ∏.Œ¥\n",
        "\tp = sortperm(s‚Ä≤) # increasing order\n",
        "\tw_tot = 0.0\n",
        "\tfor j in p\n",
        "\t\tw = s‚Ä≤[j] - w_tot\n",
        "\t\tu += w*UŒ∏.Œ∏[i...]\n",
        "\t\ti[j] -= 1\n",
        "\t\tw_tot += w\n",
        "\tend\n",
        "\tu += (1 - w_tot)*UŒ∏.Œ∏[i...]\n",
        "\treturn u\n",
        "end\n",
        "\n",
        "function fit!(UŒ∏::SimplexValueFunction, S, U)\n",
        "    UŒ∏.Œ∏ = U\n",
        "    return UŒ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 6\n",
        "\tusing LinearAlgebra\n",
        "\tfunction regression(X, y, bases::Vector)\n",
        "\t    B = [b(x) for x in X, b in bases]\n",
        "\t    return pinv(B)*y\n",
        "\tend\n",
        "\tfunction regression(X, y, bases::Function)\n",
        "\t    B = Array{Float64}(undef, length(X), length(bases(X[1])))\n",
        "\t\tfor (i,x) in enumerate(X)\n",
        "\t\t\tB[i,:] = bases(x)\n",
        "\t\tend\n",
        "\t    return pinv(B)*y\n",
        "\tend\n",
        "\n",
        "\tpolynomial_bases_1d(i, k) = [x->x[i]^p for p in 0:k]\n",
        "\tfunction polynomial_bases(n, k)\n",
        "\t\tbases = [polynomial_bases_1d(i, k) for i in 1:n]\n",
        "\t\tterms = Function[]\n",
        "\t\tfor ks in product([0:k for i in 1:n]...)\n",
        "\t\t\tif sum(ks) ‚â§ k\n",
        "\t\t\t\tpush!(terms,\n",
        "\t\t\t\t\tx->prod(b[j+1](x) for (j,b) in zip(ks,bases)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\treturn terms\n",
        "\tend\n",
        "\n",
        "\tfunction sinusoidal_bases_1d(j, k, a, b)\n",
        "\t\tT = b[j] - a[j]\n",
        "\t\tbases = Function[x->1/2]\n",
        "\t\tfor i in 1:k\n",
        "\t\t\tpush!(bases, x->sin(2œÄ*i*x[j]/T))\n",
        "\t\t\tpush!(bases, x->cos(2œÄ*i*x[j]/T))\n",
        "\t\tend\n",
        "\t\treturn bases\n",
        "\tend\n",
        "\tfunction sinusoidal_bases(k, a, b)\n",
        "\t\tn = length(a)\n",
        "\t\tbases = [sinusoidal_bases_1d(i, k, a, b) for i in 1:n]\n",
        "\t\tterms = Function[]\n",
        "\t\tfor ks in product([0:2k for i in 1:n]...)\n",
        "\t\t\tpowers = [div(k+1,2) for k in ks]\n",
        "\t\t\tif sum(powers) ‚â§ k\n",
        "\t\t\t\tpush!(terms,\n",
        "\t\t\t\t\tx->prod(b[j+1](x) for (j,b) in zip(ks,bases)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\treturn terms\n",
        "\tend\n",
        "\n",
        "\tregress(Œ≤, ss, u) = regression(ss, u, Œ≤)\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 7\n",
        "mutable struct LinearRegressionValueFunction\n",
        "    Œ≤ # basis vector function\n",
        "    Œ∏ # vector of parameters\n",
        "end\n",
        "\n",
        "function (UŒ∏::LinearRegressionValueFunction)(s)\n",
        "    return UŒ∏.Œ≤(s) ‚ãÖ UŒ∏.Œ∏\n",
        "end\n",
        "\n",
        "function fit!(UŒ∏::LinearRegressionValueFunction, S, U)\n",
        "    X = hcat([UŒ∏.Œ≤(s) for s in S]...)'\n",
        "    UŒ∏.Œ∏ = pinv(X)*U\n",
        "    return UŒ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 1\n",
        "struct RolloutLookahead\n",
        "\tùí´ # problem\n",
        "\tœÄ # rollout policy\n",
        "\td # depth\n",
        "end\n",
        "\n",
        "randstep(ùí´::MDP, s, a) = ùí´.TR(s, a)\n",
        "\n",
        "function rollout(ùí´, s, œÄ, d)\n",
        "    ret = 0.0\n",
        "    for t in 1:d\n",
        "        a = œÄ(s)\n",
        "        s, r = randstep(ùí´, s, a)\n",
        "        ret += ùí´.Œ≥^(t-1) * r\n",
        "    end\n",
        "    return ret\n",
        "end\n",
        "\n",
        "function (œÄ::RolloutLookahead)(s)\n",
        "\tU(s) = rollout(œÄ.ùí´, s, œÄ.œÄ, œÄ.d)\n",
        "    return greedy(œÄ.ùí´, U, s).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 2\n",
        "struct ForwardSearch\n",
        "    ùí´ # problem\n",
        "    d # depth\n",
        "    U # value function at depth d\n",
        "end\n",
        "\n",
        "function forward_search(ùí´, s, d, U)\n",
        "    if d ‚â§ 0\n",
        "        return (a=nothing, u=U(s))\n",
        "    end\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    U‚Ä≤(s) = forward_search(ùí´, s, d-1, U).u\n",
        "    for a in ùí´.ùíú\n",
        "        u = lookahead(ùí´, U‚Ä≤, s, a)\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "(œÄ::ForwardSearch)(s) = forward_search(œÄ.ùí´, s, œÄ.d, œÄ.U).a\n",
        "####################\n",
        "\n",
        "#################### online-approximations 3\n",
        "struct BranchAndBound\n",
        "    ùí´   # problem\n",
        "    d   # depth\n",
        "    Ulo # lower bound on value function at depth d\n",
        "    Qhi # upper bound on action value function\n",
        "end\n",
        "\n",
        "function branch_and_bound(ùí´, s, d, Ulo, Qhi)\n",
        "    if d ‚â§ 0\n",
        "        return (a=nothing, u=Ulo(s))\n",
        "    end\n",
        "    U‚Ä≤(s) = branch_and_bound(ùí´, s, d-1, Ulo, Qhi).u\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for a in sort(ùí´.ùíú, by=a->Qhi(s,a), rev=true)\n",
        "        if Qhi(s, a) < best.u\n",
        "            return best # safe to prune\n",
        "        end\n",
        "        u = lookahead(ùí´, U‚Ä≤, s, a)\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "(œÄ::BranchAndBound)(s) = branch_and_bound(œÄ.ùí´, s, œÄ.d, œÄ.Ulo, œÄ.Qhi).a\n",
        "####################\n",
        "\n",
        "#################### online-approximations 4\n",
        "struct SparseSampling\n",
        "    ùí´ # problem\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    U # value function at depth d\n",
        "end\n",
        "\n",
        "function sparse_sampling(ùí´, s, d, m, U)\n",
        "    if d ‚â§ 0\n",
        "        return (a=nothing, u=U(s))\n",
        "    end\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for a in ùí´.ùíú\n",
        "        u = 0.0\n",
        "        for i in 1:m\n",
        "            s‚Ä≤, r = randstep(ùí´, s, a)\n",
        "            a‚Ä≤, u‚Ä≤ = sparse_sampling(ùí´, s‚Ä≤, d-1, m, U)\n",
        "            u += (r + ùí´.Œ≥*u‚Ä≤) / m\n",
        "        end\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "(œÄ::SparseSampling)(s) = sparse_sampling(œÄ.ùí´, s, œÄ.d, œÄ.m, œÄ.U).a\n",
        "####################\n",
        "\n",
        "#################### online-approximations 5\n",
        "struct MonteCarloTreeSearch\n",
        "\tùí´ # problem\n",
        "\tN # visit counts\n",
        "\tQ # action value estimates\n",
        "\td # depth\n",
        "\tm # number of simulations\n",
        "\tc # exploration constant\n",
        "\tU # value function estimate\n",
        "end\n",
        "\n",
        "function (œÄ::MonteCarloTreeSearch)(s)\n",
        "\tfor k in 1:œÄ.m\n",
        "\t\tsimulate!(œÄ, s)\n",
        "\tend\n",
        "\treturn argmax(a->œÄ.Q[(s,a)], œÄ.ùí´.ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 6\n",
        "function simulate!(œÄ::MonteCarloTreeSearch, s, d=œÄ.d)\n",
        "    if d ‚â§ 0\n",
        "        return œÄ.U(s)\n",
        "    end\n",
        "    ùí´, N, Q, c = œÄ.ùí´, œÄ.N, œÄ.Q, œÄ.c\n",
        "    ùíú, TR, Œ≥ = ùí´.ùíú, ùí´.TR, ùí´.Œ≥\n",
        "    if !haskey(N, (s, first(ùíú)))\n",
        "        for a in ùíú\n",
        "            N[(s,a)] = 0\n",
        "            Q[(s,a)] = 0.0\n",
        "        end\n",
        "        return œÄ.U(s)\n",
        "    end\n",
        "    a = explore(œÄ, s)\n",
        "    s‚Ä≤, r = TR(s,a)\n",
        "    q = r + Œ≥*simulate!(œÄ, s‚Ä≤, d-1)\n",
        "    N[(s,a)] += 1\n",
        "    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]\n",
        "    return q\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 7\n",
        "bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)\n",
        "\n",
        "function explore(œÄ::MonteCarloTreeSearch, s)\n",
        "    ùíú, N, Q, c = œÄ.ùí´.ùíú, œÄ.N, œÄ.Q, œÄ.c\n",
        "    Ns = sum(N[(s,a)] for a in ùíú)\n",
        "    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 8\n",
        "struct HeuristicSearch\n",
        "    ùí´   # problem\n",
        "    Uhi # upper bound on value function\n",
        "    d   # depth\n",
        "    m   # number of simulations\n",
        "end\n",
        "\n",
        "function simulate!(œÄ::HeuristicSearch, U, s)\n",
        "    ùí´ = œÄ.ùí´\n",
        "    for d in 1:œÄ.d\n",
        "        a, u = greedy(ùí´, U, s)\n",
        "        U[s] = u\n",
        "        s = rand(ùí´.T(s, a))\n",
        "    end\n",
        "end\n",
        "\n",
        "function (œÄ::HeuristicSearch)(s)\n",
        "    U = [œÄ.Uhi(s) for s in œÄ.ùí´.ùíÆ]\n",
        "    for i in 1:œÄ.m\n",
        "        simulate!(œÄ, U, s)\n",
        "    end\n",
        "    return greedy(œÄ.ùí´, U, s).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 9\n",
        "function extractpolicy(œÄ::HeuristicSearch, s)\n",
        "    U = [œÄ.Uhi(s) for s in œÄ.ùí´.ùíÆ]\n",
        "    for i in 1:œÄ.m\n",
        "        simulate!(œÄ, U, s)\n",
        "    end\n",
        "    return ValueFunctionPolicy(œÄ.ùí´, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 10\n",
        "struct LabeledHeuristicSearch\n",
        "    ùí´     # problem\n",
        "    Uhi   # upper bound on value function\n",
        "    d     # depth\n",
        "    Œ¥     # gap threshold\n",
        "end\n",
        "\n",
        "function (œÄ::LabeledHeuristicSearch)(s)\n",
        "    U, solved = [œÄ.Uhi(s) for s in ùí´.ùíÆ], Set()\n",
        "    while s ‚àâ solved\n",
        "        simulate!(œÄ, U, solved, s)\n",
        "    end\n",
        "    return greedy(œÄ.ùí´, U, s).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 11\n",
        "function simulate!(œÄ::LabeledHeuristicSearch, U, solved, s)\n",
        "    visited = []\n",
        "    for d in 1:œÄ.d\n",
        "        if s ‚àà solved\n",
        "            break\n",
        "        end\n",
        "        push!(visited, s)\n",
        "        a, u = greedy(œÄ.ùí´, U, s)\n",
        "        U[s] = u\n",
        "        s = rand(œÄ.ùí´.T(s, a))\n",
        "    end\n",
        "    while !isempty(visited)\n",
        "        if label!(œÄ, U, solved, pop!(visited))\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 12\n",
        "function expand(œÄ::LabeledHeuristicSearch, U, solved, s)\n",
        "    ùí´, Œ¥ = œÄ.ùí´, œÄ.Œ¥\n",
        "    ùíÆ, ùíú, T = ùí´.ùíÆ, ùí´.ùíú, ùí´.T\n",
        "    found, toexpand, envelope = false, Set(s), []\n",
        "    while !isempty(toexpand)\n",
        "        s = pop!(toexpand)\n",
        "        push!(envelope, s)\n",
        "        a, u = greedy(ùí´, U, s)\n",
        "        if abs(U[s] - u) > Œ¥\n",
        "            found = true\n",
        "        else\n",
        "            for s‚Ä≤ in ùíÆ\n",
        "                if T(s,a,s‚Ä≤) > 0 && s‚Ä≤ ‚àâ (solved ‚à™ envelope)\n",
        "                    push!(toexpand, s‚Ä≤)\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return (found, envelope)\n",
        "end\n",
        "\n",
        "function label!(œÄ::LabeledHeuristicSearch, U, solved, s)\n",
        "    if s ‚àà solved\n",
        "        return false\n",
        "    end\n",
        "    found, envelope = expand(œÄ, U, solved, s)\n",
        "    if found\n",
        "        for s ‚àà reverse(envelope)\n",
        "            U[s] = greedy(œÄ.ùí´, U, s).u\n",
        "        end\n",
        "    else\n",
        "        union!(solved, envelope)\n",
        "    end\n",
        "    return found\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 13\n",
        "function extractpolicy(œÄ::LabeledHeuristicSearch, s)\n",
        "    U, solved = [œÄ.Uhi(s) for s ‚àà œÄ.ùí´.ùíÆ], Set()\n",
        "    while s ‚àâ solved\n",
        "        simulate!(œÄ, U, solved, s)\n",
        "    end\n",
        "    return ValueFunctionPolicy(œÄ.ùí´, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 1\n",
        "struct MonteCarloPolicyEvaluation\n",
        "    ùí´ # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "end\n",
        "\n",
        "function (U::MonteCarloPolicyEvaluation)(œÄ)\n",
        "    R(œÄ) = rollout(U.ùí´, rand(U.b), œÄ, U.d)\n",
        "    return mean(R(œÄ) for i = 1:U.m)\n",
        "end\n",
        "\n",
        "(U::MonteCarloPolicyEvaluation)(œÄ, Œ∏) = U(s->œÄ(Œ∏, s))\n",
        "####################\n",
        "\n",
        "#################### policy-search 2\n",
        "struct HookeJeevesPolicySearch\n",
        "    Œ∏ # initial parameterization\n",
        "    Œ± # step size\n",
        "    c # step size reduction factor\n",
        "    œµ # termination step size\n",
        "end\n",
        "\n",
        "function optimize(M::HookeJeevesPolicySearch, œÄ, U)\n",
        "    Œ∏, Œ∏‚Ä≤, Œ±, c, œµ = copy(M.Œ∏), similar(M.Œ∏), M.Œ±, M.c, M.œµ\n",
        "    u, n = U(œÄ, Œ∏), length(Œ∏)\n",
        "    while Œ± > œµ\n",
        "        copyto!(Œ∏‚Ä≤, Œ∏)\n",
        "        best = (i=0, sgn=0, u=u)\n",
        "        for i in 1:n\n",
        "            for sgn in (-1,1)\n",
        "                Œ∏‚Ä≤[i] = Œ∏[i] + sgn*Œ±\n",
        "                u‚Ä≤ = U(œÄ, Œ∏‚Ä≤)\n",
        "                if u‚Ä≤ > best.u\n",
        "                    best = (i=i, sgn=sgn, u=u‚Ä≤)\n",
        "                end\n",
        "            end\n",
        "            Œ∏‚Ä≤[i] = Œ∏[i]\n",
        "        end\n",
        "        if best.i != 0\n",
        "            Œ∏[best.i] += best.sgn*Œ±\n",
        "            u = best.u\n",
        "        else\n",
        "            Œ± *= c\n",
        "        end\n",
        "    end\n",
        "    return Œ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 3\n",
        "struct GeneticPolicySearch\n",
        "    Œ∏s      # initial population\n",
        "    œÉ       # initial standard deviation\n",
        "    m_elite # number of elite samples\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function optimize(M::GeneticPolicySearch, œÄ, U)\n",
        "    Œ∏s, œÉ = M.Œ∏s, M.œÉ\n",
        "    n, m = length(first(Œ∏s)), length(Œ∏s)\n",
        "    for k in 1:M.k_max\n",
        "        us = [U(œÄ, Œ∏) for Œ∏ in Œ∏s]\n",
        "        sp = sortperm(us, rev=true)\n",
        "        Œ∏_best = Œ∏s[sp[1]]\n",
        "        rand_elite() = Œ∏s[sp[rand(1:M.m_elite)]]\n",
        "        Œ∏s = [rand_elite() + œÉ.*randn(n) for i in 1:(m-1)]\n",
        "        push!(Œ∏s, Œ∏_best)\n",
        "    end\n",
        "    return last(Œ∏s)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 4\n",
        "struct CrossEntropyPolicySearch\n",
        "    p       # initial distribution\n",
        "    m       # number of samples\n",
        "    m_elite # number of elite samples\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function optimize_dist(M::CrossEntropyPolicySearch, œÄ, U)\n",
        "    p, m, m_elite, k_max = M.p, M.m, M.m_elite, M.k_max\n",
        "    for k in 1:k_max\n",
        "        Œ∏s = rand(p, m)\n",
        "        us = [U(œÄ, Œ∏s[:,i]) for i in 1:m]\n",
        "        Œ∏_elite = Œ∏s[:,sortperm(us)[(m-m_elite+1):m]]\n",
        "        p = Distributions.fit(typeof(p), Œ∏_elite)\n",
        "    end\n",
        "    return p\n",
        "end\n",
        "\n",
        "function optimize(M, œÄ, U)\n",
        "    return Distributions.mode(optimize_dist(M, œÄ, U))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 5\n",
        "struct EvolutionStrategies\n",
        "    D       # distribution constructor\n",
        "    œà       # initial distribution parameterization\n",
        "    ‚àálogp   # log search likelihood gradient\n",
        "    m       # number of samples\n",
        "    Œ±       # step factor\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function evolution_strategy_weights(m)\n",
        "    ws = [max(0, log(m/2+1) - log(i)) for i in 1:m]\n",
        "    ws ./= sum(ws)\n",
        "    ws .-= 1/m\n",
        "    return ws\n",
        "end\n",
        "\n",
        "function optimize_dist(M::EvolutionStrategies, œÄ, U)\n",
        "    D, œà, m, ‚àálogp, Œ± = M.D, M.œà, M.m, M.‚àálogp, M.Œ±\n",
        "    ws = evolution_strategy_weights(m)\n",
        "    for k in 1:M.k_max\n",
        "        Œ∏s = rand(D(œà), m)\n",
        "        us = [U(œÄ, Œ∏s[:,i]) for i in 1:m]\n",
        "        sp = sortperm(us, rev=true)\n",
        "        ‚àá = sum(w.*‚àálogp(œà, Œ∏s[:,i]) for (w,i) in zip(ws,sp))\n",
        "        œà += Œ±.*‚àá\n",
        "    end\n",
        "    return D(œà)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 6\n",
        "struct IsotropicEvolutionStrategies\n",
        "    œà       # initial mean\n",
        "    œÉ       # initial standard deviation\n",
        "    m       # number of samples\n",
        "    Œ±       # step factor\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function optimize_dist(M::IsotropicEvolutionStrategies, œÄ, U)\n",
        "    œà, œÉ, m, Œ±, k_max = M.œà, M.œÉ, M.m, M.Œ±, M.k_max\n",
        "    n = length(œà)\n",
        "    ws = evolution_strategy_weights(2*div(m,2))\n",
        "    for k in 1:k_max\n",
        "        œµs = [randn(n) for i in 1:div(m,2)]\n",
        "        append!(œµs, -œµs) # weight mirroring\n",
        "        us = [U(œÄ, œà + œÉ.*œµ) for œµ in œµs]\n",
        "        sp = sortperm(us, rev=true)\n",
        "        ‚àá = sum(w.*œµs[i] for (w,i) in zip(ws,sp)) / œÉ\n",
        "        œà += Œ±.*‚àá\n",
        "    end\n",
        "    return MvNormal(œà, œÉ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 1\n",
        "function simulate(ùí´::MDP, s, œÄ, d)\n",
        "\tœÑ = []\n",
        "\tfor i = 1:d\n",
        "\t    a = œÄ(s)\n",
        "\t\ts‚Ä≤, r = ùí´.TR(s,a)\n",
        "\t    push!(œÑ, (s,a,r))\n",
        "\t    s = s‚Ä≤\n",
        "    end\n",
        "    return œÑ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 2\n",
        "struct FiniteDifferenceGradient\n",
        "    ùí´ # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    Œ¥ # step size\n",
        "end\n",
        "\n",
        "function gradient(M::FiniteDifferenceGradient, œÄ, Œ∏)\n",
        "    ùí´, b, d, m, Œ¥, Œ≥, n = M.ùí´, M.b, M.d, M.m, M.Œ¥, M.ùí´.Œ≥, length(Œ∏)\n",
        "    ŒîŒ∏(i) = [i == k ? Œ¥ : 0.0 for k in 1:n]\n",
        "    R(œÑ) = sum(r*Œ≥^(k-1) for (k, (s,a,r)) in enumerate(œÑ))\n",
        "    U(Œ∏) = mean(R(simulate(ùí´, rand(b), s->œÄ(Œ∏, s), d)) for i in 1:m)\n",
        "    ŒîU = [U(Œ∏ + ŒîŒ∏(i)) - U(Œ∏) for i in 1:n]\n",
        "    return ŒîU ./ Œ¥\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 3\n",
        "struct RegressionGradient\n",
        "    ùí´ # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    Œ¥ # step size\n",
        "end\n",
        "\n",
        "function gradient(M::RegressionGradient, œÄ, Œ∏)\n",
        "    ùí´, b, d, m, Œ¥, Œ≥ = M.ùí´, M.b, M.d, M.m, M.Œ¥, M.ùí´.Œ≥\n",
        "    ŒîŒò = [Œ¥.*normalize(randn(length(Œ∏)), 2) for i = 1:m]\n",
        "    R(œÑ) = sum(r*Œ≥^(k-1) for (k, (s,a,r)) in enumerate(œÑ))\n",
        "    U(Œ∏) = R(simulate(ùí´, rand(b), s->œÄ(Œ∏,s), d))\n",
        "    ŒîU = [U(Œ∏ + ŒîŒ∏) - U(Œ∏) for ŒîŒ∏ in ŒîŒò]\n",
        "    return pinv(reduce(hcat, ŒîŒò)') * ŒîU\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 4\n",
        "struct LikelihoodRatioGradient\n",
        "    ùí´ # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    ‚àálogœÄ # gradient of log likelihood\n",
        "end\n",
        "\n",
        "function gradient(M::LikelihoodRatioGradient, œÄ, Œ∏)\n",
        "    ùí´, b, d, m, ‚àálogœÄ, Œ≥ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ, M.ùí´.Œ≥\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    R(œÑ) = sum(r*Œ≥^(k-1) for (k, (s,a,r)) in enumerate(œÑ))\n",
        "    ‚àáU(œÑ) = sum(‚àálogœÄ(Œ∏, a, s) for (s,a) in œÑ)*R(œÑ)\n",
        "    return mean(‚àáU(simulate(ùí´, rand(b), œÄŒ∏, d)) for i in 1:m)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 5\n",
        "struct RewardToGoGradient\n",
        "    ùí´ # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    ‚àálogœÄ # gradient of log likelihood\n",
        "end\n",
        "\n",
        "function gradient(M::RewardToGoGradient, œÄ, Œ∏)\n",
        "    ùí´, b, d, m, ‚àálogœÄ, Œ≥ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ, M.ùí´.Œ≥\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    R(œÑ, j) = sum(r*Œ≥^(k-1) for (k,(s,a,r)) in zip(j:d, œÑ[j:end]))\n",
        "    ‚àáU(œÑ) = sum(‚àálogœÄ(Œ∏, a, s)*R(œÑ,j) for (j, (s,a,r)) in enumerate(œÑ))\n",
        "    return mean(‚àáU(simulate(ùí´, rand(b), œÄŒ∏, d)) for i in 1:m)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 6\n",
        "struct BaselineSubtractionGradient\n",
        "    ùí´ # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    ‚àálogœÄ # gradient of log likelihood\n",
        "end\n",
        "\n",
        "function gradient(M::BaselineSubtractionGradient, œÄ, Œ∏)\n",
        "    ùí´, b, d, m, ‚àálogœÄ, Œ≥ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ, M.ùí´.Œ≥\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    ‚Ñì(a, s, k) = ‚àálogœÄ(Œ∏, a, s)*Œ≥^(k-1)\n",
        "    R(œÑ, k) = sum(r*Œ≥^(j-1) for (j,(s,a,r)) in enumerate(œÑ[k:end]))\n",
        "    numer(œÑ) = sum(‚Ñì(a,s,k).^2*R(œÑ,k) for (k,(s,a,r)) in enumerate(œÑ))\n",
        "    denom(œÑ) = sum(‚Ñì(a,s,k).^2 for (k,(s,a)) in enumerate(œÑ))\n",
        "    base(œÑ) = numer(œÑ) ./ denom(œÑ)\n",
        "    trajs = [simulate(ùí´, rand(b), œÄŒ∏, d) for i in 1:m]\n",
        "    rbase = mean(base(œÑ) for œÑ in trajs)\n",
        "    ‚àáU(œÑ) = sum(‚Ñì(a,s,k).*(R(œÑ,k).-rbase) for (k,(s,a,r)) in enumerate(œÑ))\n",
        "    return mean(‚àáU(œÑ) for œÑ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 1\n",
        "struct PolicyGradientUpdate\n",
        "    ‚àáU # policy gradient estimate\n",
        "    Œ±  # step factor\n",
        "end\n",
        "\n",
        "function update(M::PolicyGradientUpdate, Œ∏)\n",
        "    return Œ∏ + M.Œ± * M.‚àáU(Œ∏)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 2\n",
        "scale_gradient(‚àá, L2_max) = min(L2_max/norm(‚àá), 1)*‚àá\n",
        "clip_gradient(‚àá, a, b) = clamp.(‚àá, a, b)\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 3\n",
        "struct RestrictedPolicyUpdate\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ‚àálogœÄ # gradient of log likelihood\n",
        "    œÄ     # policy\n",
        "    œµ     # divergence bound\n",
        "end\n",
        "\n",
        "function update(M::RestrictedPolicyUpdate, Œ∏)\n",
        "    ùí´, b, d, m, ‚àálogœÄ, œÄ, Œ≥ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ, M.œÄ, M.ùí´.Œ≥\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    R(œÑ) = sum(r*Œ≥^(k-1) for (k, (s,a,r)) in enumerate(œÑ))\n",
        "    œÑs = [simulate(ùí´, rand(b), œÄŒ∏, d) for i in 1:m]\n",
        "    ‚àálog(œÑ) = sum(‚àálogœÄ(Œ∏, a, s) for (s,a) in œÑ)\n",
        "    ‚àáU(œÑ) = ‚àálog(œÑ)*R(œÑ)\n",
        "    u = mean(‚àáU(œÑ) for œÑ in œÑs)\n",
        "    return Œ∏ + u*sqrt(2*M.œµ/dot(u,u))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 4\n",
        "struct NaturalPolicyUpdate\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ‚àálogœÄ # gradient of log likelihood\n",
        "    œÄ     # policy\n",
        "    œµ     # divergence bound\n",
        "end\n",
        "\n",
        "function natural_update(Œ∏, ‚àáf, F, œµ, œÑs)\n",
        "    ‚àáfŒ∏ = mean(‚àáf(œÑ) for œÑ in œÑs)\n",
        "    u = mean(F(œÑ) for œÑ in œÑs) \\ ‚àáfŒ∏\n",
        "    return Œ∏ + u*sqrt(2œµ/dot(‚àáfŒ∏,u))\n",
        "end\n",
        "\n",
        "function update(M::NaturalPolicyUpdate, Œ∏)\n",
        "    ùí´, b, d, m, ‚àálogœÄ, œÄ, Œ≥ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ, M.œÄ, M.ùí´.Œ≥\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    R(œÑ) = sum(r*Œ≥^(k-1) for (k, (s,a,r)) in enumerate(œÑ))\n",
        "    ‚àálog(œÑ) = sum(‚àálogœÄ(Œ∏, a, s) for (s,a) in œÑ)\n",
        "    ‚àáU(œÑ) = ‚àálog(œÑ)*R(œÑ)\n",
        "    F(œÑ) = ‚àálog(œÑ)*‚àálog(œÑ)'\n",
        "    œÑs = [simulate(ùí´, rand(b), œÄŒ∏, d) for i in 1:m]\n",
        "    return natural_update(Œ∏, ‚àáU, F, M.œµ, œÑs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 5\n",
        "struct TrustRegionUpdate\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    œÄ     # policy œÄ(s)\n",
        "    p     # policy likelihood p(Œ∏, a, s)\n",
        "    ‚àálogœÄ # log likelihood gradient\n",
        "    KL    # KL divergence KL(Œ∏, Œ∏‚Ä≤, s)\n",
        "    œµ     # divergence bound\n",
        "    Œ±     # line search reduction factor (e.g., 0.5)\n",
        "end\n",
        "\n",
        "function surrogate_objective(M::TrustRegionUpdate, Œ∏, Œ∏‚Ä≤, œÑs)\n",
        "    d, p, Œ≥ = M.d, M.p, M.ùí´.Œ≥\n",
        "    R(œÑ, j) = sum(r*Œ≥^(k-1) for (k,(s,a,r)) in zip(j:d, œÑ[j:end]))\n",
        "    w(a,s) = p(Œ∏‚Ä≤,a,s) / p(Œ∏,a,s)\n",
        "    f(œÑ) = mean(w(a,s)*R(œÑ,k) for (k,(s,a,r)) in enumerate(œÑ))\n",
        "    return mean(f(œÑ) for œÑ in œÑs)\n",
        "end\n",
        "\n",
        "function surrogate_constraint(M::TrustRegionUpdate, Œ∏, Œ∏‚Ä≤, œÑs)\n",
        "    Œ≥ = M.ùí´.Œ≥\n",
        "    KL(œÑ) = mean(M.KL(Œ∏, Œ∏‚Ä≤, s)*Œ≥^(k-1) for (k,(s,a,r)) in enumerate(œÑ))\n",
        "    return mean(KL(œÑ) for œÑ in œÑs)\n",
        "end\n",
        "\n",
        "function linesearch(M::TrustRegionUpdate, f, g, Œ∏, Œ∏‚Ä≤)\n",
        "    fŒ∏ = f(Œ∏)\n",
        "    while g(Œ∏‚Ä≤) > M.œµ || f(Œ∏‚Ä≤) ‚â§ fŒ∏\n",
        "        Œ∏‚Ä≤ = Œ∏ + M.Œ±*(Œ∏‚Ä≤ - Œ∏)\n",
        "    end\n",
        "    return Œ∏‚Ä≤\n",
        "end\n",
        "\n",
        "function update(M::TrustRegionUpdate, Œ∏)\n",
        "    ùí´, b, d, m, ‚àálogœÄ, œÄ, Œ≥ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ, M.œÄ, M.ùí´.Œ≥\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    R(œÑ) = sum(r*Œ≥^(k-1) for (k, (s,a,r)) in enumerate(œÑ))\n",
        "    ‚àálog(œÑ) = sum(‚àálogœÄ(Œ∏, a, s) for (s,a) in œÑ)\n",
        "    ‚àáU(œÑ) = ‚àálog(œÑ)*R(œÑ)\n",
        "    F(œÑ) = ‚àálog(œÑ)*‚àálog(œÑ)'\n",
        "    œÑs = [simulate(ùí´, rand(b), œÄŒ∏, d) for i in 1:m]\n",
        "    Œ∏‚Ä≤ = natural_update(Œ∏, ‚àáU, F, M.œµ, œÑs)\n",
        "    f(Œ∏‚Ä≤) = surrogate_objective(M, Œ∏, Œ∏‚Ä≤, œÑs)\n",
        "    g(Œ∏‚Ä≤) = surrogate_constraint(M, Œ∏, Œ∏‚Ä≤, œÑs)\n",
        "    return linesearch(M, f, g, Œ∏, Œ∏‚Ä≤)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 6\n",
        "struct ClampedSurrogateUpdate\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of trajectories\n",
        "    œÄ     # policy\n",
        "    p     # policy likelihood\n",
        "    ‚àáœÄ    # policy likelihood gradient\n",
        "    œµ     # divergence bound\n",
        "    Œ±     # step size\n",
        "    k_max # number of iterations per update\n",
        "end\n",
        "\n",
        "function clamped_gradient(M::ClampedSurrogateUpdate, Œ∏, Œ∏‚Ä≤, œÑs)\n",
        "    d, p, ‚àáœÄ, œµ, Œ≥ = M.d, M.p, M.‚àáœÄ, M.œµ, M.ùí´.Œ≥\n",
        "    R(œÑ, j) = sum(r*Œ≥^(k-1) for (k,(s,a,r)) in zip(j:d, œÑ[j:end]))\n",
        "    ‚àáf(a,s,r_togo) = begin\n",
        "        P = p(Œ∏, a,s)\n",
        "        w = p(Œ∏‚Ä≤,a,s) / P\n",
        "        if (r_togo > 0 && w > 1+œµ) || (r_togo < 0 && w < 1-œµ)\n",
        "            return zeros(length(Œ∏))\n",
        "        end\n",
        "        return ‚àáœÄ(Œ∏‚Ä≤, a, s) * r_togo / P\n",
        "    end\n",
        "    ‚àáf(œÑ) = mean(‚àáf(a,s,R(œÑ,k)) for (k,(s,a,r)) in enumerate(œÑ))\n",
        "    return mean(‚àáf(œÑ) for œÑ in œÑs)\n",
        "end\n",
        "\n",
        "function update(M::ClampedSurrogateUpdate, Œ∏)\n",
        "    ùí´, b, d, m, œÄ, Œ±, k_max= M.ùí´, M.b, M.d, M.m, M.œÄ, M.Œ±, M.k_max\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    œÑs = [simulate(ùí´, rand(b), œÄŒ∏, d) for i in 1:m]\n",
        "    Œ∏‚Ä≤ = copy(Œ∏)\n",
        "    for k in 1:k_max\n",
        "        Œ∏‚Ä≤ += Œ±*clamped_gradient(M, Œ∏, Œ∏‚Ä≤, œÑs)\n",
        "    end\n",
        "    return Œ∏‚Ä≤\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### actor-critic 1\n",
        "struct ActorCritic\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ‚àálogœÄ # gradient of log likelihood ‚àálogœÄ(Œ∏,a,s)\n",
        "    U     # parameterized value function U(œï, s)\n",
        "    ‚àáU    # gradient of value function ‚àáU(œï,s)\n",
        "end\n",
        "\n",
        "function gradient(M::ActorCritic, œÄ, Œ∏, œï)\n",
        "    ùí´, b, d, m, ‚àálogœÄ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ\n",
        "    U, ‚àáU, Œ≥ = M.U, M.‚àáU, M.ùí´.Œ≥\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    R(œÑ,j) = sum(r*Œ≥^(k-1) for (k,(s,a,r)) in enumerate(œÑ[j:end]))\n",
        "    A(œÑ,j) = œÑ[j][3] + Œ≥*U(œï,œÑ[j+1][1]) - U(œï,œÑ[j][1])\n",
        "    ‚àáUŒ∏(œÑ) = sum(‚àálogœÄ(Œ∏,a,s)*A(œÑ,j)*Œ≥^(j-1) for (j, (s,a,r))\n",
        "    \t\t\t\tin enumerate(œÑ[1:end-1]))\n",
        "    ‚àá‚Ñìœï(œÑ) = sum((U(œï,s) - R(œÑ,j))*‚àáU(œï,s) for (j, (s,a,r))\n",
        "    \t\t\t\tin enumerate(œÑ))\n",
        "    trajs = [simulate(ùí´, rand(b), œÄŒ∏, d) for i in 1:m]\n",
        "    return mean(‚àáUŒ∏(œÑ) for œÑ in trajs), mean(‚àá‚Ñìœï(œÑ) for œÑ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### actor-critic 2\n",
        "struct GeneralizedAdvantageEstimation\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ‚àálogœÄ # gradient of log likelihood ‚àálogœÄ(Œ∏,a,s)\n",
        "    U     # parameterized value function U(œï, s)\n",
        "    ‚àáU    # gradient of value function ‚àáU(œï,s)\n",
        "    Œª     # weight ‚àà [0,1]\n",
        "end\n",
        "\n",
        "function gradient(M::GeneralizedAdvantageEstimation, œÄ, Œ∏, œï)\n",
        "    ùí´, b, d, m, ‚àálogœÄ = M.ùí´, M.b, M.d, M.m, M.‚àálogœÄ\n",
        "    U, ‚àáU, Œ≥, Œª = M.U, M.‚àáU, M.ùí´.Œ≥, M.Œª\n",
        "    œÄŒ∏(s) = œÄ(Œ∏, s)\n",
        "    R(œÑ,j) = sum(r*Œ≥^(k-1) for (k,(s,a,r)) in enumerate(œÑ[j:end]))\n",
        "    Œ¥(œÑ,j) = œÑ[j][3] + Œ≥*U(œï,œÑ[j+1][1]) - U(œï,œÑ[j][1])\n",
        "    A(œÑ,j) = sum((Œ≥*Œª)^(‚Ñì-1)*Œ¥(œÑ, j+‚Ñì-1) for ‚Ñì in 1:d-j)\n",
        "    ‚àáUŒ∏(œÑ) = sum(‚àálogœÄ(Œ∏,a,s)*A(œÑ,j)*Œ≥^(j-1)\n",
        "                    for (j, (s,a,r)) in enumerate(œÑ[1:end-1]))\n",
        "    ‚àá‚Ñìœï(œÑ) = sum((U(œï,s) - R(œÑ,j))*‚àáU(œï,s)\n",
        "                    for (j, (s,a,r)) in enumerate(œÑ))\n",
        "    trajs = [simulate(ùí´, rand(b), œÄŒ∏, d) for i in 1:m]\n",
        "    return mean(‚àáUŒ∏(œÑ) for œÑ in trajs), mean(‚àá‚Ñìœï(œÑ) for œÑ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### actor-critic 3\n",
        "struct DeterministicPolicyGradient\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ‚àáœÄ    # gradient of deterministic policy œÄ(Œ∏, s)\n",
        "    Q     # parameterized value function Q(œï,s,a)\n",
        "    ‚àáQœï   # gradient of value function with respect to œï\n",
        "    ‚àáQa   # gradient of value function with respect to a\n",
        "    œÉ     # policy noise\n",
        "end\n",
        "\n",
        "function gradient(M::DeterministicPolicyGradient, œÄ, Œ∏, œï)\n",
        "    ùí´, b, d, m, ‚àáœÄ = M.ùí´, M.b, M.d, M.m, M.‚àáœÄ\n",
        "    Q, ‚àáQœï, ‚àáQa, œÉ, Œ≥ = M.Q, M.‚àáQœï, M.‚àáQa, M.œÉ, M.ùí´.Œ≥\n",
        "    œÄ_rand(s) = œÄ(Œ∏, s) + œÉ*randn()*I\n",
        "    ‚àáUŒ∏(œÑ) = sum(‚àáœÄ(Œ∏,s)*‚àáQa(œï,s,œÄ(Œ∏,s))*Œ≥^(j-1) for (j,(s,a,r))\n",
        "                in enumerate(œÑ))\n",
        "    ‚àá‚Ñìœï(œÑ,j) = begin\n",
        "        s, a, r = œÑ[j]\n",
        "        s‚Ä≤ = œÑ[j+1][1]\n",
        "        a‚Ä≤ = œÄ(Œ∏,s‚Ä≤)\n",
        "        Œ¥ = r + Œ≥*Q(œï,s‚Ä≤,a‚Ä≤) - Q(œï,s,a)\n",
        "        return Œ¥*(Œ≥*‚àáQœï(œï,s‚Ä≤,a‚Ä≤) - ‚àáQœï(œï,s,a))\n",
        "    end\n",
        "    ‚àá‚Ñìœï(œÑ) = sum(‚àá‚Ñìœï(œÑ,j) for j in 1:length(œÑ)-1)\n",
        "    trajs = [simulate(ùí´, rand(b), œÄ_rand, d) for i in 1:m]\n",
        "    return mean(‚àáUŒ∏(œÑ) for œÑ in trajs), mean(‚àá‚Ñìœï(œÑ) for œÑ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### validation 1\n",
        "function adversarial(ùí´::MDP, œÄ, Œª)\n",
        "    ùíÆ, ùíú, T, R, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.T, ùí´.R, ùí´.Œ≥\n",
        "    ùíÆ‚Ä≤ = ùíú‚Ä≤ = ùíÆ\n",
        "    R‚Ä≤ = zeros(length(ùíÆ‚Ä≤), length(ùíú‚Ä≤))\n",
        "    T‚Ä≤ = zeros(length(ùíÆ‚Ä≤), length(ùíú‚Ä≤), length(ùíÆ‚Ä≤))\n",
        "    for s in ùíÆ‚Ä≤\n",
        "        for a in ùíú‚Ä≤\n",
        "            R‚Ä≤[s,a] = -R(s, œÄ(s)) + Œª*log(T(s, œÄ(s), a))\n",
        "            T‚Ä≤[s,a,a] = 1\n",
        "        end\n",
        "    end\n",
        "    return MDP(T‚Ä≤, R‚Ä≤, Œ≥)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 1\n",
        "struct BanditProblem\n",
        "    Œ∏ # vector of payoff probabilities\n",
        "    R # reward sampler\n",
        "end\n",
        "\n",
        "function BanditProblem(Œ∏)\n",
        "    R(a) = rand() < Œ∏[a] ? 1 : 0\n",
        "    return BanditProblem(Œ∏, R)\n",
        "end\n",
        "\n",
        "function simulate(ùí´::BanditProblem, model, œÄ, h)\n",
        "    for i in 1:h\n",
        "        a = œÄ(model)\n",
        "        r = ùí´.R(a)\n",
        "        update!(model, a, r)\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 2\n",
        "struct BanditModel\n",
        "    B # vector of beta distributions\n",
        "end\n",
        "\n",
        "function update!(model::BanditModel, a, r)\n",
        "    Œ±, Œ≤ = StatsBase.params(model.B[a])\n",
        "    model.B[a] = Beta(Œ± + r, Œ≤ + (1-r))\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 3\n",
        "mutable struct EpsilonGreedyExploration\n",
        "    œµ # probability of random arm\n",
        "end\n",
        "\n",
        "function (œÄ::EpsilonGreedyExploration)(model::BanditModel)\n",
        "    if rand() < œÄ.œµ\n",
        "        return rand(eachindex(model.B))\n",
        "    else\n",
        "        return argmax(mean.(model.B))\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 4\n",
        "mutable struct ExploreThenCommitExploration\n",
        "    k # pulls remaining until commitment\n",
        "end\n",
        "\n",
        "function (œÄ::ExploreThenCommitExploration)(model::BanditModel)\n",
        "    if œÄ.k > 0\n",
        "        œÄ.k -= 1\n",
        "        return rand(eachindex(model.B))\n",
        "    end\n",
        "    return argmax(mean.(model.B))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 5\n",
        "mutable struct SoftmaxExploration\n",
        "    Œª # precision parameter\n",
        "    Œ± # precision factor\n",
        "end\n",
        "\n",
        "function (œÄ::SoftmaxExploration)(model::BanditModel)\n",
        "    weights = exp.(œÄ.Œª * mean.(model.B))\n",
        "    œÄ.Œª *= œÄ.Œ±\n",
        "    return rand(Categorical(normalize(weights, 1)))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 6\n",
        "mutable struct QuantileExploration\n",
        "    Œ± # quantile (e.g., 0.95)\n",
        "end\n",
        "\n",
        "function (œÄ::QuantileExploration)(model::BanditModel)\n",
        "    return argmax([quantile(B, œÄ.Œ±) for B in model.B])\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 7\n",
        "mutable struct UCB1Exploration\n",
        "    c # exploration constant\n",
        "end\n",
        "\n",
        "function bonus(œÄ::UCB1Exploration, B, a)\n",
        "\tN = sum(b.Œ± + b.Œ≤ for b in B)\n",
        "\tNa = B[a].Œ± + B[a].Œ≤\n",
        "    return œÄ.c * sqrt(log(N)/Na)\n",
        "end\n",
        "\n",
        "function (œÄ::UCB1Exploration)(model::BanditModel)\n",
        "\tB = model.B\n",
        "    œÅ = mean.(B)\n",
        "    u = œÅ .+ [bonus(œÄ, B, a) for a in eachindex(B)]\n",
        "    return argmax(u)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 8\n",
        "struct PosteriorSamplingExploration end\n",
        "\n",
        "(œÄ::PosteriorSamplingExploration)(model::BanditModel) =\n",
        "    argmax(rand.(model.B))\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 9\n",
        "function simulate(ùí´::MDP, model, œÄ, h, s)\n",
        "    for i in 1:h\n",
        "        a = œÄ(model, s)\n",
        "        s‚Ä≤, r = ùí´.TR(s, a)\n",
        "        update!(model, s, a, r, s‚Ä≤)\n",
        "        s = s‚Ä≤\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 1\n",
        "mutable struct MaximumLikelihoodMDP\n",
        "    ùíÆ # state space (assumes 1:nstates)\n",
        "    ùíú # action space (assumes 1:nactions)\n",
        "    N # transition count N(s,a,s‚Ä≤)\n",
        "    œÅ # reward sum œÅ(s, a)\n",
        "    Œ≥ # discount\n",
        "    U # value function\n",
        "    planner\n",
        "end\n",
        "\n",
        "function lookahead(model::MaximumLikelihoodMDP, s, a)\n",
        "    ùíÆ, U, Œ≥ = model.ùíÆ, model.U, model.Œ≥\n",
        "    n = sum(model.N[s,a,:])\n",
        "    if n == 0\n",
        "        return 0.0\n",
        "    end\n",
        "    r = model.œÅ[s, a] / n\n",
        "    T(s,a,s‚Ä≤) = model.N[s,a,s‚Ä≤] / n\n",
        "    return r + Œ≥ * sum(T(s,a,s‚Ä≤)*U[s‚Ä≤] for s‚Ä≤ in ùíÆ)\n",
        "end\n",
        "\n",
        "function backup(model::MaximumLikelihoodMDP, U, s)\n",
        "    return maximum(lookahead(model, s, a) for a in model.ùíú)\n",
        "end\n",
        "\n",
        "function update!(model::MaximumLikelihoodMDP, s, a, r, s‚Ä≤)\n",
        "    model.N[s,a,s‚Ä≤] += 1\n",
        "    model.œÅ[s,a] += r\n",
        "    update!(model.planner, model, s, a, r, s‚Ä≤)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 2\n",
        "function MDP(model::MaximumLikelihoodMDP)\n",
        "    N, œÅ, ùíÆ, ùíú, Œ≥ = model.N, model.œÅ, model.ùíÆ, model.ùíú, model.Œ≥\n",
        "    T, R = similar(N), similar(œÅ)\n",
        "    for s in ùíÆ\n",
        "        for a in ùíú\n",
        "            n = sum(N[s,a,:])\n",
        "            if n == 0\n",
        "                T[s,a,:] .= 0.0\n",
        "                R[s,a] = 0.0\n",
        "            else\n",
        "                T[s,a,:] = N[s,a,:] / n\n",
        "                R[s,a] = œÅ[s,a] / n\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return MDP(T, R, Œ≥)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 3\n",
        "struct FullUpdate end\n",
        "\n",
        "function update!(planner::FullUpdate, model, s, a, r, s‚Ä≤)\n",
        "    ùí´ = MDP(model)\n",
        "    U = solve(ùí´).U\n",
        "    copy!(model.U, U)\n",
        "    return planner\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 4\n",
        "struct RandomizedUpdate\n",
        "    m # number of updates\n",
        "end\n",
        "\n",
        "function update!(planner::RandomizedUpdate, model, s, a, r, s‚Ä≤)\n",
        "    U = model.U\n",
        "    U[s] = backup(model, U, s)\n",
        "    for i in 1:planner.m\n",
        "        s = rand(model.ùíÆ)\n",
        "        U[s] = backup(model, U, s)\n",
        "    end\n",
        "    return planner\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 5\n",
        "struct PrioritizedUpdate\n",
        "    m  # number of updates\n",
        "    pq # priority queue\n",
        "end\n",
        "\n",
        "function update!(planner::PrioritizedUpdate, model, s)\n",
        "    N, U, pq = model.N, model.U, planner.pq\n",
        "    ùíÆ, ùíú = model.ùíÆ, model.ùíú\n",
        "    u = U[s]\n",
        "    U[s] = backup(model, U, s)\n",
        "    for s‚Åª in ùíÆ\n",
        "        for a‚Åª in ùíú\n",
        "            n_sa = sum(N[s‚Åª,a‚Åª,s‚Ä≤] for s‚Ä≤ in ùíÆ)\n",
        "            if n_sa > 0\n",
        "                T = N[s‚Åª,a‚Åª,s] / n_sa\n",
        "                priority = T * abs(U[s] - u)\n",
        "                if priority > 0\n",
        "                    pq[s‚Åª] = max(get(pq, s‚Åª, 0.0), priority)\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return planner\n",
        "end\n",
        "\n",
        "function update!(planner::PrioritizedUpdate, model, s, a, r, s‚Ä≤)\n",
        "    planner.pq[s] = Inf\n",
        "    for i in 1:planner.m\n",
        "        if isempty(planner.pq)\n",
        "            break\n",
        "        end\n",
        "        update!(planner, model, dequeue!(planner.pq))\n",
        "    end\n",
        "    return planner\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 6\n",
        "function (œÄ::EpsilonGreedyExploration)(model, s)\n",
        "    ùíú, œµ = model.ùíú, œÄ.œµ\n",
        "    if rand() < œµ\n",
        "        return rand(ùíú)\n",
        "    end\n",
        "    Q(s,a) = lookahead(model, s, a)\n",
        "    return argmax(a->Q(s,a), ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 7\n",
        "mutable struct RmaxMDP\n",
        "    ùíÆ # state space (assumes 1:nstates)\n",
        "    ùíú # action space (assumes 1:nactions)\n",
        "    N # transition count N(s,a,s‚Ä≤)\n",
        "    œÅ # reward sum œÅ(s, a)\n",
        "    Œ≥ # discount\n",
        "    U # value function\n",
        "    planner\n",
        "    m    # count threshold\n",
        "    rmax # maximum reward\n",
        "end\n",
        "\n",
        "function lookahead(model::RmaxMDP, s, a)\n",
        "    ùíÆ, U, Œ≥ = model.ùíÆ, model.U, model.Œ≥\n",
        "    n = sum(model.N[s,a,:])\n",
        "    if n < model.m\n",
        "        return model.rmax / (1-Œ≥)\n",
        "    end\n",
        "    r = model.œÅ[s, a] / n\n",
        "    T(s,a,s‚Ä≤) = model.N[s,a,s‚Ä≤] / n\n",
        "    return r + Œ≥ * sum(T(s,a,s‚Ä≤)*U[s‚Ä≤] for s‚Ä≤ in ùíÆ)\n",
        "end\n",
        "\n",
        "function backup(model::RmaxMDP, U, s)\n",
        "    return maximum(lookahead(model, s, a) for a in model.ùíú)\n",
        "end\n",
        "\n",
        "function update!(model::RmaxMDP, s, a, r, s‚Ä≤)\n",
        "    model.N[s,a,s‚Ä≤] += 1\n",
        "    model.œÅ[s,a] += r\n",
        "    update!(model.planner, model, s, a, r, s‚Ä≤)\n",
        "    return model\n",
        "end\n",
        "\n",
        "function MDP(model::RmaxMDP)\n",
        "    N, œÅ, ùíÆ, ùíú, Œ≥ = model.N, model.œÅ, model.ùíÆ, model.ùíú, model.Œ≥\n",
        "    T, R, m, rmax = similar(N), similar(œÅ), model.m, model.rmax\n",
        "    for s in ùíÆ\n",
        "        for a in ùíú\n",
        "            n = sum(N[s,a,:])\n",
        "            if n < m\n",
        "                T[s,a,:] .= 0.0\n",
        "                T[s,a,s] = 1.0\n",
        "                R[s,a] = rmax\n",
        "            else\n",
        "                T[s,a,:] = N[s,a,:] / n\n",
        "                R[s,a] = œÅ[s,a] / n\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return MDP(T, R, Œ≥)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 8\n",
        "mutable struct BayesianMDP\n",
        "    ùíÆ # state space (assumes 1:nstates)\n",
        "    ùíú # action space (assumes 1:nactions)\n",
        "    D # Dirichlet distributions D[s,a]\n",
        "    R # reward function as matrix (not estimated)\n",
        "    Œ≥ # discount\n",
        "    U # value function\n",
        "    planner\n",
        "end\n",
        "\n",
        "function lookahead(model::BayesianMDP, s, a)\n",
        "    ùíÆ, U, Œ≥ = model.ùíÆ, model.U, model.Œ≥\n",
        "    n = sum(model.D[s,a].alpha)\n",
        "    if n == 0\n",
        "        return 0.0\n",
        "    end\n",
        "    r = model.R(s,a)\n",
        "    T(s,a,s‚Ä≤) = model.D[s,a].alpha[s‚Ä≤] / n\n",
        "    return r + Œ≥ * sum(T(s,a,s‚Ä≤)*U[s‚Ä≤] for s‚Ä≤ in ùíÆ)\n",
        "end\n",
        "\n",
        "function update!(model::BayesianMDP, s, a, r, s‚Ä≤)\n",
        "    Œ± = model.D[s,a].alpha\n",
        "    Œ±[s‚Ä≤] += 1\n",
        "    model.D[s,a] = Dirichlet(Œ±)\n",
        "    update!(model.planner, model, s, a, r, s‚Ä≤)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 9\n",
        "struct PosteriorSamplingUpdate end\n",
        "\n",
        "function Base.rand(model::BayesianMDP)\n",
        "    ùíÆ, ùíú = model.ùíÆ, model.ùíú\n",
        "    T = zeros(length(ùíÆ), length(ùíú), length(ùíÆ))\n",
        "    for s in ùíÆ\n",
        "        for a in ùíú\n",
        "            T[s,a,:] = rand(model.D[s,a])\n",
        "        end\n",
        "    end\n",
        "    return MDP(T, model.R, model.Œ≥)\n",
        "end\n",
        "\n",
        "function update!(planner::PosteriorSamplingUpdate, model, s, a, r, s‚Ä≤)\n",
        "    ùí´ = rand(model)\n",
        "    U = solve(ùí´).U\n",
        "    copy!(model.U, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 1\n",
        "mutable struct IncrementalEstimate\n",
        "\tŒº # mean estimate\n",
        "\tŒ± # learning rate function\n",
        "\tm # number of updates\n",
        "end\n",
        "\n",
        "function update!(model::IncrementalEstimate, x)\n",
        "\tmodel.m += 1\n",
        "\tmodel.Œº += model.Œ±(model.m) * (x - model.Œº)\n",
        "\treturn model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 2\n",
        "mutable struct QLearning\n",
        "    ùíÆ # state space (assumes 1:nstates)\n",
        "    ùíú # action space (assumes 1:nactions)\n",
        "    Œ≥ # discount\n",
        "    Q # action value function\n",
        "    Œ± # learning rate\n",
        "end\n",
        "\n",
        "lookahead(model::QLearning, s, a) = model.Q[s,a]\n",
        "\n",
        "function update!(model::QLearning, s, a, r, s‚Ä≤)\n",
        "    Œ≥, Q, Œ± = model.Œ≥, model.Q, model.Œ±\n",
        "    Q[s,a] += Œ±*(r + Œ≥*maximum(Q[s‚Ä≤,:]) - Q[s,a])\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 3\n",
        "mutable struct Sarsa\n",
        "    ùíÆ # state space (assumes 1:nstates)\n",
        "    ùíú # action space (assumes 1:nactions)\n",
        "    Œ≥ # discount\n",
        "    Q # action value function\n",
        "    Œ± # learning rate\n",
        "    ‚Ñì # most recent experience tuple (s,a,r)\n",
        "end\n",
        "\n",
        "lookahead(model::Sarsa, s, a) = model.Q[s,a]\n",
        "\n",
        "function update!(model::Sarsa, s, a, r, s‚Ä≤)\n",
        "    if model.‚Ñì != nothing\n",
        "        Œ≥, Q, Œ±, ‚Ñì = model.Œ≥, model.Q, model.Œ±,  model.‚Ñì\n",
        "        model.Q[‚Ñì.s,‚Ñì.a] += Œ±*(‚Ñì.r + Œ≥*Q[s,a] - Q[‚Ñì.s,‚Ñì.a])\n",
        "    end\n",
        "    model.‚Ñì = (s=s, a=a, r=r)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 4\n",
        "mutable struct SarsaLambda\n",
        "    ùíÆ # state space (assumes 1:nstates)\n",
        "    ùíú # action space (assumes 1:nactions)\n",
        "    Œ≥ # discount\n",
        "    Q # action value function\n",
        "    N # trace\n",
        "    Œ± # learning rate\n",
        "    Œª # trace decay rate\n",
        "    ‚Ñì # most recent experience tuple (s,a,r)\n",
        "end\n",
        "\n",
        "lookahead(model::SarsaLambda, s, a) = model.Q[s,a]\n",
        "\n",
        "function update!(model::SarsaLambda, s, a, r, s‚Ä≤)\n",
        "    if model.‚Ñì != nothing\n",
        "        Œ≥, Œª, Q, Œ±, ‚Ñì = model.Œ≥, model.Œª, model.Q, model.Œ±, model.‚Ñì\n",
        "        model.N[‚Ñì.s,‚Ñì.a] += 1\n",
        "        Œ¥ = ‚Ñì.r + Œ≥*Q[s,a] - Q[‚Ñì.s,‚Ñì.a]\n",
        "        for s in model.ùíÆ\n",
        "            for a in model.ùíú\n",
        "                model.Q[s,a] += Œ±*Œ¥*model.N[s,a]\n",
        "                model.N[s,a] *= Œ≥*Œª\n",
        "            end\n",
        "        end\n",
        "    else\n",
        "    \tmodel.N[:,:] .= 0.0\n",
        "    end\n",
        "    model.‚Ñì = (s=s, a=a, r=r)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 5\n",
        "struct GradientQLearning\n",
        "    ùíú  # action space (assumes 1:nactions)\n",
        "    Œ≥  # discount\n",
        "    Q  # parameterized action value function Q(Œ∏,s,a)\n",
        "    ‚àáQ # gradient of action value function\n",
        "    Œ∏  # action value function parameter\n",
        "    Œ±  # learning rate\n",
        "end\n",
        "\n",
        "function lookahead(model::GradientQLearning, s, a)\n",
        "    return model.Q(model.Œ∏, s,a)\n",
        "end\n",
        "\n",
        "function update!(model::GradientQLearning, s, a, r, s‚Ä≤)\n",
        "    ùíú, Œ≥, Q, Œ∏, Œ± = model.ùíú, model.Œ≥, model.Q, model.Œ∏, model.Œ±\n",
        "    u = maximum(Q(Œ∏,s‚Ä≤,a‚Ä≤) for a‚Ä≤ in ùíú)\n",
        "    Œî = (r + Œ≥*u - Q(Œ∏,s,a))*model.‚àáQ(Œ∏,s,a)\n",
        "    Œ∏[:] += Œ±*scale_gradient(Œî, 1)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 6\n",
        "struct ReplayGradientQLearning\n",
        "    ùíú      # action space (assumes 1:nactions)\n",
        "    Œ≥      # discount\n",
        "    Q      # parameterized action value function Q(Œ∏,s,a)\n",
        "    ‚àáQ     # gradient of action value function\n",
        "    Œ∏      # action value function parameter\n",
        "    Œ±      # learning rate\n",
        "    buffer # circular memory buffer\n",
        "    m      # number of steps between gradient updates\n",
        "    m_grad # batch size\n",
        "end\n",
        "\n",
        "function lookahead(model::ReplayGradientQLearning, s, a)\n",
        "    return model.Q(model.Œ∏, s,a)\n",
        "end\n",
        "\n",
        "function update!(model::ReplayGradientQLearning, s, a, r, s‚Ä≤)\n",
        "    ùíú, Œ≥, Q, Œ∏, Œ± = model.ùíú, model.Œ≥, model.Q, model.Œ∏, model.Œ±\n",
        "    buffer, m, m_grad = model.buffer, model.m, model.m_grad\n",
        "    if isfull(buffer)\n",
        "        U(s) = maximum(Q(Œ∏,s,a) for a in ùíú)\n",
        "        ‚àáQ(s,a,r,s‚Ä≤) = (r + Œ≥*U(s‚Ä≤) - Q(Œ∏,s,a))*model.‚àáQ(Œ∏,s,a)\n",
        "        Œî = mean(‚àáQ(s,a,r,s‚Ä≤) for (s,a,r,s‚Ä≤) in rand(buffer, m_grad))\n",
        "        Œ∏[:] += Œ±*scale_gradient(Œî, 1)\n",
        "        for i in 1:m # discard oldest experiences\n",
        "            popfirst!(buffer)\n",
        "        end\n",
        "    else\n",
        "        push!(buffer, (s,a,r,s‚Ä≤))\n",
        "    end\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 1\n",
        "struct BehavioralCloning\n",
        "    Œ±     # step size\n",
        "    k_max # number of iterations\n",
        "    ‚àálogœÄ # log likelihood gradient\n",
        "end\n",
        "\n",
        "function optimize(M::BehavioralCloning, D, Œ∏)\n",
        "\tŒ±, k_max, ‚àálogœÄ = M.Œ±, M.k_max, M.‚àálogœÄ\n",
        "\tfor k in 1:k_max\n",
        "\t\t‚àá = mean(‚àálogœÄ(Œ∏, a, s) for (s,a) in D)\n",
        "\t\tŒ∏ += Œ±*‚àá\n",
        "\tend\n",
        "\treturn Œ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 2\n",
        "struct CostSensitiveMultiClassifier\n",
        "\tùíú     # action space\n",
        "    Œ±     # step size\n",
        "    C     # cost function\n",
        "    k_max # number of iterations\n",
        "    ‚àáœÄ    # policy likelihood gradient\n",
        "end\n",
        "\n",
        "function optimize(M::CostSensitiveMultiClassifier, D, Œ∏)\n",
        "\tùíú, Œ±, C, k_max, ‚àáœÄ = M.ùíú, M.Œ±, M.C, M.k_max, M.‚àáœÄ\n",
        "\tfor k in 1:k_max\n",
        "\t\t‚àá = mean(sum(C(s,a,a_pred)*‚àáœÄ(Œ∏, a_pred, s)\n",
        "\t\t\t\tfor a_pred in ùíú)\n",
        "\t\t\t\t\tfor (s,a) in D)\n",
        "\t\tŒ∏ -= Œ±*‚àá\n",
        "\tend\n",
        "\treturn Œ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 3\n",
        "struct DataSetAggregation\n",
        "\tùí´     # problem with unknown reward function\n",
        "\tbc    # behavioral cloning struct\n",
        "\tk_max # number of iterations\n",
        "\tm     # number of rollouts per iteration\n",
        "\td     # rollout depth\n",
        "\tb     # initial state distribution\n",
        "\tœÄE    # expert\n",
        "\tœÄŒ∏    # parameterized policy\n",
        "end\n",
        "\n",
        "function optimize(M::DataSetAggregation, D, Œ∏)\n",
        "\tùí´, bc, k_max, m = M.ùí´, M.bc, M.k_max, M.m\n",
        "\td, b, œÄE, œÄŒ∏ = M.d, M.b, M.œÄE, M.œÄŒ∏\n",
        "\tŒ∏ = optimize(bc, D, Œ∏)\n",
        "\tfor k in 2:k_max\n",
        "\t\tfor i in 1:m\n",
        "\t\t\ts = rand(b)\n",
        "\t\t\tfor j in 1:d\n",
        "\t\t\t\tpush!(D, (s, œÄE(s)))\n",
        "\t\t\t\ta = rand(œÄŒ∏(Œ∏, s))\n",
        "\t\t\t\ts = rand(ùí´.T(s, a))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\tŒ∏ = optimize(bc, D, Œ∏)\n",
        "\tend\n",
        "\treturn Œ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 4\n",
        "struct SEARN\n",
        "\tùí´     # problem with unknown reward\n",
        "\tmc    # cost-sensitive multiclass classifier struct\n",
        "\tk_max # number of iterations\n",
        "\tm     # number of rollouts per iteration\n",
        "\td     # rollout depth\n",
        "\tb     # initial state distribution\n",
        "\tŒ≤     # mixing scalar\n",
        "\tœÄE    # expert policy\n",
        "\tœÄŒ∏    # parameterized policy\n",
        "end\n",
        "\n",
        "function optimize(M::SEARN, Œ∏)\n",
        "\tùí´, mc, k_max, m = M.ùí´, M.mc, M.k_max, M.m\n",
        "\td, b, Œ≤, œÄE, œÄŒ∏ = M.d, M.b, M.Œ≤, M.œÄE, M.œÄŒ∏\n",
        "\tŒ∏s, œÄ = Vector{Float64}[], s -> œÄE(s)\n",
        "    T, ùíú = ùí´.T, ùí´.ùíú\n",
        "\tfor k in 1:k_max\n",
        "        D = []\n",
        "\t\tfor i in 1:m\n",
        "\t\t\ts = rand(b)\n",
        "\t\t\tfor j in 1:d\n",
        "\t\t\t\tc = [rollout(ùí´, rand(T(s, a)), œÄ, d-j) for a in ùíú]\n",
        "\t\t\t\tc = maximum(c) .- c\n",
        "\n",
        "\t\t\t\tpush!(D, (s, c))\n",
        "\t\t\t\ts = rand(T(s, œÄ(s)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\n",
        "\t\tŒ∏ = optimize(mc, D, Œ∏)\n",
        "\t\tpush!(Œ∏s, Œ∏)\n",
        "\n",
        "\t\tœÄ_hat = s -> rand(Categorical(œÄŒ∏(Œ∏, s)))\n",
        "\t\tœÄ = s -> rand() < Œ≤ ? œÄ_hat(s) : œÄ(s)\n",
        "\tend\n",
        "\n",
        "\t# Compute a policy that does not contain the expert\n",
        "\tPœÄ = Categorical(normalize([(1-Œ≤)^(k_max-i) for i in 1:k_max],1))\n",
        "\treturn œÄ = s -> rand(Categorical(œÄŒ∏(Œ∏s[rand(PœÄ)], s)))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 5\n",
        "struct SMILe\n",
        "\tùí´     # problem with unknown reward\n",
        "\tbc    # Behavioral cloning struct\n",
        "\tk_max # number of iterations\n",
        "\tm     # number of rollouts per iteration\n",
        "\td     # rollout depth\n",
        "\tb     # initial state distribution\n",
        "\tŒ≤     # mixing scalar (e.g., d^-3)\n",
        "\tœÄE    # expert policy\n",
        "\tœÄŒ∏    # parameterized policy\n",
        "end\n",
        "\n",
        "function optimize(M::SMILe, Œ∏)\n",
        "    ùí´, bc, k_max, m = M.ùí´, M.bc, M.k_max, M.m\n",
        "    d, b, Œ≤, œÄE, œÄŒ∏ = M.d, M.b, M.Œ≤, M.œÄE, M.œÄŒ∏\n",
        "    ùíú, T = ùí´.ùíú, ùí´.T\n",
        "\tŒ∏s = []\n",
        "\tœÄ = s -> œÄE(s)\n",
        "\tfor k in 1:k_max\n",
        "\t\t# execute latest œÄ to get new data set D\n",
        "        D = []\n",
        "\t\tfor i in 1:m\n",
        "\t\t\ts = rand(b)\n",
        "\t\t\tfor j in 1:d\n",
        "\t\t\t\tpush!(D, (s, œÄE(s)))\n",
        "\t\t\t\ta = œÄ(s)\n",
        "\t\t\t\ts = rand(T(s, a))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\t# train new policy classifier\n",
        "\t\tŒ∏ = optimize(bc, D, Œ∏)\n",
        "\t\tpush!(Œ∏s, Œ∏)\n",
        "\t\t# compute a new policy mixture\n",
        "\t\tPœÄ = Categorical(normalize([(1-Œ≤)^(i-1) for i in 1:k],1))\n",
        "\t\tœÄ = s -> begin\n",
        "\t\t\tif rand() < (1-Œ≤)^(k-1)\n",
        "\t\t\t\treturn œÄE(s)\n",
        "\t\t\telse\n",
        "\t\t\t\treturn rand(Categorical(œÄŒ∏(Œ∏s[rand(PœÄ)], s)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\tPs = normalize([(1-Œ≤)^(i-1) for i in 1:k_max],1)\n",
        "    return Ps, Œ∏s\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 6\n",
        "struct InverseReinforcementLearning\n",
        "    ùí´  # problem\n",
        "    b  # initial state distribution\n",
        "    d  # depth\n",
        "    m  # number of samples\n",
        "    œÄ  # parameterized policy\n",
        "    Œ≤  # binary feature mapping\n",
        "    ŒºE # expert feature expectations\n",
        "    RL # reinforcement learning method\n",
        "    œµ  # tolerance\n",
        "end\n",
        "\n",
        "function feature_expectations(M::InverseReinforcementLearning, œÄ)\n",
        "    ùí´, b, m, d, Œ≤, Œ≥ = M.ùí´, M.b, M.m, M.d, M.Œ≤, M.ùí´.Œ≥\n",
        "    Œº(œÑ) = sum(Œ≥^(k-1)*Œ≤(s, a) for (k,(s,a)) in enumerate(œÑ))\n",
        "    œÑs = [simulate(ùí´, rand(b), œÄ, d) for i in 1:m]\n",
        "    return mean(Œº(œÑ) for œÑ in œÑs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 7\n",
        "function calc_weighting(M::InverseReinforcementLearning, Œºs)\n",
        "    ŒºE = M.ŒºE\n",
        "    k = length(ŒºE)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, t)\n",
        "    @variable(model, œï[1:k] ‚â• 0)\n",
        "    @objective(model, Max, t)\n",
        "    for Œº in Œºs\n",
        "        @constraint(model, œï‚ãÖŒºE ‚â• œï‚ãÖŒº + t)\n",
        "    end\n",
        "    @constraint(model, œï‚ãÖœï ‚â§ 1)\n",
        "    optimize!(model)\n",
        "    return (value(t), value.(œï))\n",
        "end\n",
        "\n",
        "function calc_policy_mixture(M::InverseReinforcementLearning, Œºs)\n",
        "    ŒºE = M.ŒºE\n",
        "    k = length(Œºs)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, Œª[1:k] ‚â• 0)\n",
        "    @objective(model, Min, (ŒºE - sum(Œª[i]*Œºs[i] for i in 1:k))‚ãÖ\n",
        "                            (ŒºE - sum(Œª[i]*Œºs[i] for i in 1:k)))\n",
        "    @constraint(model, sum(Œª) == 1)\n",
        "    optimize!(model)\n",
        "    return value.(Œª)\n",
        "end\n",
        "\n",
        "function optimize(M::InverseReinforcementLearning, Œ∏)\n",
        "    œÄ, œµ, RL = M.œÄ, M.œµ, M.RL\n",
        "    Œ∏s = [Œ∏]\n",
        "    Œºs = [feature_expectations(M, s->œÄ(Œ∏,s))]\n",
        "    while true\n",
        "        t, œï = calc_weighting(M, Œºs)\n",
        "        if t ‚â§ œµ\n",
        "            break\n",
        "        end\n",
        "        copyto!(RL.œï, œï) # R(s,a) = œï‚ãÖŒ≤(s,a)\n",
        "        Œ∏ = optimize(RL, œÄ, Œ∏)\n",
        "        push!(Œ∏s, Œ∏)\n",
        "        push!(Œºs, feature_expectations(M, s->œÄ(Œ∏,s)))\n",
        "    end\n",
        "    Œª = calc_policy_mixture(M, Œºs)\n",
        "    return Œª, Œ∏s\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 8\n",
        "struct MaximumEntropyIRL\n",
        "    ùí´     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    œÄ     # parameterized policy œÄ(Œ∏,s)\n",
        "    PœÄ    # parameterized policy likelihood œÄ(Œ∏, a, s)\n",
        "    ‚àáR    # reward function gradient\n",
        "    RL    # reinforcement learning method\n",
        "    Œ±     # step size\n",
        "    k_max # number of iterations\n",
        "end\n",
        "\n",
        "function discounted_state_visitations(M::MaximumEntropyIRL, Œ∏)\n",
        "\tùí´, b, d, PœÄ = M.ùí´, M.b, M.d, M.PœÄ\n",
        "\tùíÆ, ùíú, T, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.T, ùí´.Œ≥\n",
        "\tb_sk = zeros(length(ùí´.ùíÆ), d)\n",
        "\tb_sk[:,1] = [pdf(b, s) for s in ùíÆ]\n",
        "\tfor k in 2:d\n",
        "        for (si‚Ä≤, s‚Ä≤) in enumerate(ùíÆ)\n",
        "            b_sk[si‚Ä≤,k] = Œ≥*sum(sum(b_sk[si,k-1]*PœÄ(Œ∏, a, s)*T(s, a, s‚Ä≤)\n",
        "\t\t\t\t\tfor (si,s) in enumerate(ùíÆ))\n",
        "\t\t\t\tfor a in ùíú)\n",
        "\t\tend\n",
        "\tend\n",
        "    return normalize!(vec(mean(b_sk, dims=2)),1)\n",
        "end\n",
        "\n",
        "function optimize(M::MaximumEntropyIRL, D, œï, Œ∏)\n",
        "\tùí´, œÄ, PœÄ, ‚àáR, RL, Œ±, k_max = M.ùí´, M.œÄ, M.PœÄ, M.‚àáR, M.RL, M.Œ±, M.k_max\n",
        "    ùíÆ, ùíú, Œ≥, nD = ùí´.ùíÆ, ùí´.ùíú, ùí´.Œ≥, length(D)\n",
        "    for k in 1:k_max\n",
        "    \tcopyto!(RL.œï, œï) # update parameters\n",
        "\t\tŒ∏ = optimize(RL, œÄ, Œ∏)\n",
        "    \tb = discounted_state_visitations(M, Œ∏)\n",
        "    \t‚àáRœÑ = œÑ -> sum(Œ≥^(i-1)*‚àáR(œï,s,a) for (i,(s,a)) in enumerate(œÑ))\n",
        "    \t‚àáf = sum(‚àáRœÑ(œÑ) for œÑ in D) - nD*sum(b[si]*sum(PœÄ(Œ∏,a,s)*‚àáR(œï,s,a)\n",
        "        \t\t\tfor (ai,a) in enumerate(ùíú))\n",
        "        \t\tfor (si, s) in enumerate(ùíÆ))\n",
        "        œï += Œ±*‚àáf\n",
        "    end\n",
        "    return œï, Œ∏\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 1\n",
        "struct POMDP\n",
        "    Œ≥   # discount factor\n",
        "    ùíÆ   # state space\n",
        "    ùíú   # action space\n",
        "    ùí™   # observation space\n",
        "    T   # transition function\n",
        "    R   # reward function\n",
        "    O   # observation function\n",
        "    TRO # sample transition, reward, and observation\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 2\n",
        "function update(b::Vector{Float64}, ùí´, a, o)\n",
        "\tùíÆ, T, O = ùí´.ùíÆ, ùí´.T, ùí´.O\n",
        "    b‚Ä≤ = similar(b)\n",
        "    for (i‚Ä≤, s‚Ä≤) in enumerate(ùíÆ)\n",
        "        po = O(a, s‚Ä≤, o)\n",
        "        b‚Ä≤[i‚Ä≤] = po * sum(T(s, a, s‚Ä≤) * b[i] for (i, s) in enumerate(ùíÆ))\n",
        "    end\n",
        "    if sum(b‚Ä≤) ‚âà 0.0\n",
        "    \tfill!(b‚Ä≤, 1)\n",
        "    end\n",
        "    return normalize!(b‚Ä≤, 1)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 3\n",
        "struct KalmanFilter\n",
        "\tŒºb # mean vector\n",
        "\tŒ£b # covariance matrix\n",
        "end\n",
        "\n",
        "function update(b::KalmanFilter, ùí´, a, o)\n",
        "\tŒºb, Œ£b = b.Œºb, b.Œ£b\n",
        "\tTs, Ta, Os = ùí´.Ts, ùí´.Ta, ùí´.Os\n",
        "\tŒ£s, Œ£o = ùí´.Œ£s, ùí´.Œ£o\n",
        "\t# predict\n",
        "\tŒºp = Ts*Œºb + Ta*a\n",
        "\tŒ£p = Ts*Œ£b*Ts' + Œ£s\n",
        "\t# update\n",
        "\tŒ£po = Œ£p*Os'\n",
        "\tK = Œ£po/(Os*Œ£p*Os' + Œ£o)\n",
        "\tŒºb‚Ä≤ = Œºp + K*(o - Os*Œºp)\n",
        "\tŒ£b‚Ä≤ = (I - K*Os)*Œ£p\n",
        "\treturn KalmanFilter(Œºb‚Ä≤, Œ£b‚Ä≤)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 4\n",
        "struct ExtendedKalmanFilter\n",
        "\tŒºb # mean vector\n",
        "\tŒ£b # covariance matrix\n",
        "end\n",
        "\n",
        "import ForwardDiff: jacobian\n",
        "function update(b::ExtendedKalmanFilter, ùí´, a, o)\n",
        "\tŒºb, Œ£b = b.Œºb, b.Œ£b\n",
        "\tfT, fO = ùí´.fT, ùí´.fO\n",
        "\tŒ£s, Œ£o = ùí´.Œ£s, ùí´.Œ£o\n",
        "\t# predict\n",
        "\tŒºp = fT(Œºb, a)\n",
        "\tTs = jacobian(s->fT(s, a), Œºb)\n",
        "\tOs = jacobian(fO, Œºp)\n",
        "\tŒ£p = Ts*Œ£b*Ts' + Œ£s\n",
        "\t# update\n",
        "\tŒ£po = Œ£p*Os'\n",
        "\tK = Œ£po/(Os*Œ£p*Os' + Œ£o)\n",
        "\tŒºb‚Ä≤ = Œºp + K*(o - fO(Œºp))\n",
        "\tŒ£b‚Ä≤ = (I - K*Os)*Œ£p\n",
        "\treturn ExtendedKalmanFilter(Œºb‚Ä≤, Œ£b‚Ä≤)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 5\n",
        "struct UnscentedKalmanFilter\n",
        "\tŒºb # mean vector\n",
        "\tŒ£b # covariance matrix\n",
        "\tŒª  # spread parameter\n",
        "end\n",
        "\n",
        "function unscented_transform(Œº, Œ£, f, Œª, ws)\n",
        "    n = length(Œº)\n",
        "    Œî = cholesky((n + Œª) * Œ£).L\n",
        "\tS = [Œº]\n",
        "\tfor i in 1:n\n",
        "\t\tpush!(S, Œº + Œî[:,i])\n",
        "\t\tpush!(S, Œº - Œî[:,i])\n",
        "\tend\n",
        "    S‚Ä≤ = f.(S)\n",
        "\tŒº‚Ä≤ = sum(w*s for (w,s) in zip(ws, S‚Ä≤))\n",
        "\tŒ£‚Ä≤ = sum(w*(s - Œº‚Ä≤)*(s - Œº‚Ä≤)' for (w,s) in zip(ws, S‚Ä≤))\n",
        "\treturn (Œº‚Ä≤, Œ£‚Ä≤, S, S‚Ä≤)\n",
        "end\n",
        "\n",
        "function update(b::UnscentedKalmanFilter, ùí´, a, o)\n",
        "\tŒºb, Œ£b, Œª = b.Œºb, b.Œ£b, b.Œª\n",
        "\tfT, fO = ùí´.fT, ùí´.fO\n",
        "\tn = length(Œºb)\n",
        "\tws = [Œª / (n + Œª); fill(1/(2(n + Œª)), 2n)]\n",
        "    # predict\n",
        "    Œºp, Œ£p, Sp, Sp‚Ä≤ = unscented_transform(Œºb, Œ£b, s->fT(s,a), Œª, ws)\n",
        "\tŒ£p += ùí´.Œ£s\n",
        "    # update\n",
        "    Œºo, Œ£o, So, So‚Ä≤ = unscented_transform(Œºp, Œ£p, fO, Œª, ws)\n",
        "\tŒ£o += ùí´.Œ£o\n",
        "\tŒ£po = sum(w*(s - Œºp)*(s‚Ä≤ - Œºo)' for (w,s,s‚Ä≤) in zip(ws, So, So‚Ä≤))\n",
        "\tK = Œ£po / Œ£o\n",
        "\tŒºb‚Ä≤ = Œºp + K*(o - Œºo)\n",
        "\tŒ£b‚Ä≤ = Œ£p - K*Œ£o*K'\n",
        "\treturn UnscentedKalmanFilter(Œºb‚Ä≤, Œ£b‚Ä≤, Œª)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 6\n",
        "struct ParticleFilter\n",
        "\tstates # vector of state samples\n",
        "end\n",
        "\n",
        "function update(b::ParticleFilter, ùí´, a, o)\n",
        "\tT, O = ùí´.T, ùí´.O\n",
        "\tstates = [rand(T(s, a)) for s in b.states]\n",
        "\tweights = [O(a, s‚Ä≤, o) for s‚Ä≤ in states]\n",
        "\tD = SetCategorical(states, weights)\n",
        "\treturn ParticleFilter(rand(D, length(states)))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 7\n",
        "struct RejectionParticleFilter\n",
        "\tstates # vector of state samples\n",
        "end\n",
        "\n",
        "function update(b::RejectionParticleFilter, ùí´, a, o)\n",
        "\tT, O = ùí´.T, ùí´.O\n",
        "\tstates = similar(b.states)\n",
        "\ti = 1\n",
        "\twhile i ‚â§ length(states)\n",
        "\t\ts = rand(b.states)\n",
        "\t\ts‚Ä≤ = rand(T(s,a))\n",
        "\t\tif rand(O(a,s‚Ä≤)) == o\n",
        "\t\t\tstates[i] = s‚Ä≤\n",
        "\t\t\ti += 1\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn RejectionParticleFilter(states)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 8\n",
        "struct InjectionParticleFilter\n",
        "\tstates # vector of state samples\n",
        "\tm_inject # number of samples to inject\n",
        "\tD_inject # injection distribution\n",
        "end\n",
        "\n",
        "function update(b::InjectionParticleFilter, ùí´, a, o)\n",
        "\tT, O, m_inject, D_inject = ùí´.T, ùí´.O, b.m_inject, b.D_inject\n",
        "\tstates = [rand(T(s, a)) for s in b.states]\n",
        "\tweights = [O(a, s‚Ä≤, o) for s‚Ä≤ in states]\n",
        "\tD = SetCategorical(states, weights)\n",
        "\tm = length(states)\n",
        "\tstates = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))\n",
        "\treturn InjectionParticleFilter(states, m_inject, D_inject)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 9\n",
        "mutable struct AdaptiveInjectionParticleFilter\n",
        "\tstates   # vector of state samples\n",
        "\tw_slow   # slow moving average\n",
        "\tw_fast   # fast moving average\n",
        "\tŒ±_slow   # slow moving average parameter\n",
        "\tŒ±_fast   # fast moving average parameter\n",
        "\tŒΩ        # injection parameter\n",
        "\tD_inject # injection distribution\n",
        "end\n",
        "\n",
        "function update(b::AdaptiveInjectionParticleFilter, ùí´, a, o)\n",
        "\tT, O = ùí´.T, ùí´.O\n",
        "\tw_slow, w_fast, Œ±_slow, Œ±_fast, ŒΩ, D_inject =\n",
        "\t\tb.w_slow, b.w_fast, b.Œ±_slow, b.Œ±_fast, b.ŒΩ, b.D_inject\n",
        "\tstates = [rand(T(s, a)) for s in b.states]\n",
        "\tweights = [O(a, s‚Ä≤, o) for s‚Ä≤ in states]\n",
        "\tw_mean = mean(weights)\n",
        "\tw_slow += Œ±_slow*(w_mean - w_slow)\n",
        "\tw_fast += Œ±_fast*(w_mean - w_fast)\n",
        "\tm = length(states)\n",
        "\tm_inject = round(Int, m * max(0, 1.0 - ŒΩ*w_fast / w_slow))\n",
        "\tD = SetCategorical(states, weights)\n",
        "\tstates = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))\n",
        "\tb.w_slow, b.w_fast = w_slow, w_fast\n",
        "\treturn AdaptiveInjectionParticleFilter(states,\n",
        "\t\tw_slow, w_fast, Œ±_slow, Œ±_fast, ŒΩ, D_inject)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 1\n",
        "struct ConditionalPlan\n",
        "    a        # action to take at root\n",
        "    subplans # dictionary mapping observations to subplans\n",
        "end\n",
        "\n",
        "ConditionalPlan(a) = ConditionalPlan(a, Dict())\n",
        "\n",
        "(œÄ::ConditionalPlan)() = œÄ.a\n",
        "(œÄ::ConditionalPlan)(o) = œÄ.subplans[o]\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 2\n",
        "ConditionalPlan(œÄ::Tuple) = ConditionalPlan(œÄ[1], Dict(k=>ConditionalPlan(v) for (k,v) in œÄ[2]))\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 3\n",
        "function lookahead(ùí´::POMDP, U, s, a)\n",
        "    ùíÆ, ùí™, T, O, R, Œ≥ = ùí´.ùíÆ, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    u‚Ä≤ = sum(T(s,a,s‚Ä≤)*sum(O(a,s‚Ä≤,o)*U(o,s‚Ä≤) for o in ùí™) for s‚Ä≤ in ùíÆ)\n",
        "    return R(s,a) + Œ≥*u‚Ä≤\n",
        "end\n",
        "\n",
        "function evaluate_plan(ùí´::POMDP, œÄ::ConditionalPlan, s)\n",
        "    U(o,s‚Ä≤) = evaluate_plan(ùí´, œÄ(o), s‚Ä≤)\n",
        "    return isempty(œÄ.subplans) ? ùí´.R(s,œÄ()) : lookahead(ùí´, U, s, œÄ())\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 4\n",
        "function alphavector(ùí´::POMDP, œÄ::ConditionalPlan)\n",
        "    return [evaluate_plan(ùí´, œÄ, s) for s in ùí´.ùíÆ]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 5\n",
        "struct AlphaVectorPolicy\n",
        "    ùí´ # POMDP problem\n",
        "    Œì # alpha vectors\n",
        "    a # actions associated with alpha vectors\n",
        "end\n",
        "\n",
        "function utility(œÄ::AlphaVectorPolicy, b)\n",
        "    return maximum(Œ±‚ãÖb for Œ± in œÄ.Œì)\n",
        "end\n",
        "\n",
        "function (œÄ::AlphaVectorPolicy)(b)\n",
        "    i = argmax([Œ±‚ãÖb for Œ± in œÄ.Œì])\n",
        "    return œÄ.a[i]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 6\n",
        "function lookahead(ùí´::POMDP, U, b::Vector, a)\n",
        "    ùíÆ, ùí™, T, O, R, Œ≥ = ùí´.ùíÆ, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    r = sum(R(s,a)*b[i] for (i,s) in enumerate(ùíÆ))\n",
        "    Posa(o,s,a) = sum(O(a,s‚Ä≤,o)*T(s,a,s‚Ä≤) for s‚Ä≤ in ùíÆ)\n",
        "    Poba(o,b,a) = sum(b[i]*Posa(o,s,a) for (i,s) in enumerate(ùíÆ))\n",
        "    return r + Œ≥*sum(Poba(o,b,a)*U(update(b, ùí´, a, o)) for o in ùí™)\n",
        "end\n",
        "\n",
        "function greedy(ùí´::POMDP, U, b::Vector)\n",
        "    u, a = findmax(a->lookahead(ùí´, U, b, a), ùí´.ùíú)\n",
        "    return (a=a, u=u)\n",
        "end\n",
        "\n",
        "struct LookaheadAlphaVectorPolicy\n",
        "    ùí´ # POMDP problem\n",
        "    Œì # alpha vectors\n",
        "end\n",
        "\n",
        "function utility(œÄ::LookaheadAlphaVectorPolicy, b)\n",
        "    return maximum(Œ±‚ãÖb for Œ± in œÄ.Œì)\n",
        "end\n",
        "\n",
        "function greedy(œÄ, b)\n",
        "    U(b) = utility(œÄ, b)\n",
        "    return greedy(œÄ.ùí´, U, b)\n",
        "end\n",
        "\n",
        "(œÄ::LookaheadAlphaVectorPolicy)(b) = greedy(œÄ, b).a\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 7\n",
        "function find_maximal_belief(Œ±, Œì)\n",
        "\tm = length(Œ±)\n",
        "\tif isempty(Œì)\n",
        "\t\treturn fill(1/m, m) # arbitrary belief\n",
        "\tend\n",
        "\tmodel = Model(GLPK.Optimizer)\n",
        "\t@variable(model, Œ¥)\n",
        "\t@variable(model, b[i=1:m] ‚â• 0)\n",
        "\t@constraint(model, sum(b) == 1.0)\n",
        "\tfor a in Œì\n",
        "\t\t@constraint(model, (Œ±-a)‚ãÖb ‚â• Œ¥)\n",
        "\tend\n",
        "\t@objective(model, Max, Œ¥)\n",
        "\toptimize!(model)\n",
        "\treturn value(Œ¥) > 0 ? value.(b) : nothing\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 8\n",
        "function find_dominating(Œì)\n",
        "    n = length(Œì)\n",
        "    candidates, dominating = trues(n), falses(n)\n",
        "    while any(candidates)\n",
        "        i = findfirst(candidates)\n",
        "        b = find_maximal_belief(Œì[i], Œì[dominating])\n",
        "        if b === nothing\n",
        "            candidates[i] = false\n",
        "        else\n",
        "            k = argmax([candidates[j] ? b‚ãÖŒì[j] : -Inf for j in 1:n])\n",
        "            candidates[k], dominating[k] = false, true\n",
        "        end\n",
        "    end\n",
        "    return dominating\n",
        "end\n",
        "\n",
        "function prune(plans, Œì)\n",
        "    d = find_dominating(Œì)\n",
        "    return (plans[d], Œì[d])\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 9\n",
        "function value_iteration(ùí´::POMDP, k_max)\n",
        "    ùíÆ, ùíú, R = ùí´.ùíÆ, ùí´.ùíú, ùí´.R\n",
        "    plans = [ConditionalPlan(a) for a in ùíú]\n",
        "    Œì = [[R(s,a) for s in ùíÆ] for a in ùíú]\n",
        "    plans, Œì = prune(plans, Œì)\n",
        "    for k in 2:k_max\n",
        "        plans, Œì = expand(plans, Œì, ùí´)\n",
        "        plans, Œì = prune(plans, Œì)\n",
        "    end\n",
        "    return (plans, Œì)\n",
        "end\n",
        "\n",
        "function solve(M::ValueIteration, ùí´::POMDP)\n",
        "    plans, Œì = value_iteration(ùí´, M.k_max)\n",
        "    return LookaheadAlphaVectorPolicy(ùí´, Œì)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 10\n",
        "function ConditionalPlan(ùí´::POMDP, a, plans)\n",
        "    subplans = Dict(o=>œÄ for (o, œÄ) in zip(ùí´.ùí™, plans))\n",
        "    return ConditionalPlan(a, subplans)\n",
        "end\n",
        "\n",
        "function combine_lookahead(ùí´::POMDP, s, a, Œìo)\n",
        "    ùíÆ, ùí™, T, O, R, Œ≥ = ùí´.ùíÆ, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    U‚Ä≤(s‚Ä≤,i) = sum(O(a,s‚Ä≤,o)*Œ±[i] for (o,Œ±) in zip(ùí™,Œìo))\n",
        "    return R(s,a) + Œ≥*sum(T(s,a,s‚Ä≤)*U‚Ä≤(s‚Ä≤,i) for (i,s‚Ä≤) in enumerate(ùíÆ))\n",
        "end\n",
        "\n",
        "function combine_alphavector(ùí´::POMDP, a, Œìo)\n",
        "    return [combine_lookahead(ùí´, s, a, Œìo) for s in ùí´.ùíÆ]\n",
        "end\n",
        "\n",
        "function expand(plans, Œì, ùí´)\n",
        "    ùíÆ, ùíú, ùí™, T, O, R = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R\n",
        "    plans‚Ä≤, Œì‚Ä≤ = [], []\n",
        "    for a in ùíú\n",
        "        # iterate over all possible mappings from observations to plans\n",
        "        for inds in product([eachindex(plans) for o in ùí™]...)\n",
        "            œÄo = plans[[inds...]]\n",
        "            Œìo = Œì[[inds...]]\n",
        "            œÄ = ConditionalPlan(ùí´, a, œÄo)\n",
        "            Œ± = combine_alphavector(ùí´, a, Œìo)\n",
        "            push!(plans‚Ä≤, œÄ)\n",
        "            push!(Œì‚Ä≤, Œ±)\n",
        "        end\n",
        "    end\n",
        "    return (plans‚Ä≤, Œì‚Ä≤)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 1\n",
        "function alphavector_iteration(ùí´::POMDP, M, Œì)\n",
        "    for k in 1:M.k_max\n",
        "        Œì = update(ùí´, M, Œì)\n",
        "    end\n",
        "    return Œì\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 2\n",
        "struct QMDP\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(ùí´::POMDP, M::QMDP, Œì)\n",
        "    ùíÆ, ùíú, R, T, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T, ùí´.Œ≥\n",
        "    Œì‚Ä≤ = [[R(s,a) + Œ≥*sum(T(s,a,s‚Ä≤)*maximum(Œ±‚Ä≤[j] for Œ±‚Ä≤ in Œì)\n",
        "        for (j,s‚Ä≤) in enumerate(ùíÆ)) for s in ùíÆ] for a in ùíú]\n",
        "    return Œì‚Ä≤\n",
        "end\n",
        "\n",
        "function solve(M::QMDP, ùí´::POMDP)\n",
        "    Œì = [zeros(length(ùí´.ùíÆ)) for a in ùí´.ùíú]\n",
        "    Œì = alphavector_iteration(ùí´, M, Œì)\n",
        "    return AlphaVectorPolicy(ùí´, Œì, ùí´.ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 3\n",
        "struct FastInformedBound\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(ùí´::POMDP, M::FastInformedBound, Œì)\n",
        "    ùíÆ, ùíú, ùí™, R, T, O, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.R, ùí´.T, ùí´.O, ùí´.Œ≥\n",
        "    Œì‚Ä≤ = [[R(s, a) + Œ≥*sum(maximum(sum(O(a,s‚Ä≤,o)*T(s,a,s‚Ä≤)*Œ±‚Ä≤[j]\n",
        "        for (j,s‚Ä≤) in enumerate(ùíÆ)) for Œ±‚Ä≤ in Œì) for o in ùí™)\n",
        "        for s in ùíÆ] for a in ùíú]\n",
        "    return Œì‚Ä≤\n",
        "end\n",
        "\n",
        "function solve(M::FastInformedBound, ùí´::POMDP)\n",
        "    Œì = [zeros(length(ùí´.ùíÆ)) for a in ùí´.ùíú]\n",
        "    Œì = alphavector_iteration(ùí´, M, Œì)\n",
        "    return AlphaVectorPolicy(ùí´, Œì, ùí´.ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 4\n",
        "function baws_lowerbound(ùí´::POMDP)\n",
        "    ùíÆ, ùíú, R, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.Œ≥\n",
        "    r = maximum(minimum(R(s, a) for s in ùíÆ) for a in ùíú) / (1-Œ≥)\n",
        "    Œ± = fill(r, length(ùíÆ))\n",
        "    return Œ±\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 5\n",
        "function blind_lowerbound(ùí´, k_max)\n",
        "    ùíÆ, ùíú, T, R, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.T, ùí´.R, ùí´.Œ≥\n",
        "    Q(s,a,Œ±) = R(s,a) + Œ≥*sum(T(s,a,s‚Ä≤)*Œ±[j] for (j,s‚Ä≤) in enumerate(ùíÆ))\n",
        "    Œì = [baws_lowerbound(ùí´) for a in ùíú]\n",
        "    for k in 1:k_max\n",
        "        Œì = [[Q(s,a,Œ±) for s in ùíÆ] for (Œ±,a) in zip(Œì, ùíú)]\n",
        "    end\n",
        "    return Œì\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 6\n",
        "function backup(ùí´::POMDP, Œì, b)\n",
        "    ùíÆ, ùíú, ùí™, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.Œ≥\n",
        "    R, T, O = ùí´.R, ùí´.T, ùí´.O\n",
        "    Œìa = []\n",
        "    for a in ùíú\n",
        "        Œìao = []\n",
        "        for o in ùí™\n",
        "            b‚Ä≤ = update(b, ùí´, a, o)\n",
        "            push!(Œìao, argmax(Œ±->Œ±‚ãÖb‚Ä≤, Œì))\n",
        "        end\n",
        "        Œ± = [R(s, a) + Œ≥*sum(sum(T(s, a, s‚Ä≤)*O(a, s‚Ä≤, o)*Œìao[i][j]\n",
        "            for (j,s‚Ä≤) in enumerate(ùíÆ)) for (i,o) in enumerate(ùí™))\n",
        "            for s in ùíÆ]\n",
        "        push!(Œìa, Œ±)\n",
        "    end\n",
        "    return argmax(Œ±->Œ±‚ãÖb, Œìa)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 7\n",
        "struct PointBasedValueIteration\n",
        "    B     # set of belief points\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(ùí´::POMDP, M::PointBasedValueIteration, Œì)\n",
        "    return [backup(ùí´, Œì, b) for b in M.B]\n",
        "end\n",
        "\n",
        "function solve(M::PointBasedValueIteration, ùí´)\n",
        "    Œì = fill(baws_lowerbound(ùí´), length(ùí´.ùíú))\n",
        "    Œì = alphavector_iteration(ùí´, M, Œì)\n",
        "    return LookaheadAlphaVectorPolicy(ùí´, Œì)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 8\n",
        "struct RandomizedPointBasedValueIteration\n",
        "    B     # set of belief points\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(ùí´::POMDP, M::RandomizedPointBasedValueIteration, Œì)\n",
        "    Œì‚Ä≤, B‚Ä≤ = [], copy(M.B)\n",
        "    while !isempty(B‚Ä≤)\n",
        "        b = rand(B‚Ä≤)\n",
        "        Œ± = argmax(Œ±->Œ±‚ãÖb, Œì)\n",
        "        Œ±‚Ä≤ = backup(ùí´, Œì, b)\n",
        "        if Œ±‚Ä≤‚ãÖb ‚â• Œ±‚ãÖb\n",
        "            push!(Œì‚Ä≤, Œ±‚Ä≤)\n",
        "        else\n",
        "            push!(Œì‚Ä≤, Œ±)\n",
        "        end\n",
        "        filter!(b->maximum(Œ±‚ãÖb for Œ± in Œì‚Ä≤) <\n",
        "            maximum(Œ±‚ãÖb for Œ± in Œì), B‚Ä≤)\n",
        "    end\n",
        "    return Œì‚Ä≤\n",
        "end\n",
        "\n",
        "function solve(M::RandomizedPointBasedValueIteration, ùí´)\n",
        "    Œì = [baws_lowerbound(ùí´)]\n",
        "    Œì = alphavector_iteration(ùí´, M, Œì)\n",
        "    return LookaheadAlphaVectorPolicy(ùí´, Œì)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 9\n",
        "struct SawtoothPolicy\n",
        "    ùí´ # POMDP problem\n",
        "    V # dictionary mapping beliefs to utilities\n",
        "end\n",
        "\n",
        "function basis(ùí´)\n",
        "    n = length(ùí´.ùíÆ)\n",
        "    e(i) = [j == i ? 1.0 : 0.0 for j in 1:n]\n",
        "    return [e(i) for i in 1:n]\n",
        "end\n",
        "\n",
        "function utility(œÄ::SawtoothPolicy, b)\n",
        "    ùí´, V = œÄ.ùí´, œÄ.V\n",
        "    if haskey(V, b)\n",
        "        return V[b]\n",
        "    end\n",
        "    n = length(ùí´.ùíÆ)\n",
        "    E = basis(ùí´)\n",
        "    u = sum(V[E[i]] * b[i] for i in 1:n)\n",
        "    for (b‚Ä≤, u‚Ä≤) in V\n",
        "        if b‚Ä≤ ‚àâ E\n",
        "            i = argmax([norm(b-e, 1) - norm(b‚Ä≤-e, 1) for e in E])\n",
        "            w = [norm(b - e, 1) for e in E]\n",
        "            w[i] = norm(b - b‚Ä≤, 1)\n",
        "            w /= sum(w)\n",
        "            w = [1 - wi for wi in w]\n",
        "            Œ± = [V[e] for e in E]\n",
        "            Œ±[i] = u‚Ä≤\n",
        "            u = min(u, w‚ãÖŒ±)\n",
        "        end\n",
        "    end\n",
        "    return u\n",
        "end\n",
        "\n",
        "(œÄ::SawtoothPolicy)(b) = greedy(œÄ, b).a\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 10\n",
        "struct SawtoothIteration\n",
        "    V     # initial mapping from beliefs to utilities\n",
        "    B     # beliefs to compute values including those in V map\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::SawtoothIteration, ùí´::POMDP)\n",
        "    E = basis(ùí´)\n",
        "    œÄ = SawtoothPolicy(ùí´, M.V)\n",
        "    for k in 1:M.k_max\n",
        "        V = Dict(b => (b ‚àà E ? M.V[b] : greedy(œÄ, b).u) for b in M.B)\n",
        "        œÄ = SawtoothPolicy(ùí´, V)\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 11\n",
        "function randstep(ùí´::POMDP, b, a)\n",
        "\ts = rand(SetCategorical(ùí´.ùíÆ, b))\n",
        "\ts‚Ä≤, r, o = ùí´.TRO(s, a)\n",
        "\tb‚Ä≤ = update(b, ùí´, a, o)\n",
        "\treturn b‚Ä≤, r\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 12\n",
        "function random_belief_expansion(ùí´, B)\n",
        "\tB‚Ä≤ = copy(B)\n",
        "\tfor b in B\n",
        "\t\ta = rand(ùí´.ùíú)\n",
        "\t\tb‚Ä≤, r = randstep(ùí´, b, a)\n",
        "\t\tpush!(B‚Ä≤, b‚Ä≤)\n",
        "\tend\n",
        "\treturn unique!(B‚Ä≤)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 13\n",
        "function exploratory_belief_expansion(ùí´, B)\n",
        "    B‚Ä≤ = copy(B)\n",
        "    for b in B\n",
        "        best = (b=copy(b), d=0.0)\n",
        "        for a in ùí´.ùíú\n",
        "            b‚Ä≤, r = randstep(ùí´, b, a)\n",
        "            d = minimum(norm(b - b‚Ä≤, 1) for b in B‚Ä≤)\n",
        "            if d > best.d\n",
        "                best = (b=b‚Ä≤, d=d)\n",
        "            end\n",
        "        end\n",
        "        push!(B‚Ä≤, best.b)\n",
        "    end\n",
        "    return unique!(B‚Ä≤)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 14\n",
        "function directed_belief_expansion(œÄ, b)\n",
        "    ùí´, ùíÆ, ùíú, ùí™ = œÄ.ùí´, œÄ.ùí´.ùíÆ, œÄ.ùí´.ùíú, œÄ.ùí´.ùí™\n",
        "    T, O, R, Œ≥ = œÄ.ùí´.T, œÄ.ùí´.O, œÄ.ùí´.R, œÄ.ùí´.Œ≥\n",
        "    n, Œì, Œ± = length(ùíÆ), œÄ.Œì, argmax(Œ± -> Œ±‚ãÖb, œÄ.Œì)\n",
        "\n",
        "    B = []\n",
        "    for (a, o) in Iterators.product(ùíú, ùí™)\n",
        "        b‚Ä≤ = update(b, ùí´, œÄ(b), o)\n",
        "        Œ±‚Ä≤ = argmax(Œ± -> Œ±‚ãÖb‚Ä≤, Œì)\n",
        "        backup(a‚Ä≤) = [sum(T(s, a‚Ä≤, s‚Ä≤) * O(a‚Ä≤, s‚Ä≤, o) * Œ±‚Ä≤[s‚Ä≤Index]\n",
        "                      for (s‚Ä≤Index, s‚Ä≤) in enumerate(ùíÆ)) for s in ùíÆ]\n",
        "        Œ≤ = backup(a) - backup(œÄ(b‚Ä≤))\n",
        "\n",
        "        model = Model(Ipopt.Optimizer)\n",
        "        @variable(model, belief[1:n] >= 0.0)\n",
        "        @objective(model, Max, belief‚ãÖŒ≤)\n",
        "        for Œ±‚Ä≤‚Ä≤ in Œì\n",
        "            @constraint(model, belief‚ãÖŒ± >= belief‚ãÖŒ±‚Ä≤‚Ä≤)\n",
        "        end\n",
        "        @constraint(model, sum(belief[s] for s in 1:n) == 1.0)\n",
        "        optimize!(model)\n",
        "\n",
        "        println(termination_status(model))\n",
        "        println(objective_value(model))\n",
        "        if (termination_status(model) in [MOI.OPTIMAL,MOI.LOCALLY_SOLVED,\n",
        "                    MOI.ALMOST_OPTIMAL, MOI.ALMOST_LOCALLY_SOLVED] &&\n",
        "                objective_value(model) > 0.0)\n",
        "            push!(B, value.(belief))\n",
        "        end\n",
        "    end\n",
        "\n",
        "    return unique(B)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 15\n",
        "struct SawtoothHeuristicSearch\n",
        "    b     # initial belief\n",
        "    Œ¥     # gap threshold\n",
        "    d     # depth\n",
        "    k_max # maximum number of iterations\n",
        "    k_fib # number of iterations for fast informed bound\n",
        "end\n",
        "\n",
        "function explore!(M::SawtoothHeuristicSearch, ùí´, œÄhi, œÄlo, b, d=0)\n",
        "    ùíÆ, ùíú, ùí™, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.Œ≥\n",
        "    œµ(b‚Ä≤) = utility(œÄhi, b‚Ä≤) - utility(œÄlo, b‚Ä≤)\n",
        "    if d ‚â• M.d || œµ(b) ‚â§ M.Œ¥ / Œ≥^d\n",
        "        return\n",
        "    end\n",
        "    a = œÄhi(b)\n",
        "    o = argmax(o -> œµ(update(b, ùí´, a, o)), ùí™)\n",
        "    b‚Ä≤ = update(b, ùí´, a, o)\n",
        "    explore!(M, ùí´, œÄhi, œÄlo, b‚Ä≤, d+1)\n",
        "    if b‚Ä≤ ‚àâ basis(ùí´)\n",
        "        œÄhi.V[b‚Ä≤] = greedy(œÄhi, b‚Ä≤).u\n",
        "    end\n",
        "    push!(œÄlo.Œì, backup(ùí´, œÄlo.Œì, b‚Ä≤))\n",
        "end\n",
        "\n",
        "function solve(M::SawtoothHeuristicSearch, ùí´::POMDP)\n",
        "    œÄfib = solve(FastInformedBound(M.k_fib), ùí´)\n",
        "    Vhi = Dict(e => utility(œÄfib, e) for e in basis(ùí´))\n",
        "    œÄhi = SawtoothPolicy(ùí´, Vhi)\n",
        "    œÄlo = LookaheadAlphaVectorPolicy(ùí´, [baws_lowerbound(ùí´)])\n",
        "    for i in 1:M.k_max\n",
        "        explore!(M, ùí´, œÄhi, œÄlo, M.b)\n",
        "        if utility(œÄhi, M.b) - utility(œÄlo, M.b) < M.Œ¥\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "    return œÄlo\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 16\n",
        "struct FreudenthalTriangulation\n",
        "    n::Int # dimensionality\n",
        "    m::Int # granularity\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    vertices(T::FreudenthalTriangulation)\n",
        "Construct the list of Freudenthal vertices. The vertices are represented by a list of `n` dimensional vectors.\n",
        "\"\"\"\n",
        "function vertices(T::FreudenthalTriangulation)\n",
        "    V = Vector{Int}[]\n",
        "    v = Vector{Int}(undef, T.n)\n",
        "    v[1] = T.m\n",
        "    _vertices!(V, v, 2)\n",
        "    return V\n",
        "end\n",
        "\n",
        "function _vertices!(V::Vector{Vector{Int}}, v::Vector{Int}, i::Int)\n",
        "    n = length(v)\n",
        "    if i > n\n",
        "        push!(V, copy(v))\n",
        "        return\n",
        "    end\n",
        "    for k in 0 : v[i-1]\n",
        "        v[i] = k\n",
        "        _vertices!(V, v, i+1)\n",
        "    end\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    _freudenthal_simplex(x::Vector{Float64})\n",
        "Returns the list of vertices of the simplex of point `x` in the Freudenthal grid.\n",
        "\"\"\"\n",
        "function _freudenthal_simplex(x::Vector{Float64})\n",
        "    n = length(x)\n",
        "    V = Vector{Vector{Int}}(undef, n+1)\n",
        "    V[1] = floor.(Int, x)\n",
        "    d = x - V[1]\n",
        "    p = sortperm(d, rev=true)\n",
        "    for i in 2 : n+1\n",
        "        V[i] = copy(V[i-1])\n",
        "        V[i][p[i-1]] += 1\n",
        "    end\n",
        "    return V\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    _barycentric_coordinates(x::Vector{Float64}, V::Vector{Vector{Int}})\n",
        "Given a point `x` and its simplex `V` in the Freudenthal grid, returns the barycentric coordinates\n",
        "of `x` in the grid. `V` must be in the same order as provided by the output of `freudenthal_simplex`\n",
        "\"\"\"\n",
        "function _barycentric_coordinates(x::Vector{Float64}, V::Vector{Vector{Int}})\n",
        "    d = x - V[1]\n",
        "    p = sortperm(d, rev=true)\n",
        "    n = length(x)\n",
        "    Œª = Vector{Float64}(undef, n+1)\n",
        "    Œª[n+1] = d[p[n]]\n",
        "    for i in n:-1:2\n",
        "        Œª[i] = d[p[i-1]] - d[p[i]]\n",
        "    end\n",
        "    Œª[1] = 1.0 - sum(Œª[2:end])\n",
        "    return Œª\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    simplex(T::FreudenthalTriangulation, x::Vector{Float64})\n",
        "Given a point `x`, returns the simplex of the point `x` and the barycentric coordinates of `x` in the grid.\n",
        "\"\"\"\n",
        "function simplex(T::FreudenthalTriangulation, x::Vector{Float64})\n",
        "    V = _freudenthal_simplex(x)\n",
        "    return V, _barycentric_coordinates(x, V)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    _to_belief(x)\n",
        "Transform a point `x` in the Freudenthal space to a point in the belief space.\n",
        "`m` is the resolution of the Freudenthal grid.\n",
        "\"\"\"\n",
        "_to_belief(x) = (push!(x[1:end-1] - x[2:end], x[end]))./x[1]\n",
        "\n",
        "\"\"\"\n",
        "    _to_freudenthal(b, m::Int64)\n",
        "Transform a point `b` in the belief space to a point in the Freudenthal space.\n",
        "`m` is the resolution of the Freudenthal grid.\n",
        "\"\"\"\n",
        "_to_freudenthal(b, m::Int64) = [sum(b[k] for k in i : length(b))*m for i in 1 : length(b)]\n",
        "\n",
        "belief_vertices(T::FreudenthalTriangulation) = _to_belief.(vertices(T))\n",
        "\n",
        "function belief_simplex(T::FreudenthalTriangulation, b)\n",
        "    x = _to_freudenthal(b, T.m)\n",
        "    V, Œª = simplex(T, x)\n",
        "    B = _to_belief.(V)\n",
        "    valid = Œª .> ‚àöeps()\n",
        "    return B[valid], Œª[valid]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 17\n",
        "struct TriangulatedPolicy\n",
        "    ùí´ # POMDP problem\n",
        "    V # dictionary mapping beliefs to utilities\n",
        "    B # beliefs\n",
        "    T # Freudenthal triangulation\n",
        "end\n",
        "\n",
        "function TriangulatedPolicy(ùí´::POMDP, m)\n",
        "    T = FreudenthalTriangulation(length(ùí´.ùíÆ), m)\n",
        "    B = belief_vertices(T)\n",
        "    V = Dict(b => 0.0 for b in B)\n",
        "    return TriangulatedPolicy(ùí´, V, B, T)\n",
        "end\n",
        "\n",
        "function utility(œÄ::TriangulatedPolicy, b)\n",
        "    B, Œª = belief_simplex(œÄ.T, b)\n",
        "    return sum(Œªi*œÄ.V[b] for (Œªi, b) in zip(Œª, B))\n",
        "end\n",
        "\n",
        "(œÄ::TriangulatedPolicy)(b) = greedy(œÄ, b).a\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 18\n",
        "struct TriangulatedIteration\n",
        "    m     # granularity\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::TriangulatedIteration, ùí´)\n",
        "    œÄ = TriangulatedPolicy(ùí´, M.m)\n",
        "    U(b) = utility(œÄ, b)\n",
        "    for k in 1:M.k_max\n",
        "        U‚Ä≤ = [greedy(ùí´, U, b).u for b in œÄ.B]\n",
        "        for (b, u‚Ä≤) in zip(œÄ.B, U‚Ä≤)\n",
        "            œÄ.V[b] = u‚Ä≤\n",
        "        end\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 1\n",
        "struct BeliefExplorationPolicy\n",
        "\tùí´\n",
        "\tN::Dict\n",
        "\tQ::Dict\n",
        "\tc::Float64\n",
        "end\n",
        "\n",
        "function (œÄ::BeliefExplorationPolicy)(h)\n",
        "\tùíú, N, Q, c = œÄ.ùí´.ùíú, œÄ.N, œÄ.Q, œÄ.c\n",
        "\tNh = sum(get(N, (h,a), 0) for a in ùíú)\n",
        "\tbest = (a=nothing, u=-Inf)\n",
        "\tfor a in ùíú\n",
        "\t\tu = Q[(h,a)] + c*(N[(h,a)] == 0 ? Inf :\n",
        "\t\t\t\t\t\t\tsqrt(log(Nh)/N[(h,a)]))\n",
        "\t\tif u > best.u\n",
        "\t\t\tbest = (a=a, u=u)\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn best.a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 2\n",
        "struct HistoryMonteCarloTreeSearch\n",
        "\tùí´ # problem\n",
        "\tN # visit counts\n",
        "\tQ # action value estimates\n",
        "\td # depth\n",
        "\tm # number of simulations\n",
        "\tc # exploration constant\n",
        "\tU # value function estimate\n",
        "end\n",
        "\n",
        "function explore(œÄ::HistoryMonteCarloTreeSearch, h)\n",
        "    ùíú, N, Q, c = œÄ.ùí´.ùíú, œÄ.N, œÄ.Q, œÄ.c\n",
        "    Nh = sum(get(N, (h,a), 0) for a in ùíú)\n",
        "    return argmax(a->Q[(h,a)] + c*bonus(N[(h,a)], Nh), ùíú)\n",
        "end\n",
        "\n",
        "function simulate(œÄ::HistoryMonteCarloTreeSearch, s, h, d)\n",
        "    if d ‚â§ 0\n",
        "        return œÄ.U(s)\n",
        "    end\n",
        "    ùí´, N, Q, c = œÄ.ùí´, œÄ.N, œÄ.Q, œÄ.c\n",
        "    ùíÆ, ùíú, TRO, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.TRO, ùí´.Œ≥\n",
        "    if !haskey(N, (h, first(ùíú)))\n",
        "        for a in ùíú\n",
        "            N[(h,a)] = 0\n",
        "            Q[(h,a)] = 0.0\n",
        "        end\n",
        "        return œÄ.U(s)\n",
        "    end\n",
        "    a = explore(œÄ, h)\n",
        "    s‚Ä≤, r, o = TRO(s,a)\n",
        "    q = r + Œ≥*simulate(œÄ, s‚Ä≤, vcat(h, (a,o)), d-1)\n",
        "    N[(h,a)] += 1\n",
        "    Q[(h,a)] += (q-Q[(h,a)])/N[(h,a)]\n",
        "    return q\n",
        "end\n",
        "\n",
        "function (œÄ::HistoryMonteCarloTreeSearch)(b, h=[])\n",
        "    for i in 1:œÄ.m\n",
        "        s = rand(SetCategorical(œÄ.ùí´.ùíÆ, b))\n",
        "        simulate(œÄ, s, h, œÄ.d)\n",
        "    end\n",
        "    return argmax(a->œÄ.Q[(h,a)], œÄ.ùí´.ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 3\n",
        "struct DeterminizedParticle\n",
        "    s # state\n",
        "    i # scenario index\n",
        "    j # depth index\n",
        "end\n",
        "\n",
        "function successor(ùí´, Œ¶, œï, a)\n",
        "    ùíÆ, ùí™, T, O = ùí´.ùíÆ, ùí´.ùí™, ùí´.T, ùí´.O\n",
        "    p = 0.0\n",
        "    for (s‚Ä≤, o) in product(ùíÆ, ùí™)\n",
        "        p += T(œï.s, a, s‚Ä≤) * O(a, s‚Ä≤, o)\n",
        "        if p ‚â• Œ¶[œï.i, œï.j]\n",
        "            return (s‚Ä≤, o)\n",
        "        end\n",
        "    end\n",
        "    return last(ùíÆ), last(ùí™)\n",
        "end\n",
        "\n",
        "function possible_observations(ùí´, Œ¶, b, a)\n",
        "    ùí™ = []\n",
        "    for œï in b\n",
        "        s‚Ä≤, o = successor(ùí´, Œ¶, œï, a)\n",
        "        push!(ùí™, o)\n",
        "    end\n",
        "    return unique(ùí™)\n",
        "end\n",
        "\n",
        "function update(b, Œ¶, ùí´, a, o)\n",
        "    b‚Ä≤ = []\n",
        "    for œï in b\n",
        "        s‚Ä≤, o‚Ä≤ = successor(ùí´, Œ¶, œï, a)\n",
        "        if o == o‚Ä≤\n",
        "            push!(b‚Ä≤, DeterminizedParticle(s‚Ä≤, œï.i, œï.j + 1))\n",
        "        end\n",
        "    end\n",
        "    return b‚Ä≤\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 4\n",
        "struct DeterminizedSparseTreeSearch\n",
        "\tùí´ # problem\n",
        "    d # depth\n",
        "    Œ¶ # m√ód determinizing matrix\n",
        "    U # value function to use at leaf nodes\n",
        "end\n",
        "\n",
        "function determinized_sparse_tree_search(ùí´, b, d, Œ¶, U)\n",
        "    ùíÆ, ùíú, ùí™, T, R, O, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.T, ùí´.R, ùí´.O, ùí´.Œ≥\n",
        "    if d == 0\n",
        "        return (a=nothing, u=U(b))\n",
        "    end\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for a in ùíú\n",
        "        u = sum(R(œï.s, a) for œï in b) / length(b)\n",
        "        for o in possible_observations(ùí´, Œ¶, b, a)\n",
        "            Poba = sum(sum(O(a,s‚Ä≤,o)*T(œï.s,a,s‚Ä≤) for s‚Ä≤ in ùíÆ)\n",
        "                       for œï in b) / length(b)\n",
        "            b‚Ä≤ = update(b, Œ¶, ùí´, a, o)\n",
        "            u‚Ä≤ = determinized_sparse_tree_search(ùí´,b‚Ä≤,d-1,Œ¶,U).u\n",
        "            u += Œ≥*Poba*u‚Ä≤\n",
        "        end\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "function determinized_belief(b, ùí´, m)\n",
        "    particles = []\n",
        "    for i in 1:m\n",
        "        s = rand(SetCategorical(ùí´.ùíÆ, b))\n",
        "\t\tpush!(particles, DeterminizedParticle(s, i, 1))\n",
        "    end\n",
        "    return particles\n",
        "end\n",
        "\n",
        "function (œÄ::DeterminizedSparseTreeSearch)(b)\n",
        "\tparticles = determinized_belief(b, œÄ.ùí´, size(œÄ.Œ¶,1))\n",
        "    return determinized_sparse_tree_search(œÄ.ùí´,particles,œÄ.d,œÄ.Œ¶,œÄ.U).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 5\n",
        "struct GapHeuristicSearch\n",
        "    ùí´     # problem\n",
        "    Ulo   # lower bound on value function\n",
        "    Uhi   # upper bound on value function\n",
        "    Œ¥     # gap threshold\n",
        "    k_max # maximum number of simulations\n",
        "    d_max # maximum depth\n",
        "end\n",
        "\n",
        "function heuristic_search(œÄ::GapHeuristicSearch, Ulo, Uhi, b, d)\n",
        "    ùí´, Œ¥ = œÄ.ùí´, œÄ.Œ¥\n",
        "    ùíÆ, ùíú, ùí™, R, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.R, ùí´.Œ≥\n",
        "    B = Dict((a,o)=>update(b,ùí´,a,o) for (a,o) in product(ùíú,ùí™))\n",
        "    B = merge(B, Dict(()=>copy(b)))\n",
        "    for (ao, b‚Ä≤) in B\n",
        "        if !haskey(Uhi, b‚Ä≤)\n",
        "            Ulo[b‚Ä≤], Uhi[b‚Ä≤] = œÄ.Ulo(b‚Ä≤), œÄ.Uhi(b‚Ä≤)\n",
        "        end\n",
        "    end\n",
        "    if d == 0 || Uhi[b] - Ulo[b] ‚â§ Œ¥\n",
        "        return\n",
        "    end\n",
        "    a = argmax(a -> lookahead(ùí´,b‚Ä≤->Uhi[b‚Ä≤],b,a), ùíú)\n",
        "    o = argmax(o -> Uhi[B[(a, o)]] - Ulo[B[(a, o)]], ùí™)\n",
        "    b‚Ä≤ = update(b,ùí´,a,o)\n",
        "    heuristic_search(œÄ,Ulo,Uhi,b‚Ä≤,d-1)\n",
        "    Ulo[b] = maximum(lookahead(ùí´,b‚Ä≤->Ulo[b‚Ä≤],b,a) for a in ùíú)\n",
        "    Uhi[b] = maximum(lookahead(ùí´,b‚Ä≤->Uhi[b‚Ä≤],b,a) for a in ùíú)\n",
        "end\n",
        "\n",
        "function (œÄ::GapHeuristicSearch)(b)\n",
        "    ùí´, k_max, d_max, Œ¥ = œÄ.ùí´, œÄ.k_max, œÄ.d_max, œÄ.Œ¥\n",
        "    Ulo = Dict{Vector{Float64}, Float64}()\n",
        "    Uhi = Dict{Vector{Float64}, Float64}()\n",
        "    for i in 1:k_max\n",
        "        heuristic_search(œÄ, Ulo, Uhi, b, d_max)\n",
        "        if Uhi[b] - Ulo[b] < Œ¥\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "    return argmax(a -> lookahead(ùí´,b‚Ä≤->Ulo[b‚Ä≤],b,a), ùí´.ùíú)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 1\n",
        "mutable struct ControllerPolicy\n",
        "    ùí´ # problem\n",
        "    X # set of controller nodes\n",
        "    œà # action selection distribution\n",
        "    Œ∑ # successor selection distribution\n",
        "end\n",
        "\n",
        "function (œÄ::ControllerPolicy)(x)\n",
        "    ùíú, œà = œÄ.ùí´.ùíú, œÄ.œà\n",
        "    dist = [œà[x, a] for a in ùíú]\n",
        "    return rand(SetCategorical(ùíú, dist))\n",
        "end\n",
        "\n",
        "function update(œÄ::ControllerPolicy, x, a, o)\n",
        "    X, Œ∑ = œÄ.X, œÄ.Œ∑\n",
        "    dist = [Œ∑[x, a, o, x‚Ä≤] for x‚Ä≤ in X]\n",
        "    return rand(SetCategorical(X, dist))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 2\n",
        "function utility(œÄ::ControllerPolicy, U, x, s)\n",
        "    ùíÆ, ùíú, ùí™ = œÄ.ùí´.ùíÆ, œÄ.ùí´.ùíú, œÄ.ùí´.ùí™\n",
        "    T, O, R, Œ≥ = œÄ.ùí´.T, œÄ.ùí´.O, œÄ.ùí´.R, œÄ.ùí´.Œ≥\n",
        "    X, œà, Œ∑ = œÄ.X, œÄ.œà, œÄ.Œ∑\n",
        "    U‚Ä≤(a,s‚Ä≤,o) = sum(Œ∑[x,a,o,x‚Ä≤]*U[x‚Ä≤,s‚Ä≤] for x‚Ä≤ in X)\n",
        "    U‚Ä≤(a,s‚Ä≤) = T(s,a,s‚Ä≤)*sum(O(a,s‚Ä≤,o)*U‚Ä≤(a,s‚Ä≤,o) for o in ùí™)\n",
        "    U‚Ä≤(a) = R(s,a) + Œ≥*sum(U‚Ä≤(a,s‚Ä≤) for s‚Ä≤ in ùíÆ)\n",
        "    return sum(œà[x,a]*U‚Ä≤(a) for a in ùíú)\n",
        "end\n",
        "\n",
        "function iterative_policy_evaluation(œÄ::ControllerPolicy, k_max)\n",
        "    ùíÆ, X = œÄ.ùí´.ùíÆ, œÄ.X\n",
        "    U = Dict((x, s) => 0.0 for x in X, s in ùíÆ)\n",
        "    for k in 1:k_max\n",
        "        U = Dict((x, s) => utility(œÄ, U, x, s) for x in X, s in ùíÆ)\n",
        "    end\n",
        "    return U\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 3\n",
        "struct ControllerPolicyIteration\n",
        "    k_max    # number of iterations\n",
        "    eval_max # number of evaluation iterations\n",
        "end\n",
        "\n",
        "function solve(M::ControllerPolicyIteration, ùí´::POMDP)\n",
        "    ùíú, ùí™, k_max, eval_max = ùí´.ùíú, ùí´.ùí™, M.k_max, M.eval_max\n",
        "    X = [1]\n",
        "    œà = Dict((x, a) => 1.0 / length(ùíú) for x in X, a in ùíú)\n",
        "    Œ∑ = Dict((x, a, o, x‚Ä≤) => 1.0 for x in X, a in ùíú, o in ùí™, x‚Ä≤ in X)\n",
        "    œÄ = ControllerPolicy(ùí´, X, œà, Œ∑)\n",
        "    for i in 1:k_max\n",
        "        prevX = copy(œÄ.X)\n",
        "        U = iterative_policy_evaluation(œÄ, eval_max)\n",
        "        policy_improvement!(œÄ, U, prevX)\n",
        "        prune!(œÄ, U, prevX)\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "\n",
        "function policy_improvement!(œÄ::ControllerPolicy, U, prevX)\n",
        "    ùíÆ, ùíú, ùí™ = œÄ.ùí´.ùíÆ, œÄ.ùí´.ùíú, œÄ.ùí´.ùí™\n",
        "    X, œà, Œ∑ = œÄ.X, œÄ.œà, œÄ.Œ∑\n",
        "    repeatXùí™ = fill(X, length(ùí™))\n",
        "    assignùíúX‚Ä≤ = vec(collect(product(ùíú, repeatXùí™...)))\n",
        "    for ax‚Ä≤ in assignùíúX‚Ä≤\n",
        "        x, a = maximum(X) + 1, ax‚Ä≤[1]\n",
        "        push!(X, x)\n",
        "        successor(o) = ax‚Ä≤[findfirst(isequal(o), ùí™) + 1]\n",
        "        U‚Ä≤(o,s‚Ä≤) = U[successor(o), s‚Ä≤]\n",
        "        for s in ùíÆ\n",
        "            U[x, s] = lookahead(œÄ.ùí´, U‚Ä≤, s, a)\n",
        "        end\n",
        "        for a‚Ä≤ in ùíú\n",
        "            œà[x, a‚Ä≤] = a‚Ä≤ == a ? 1.0 : 0.0\n",
        "            for (o, x‚Ä≤) in product(ùí™, prevX)\n",
        "                Œ∑[x, a‚Ä≤, o, x‚Ä≤] = x‚Ä≤ == successor(o) ? 1.0 : 0.0\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    for (x, a, o, x‚Ä≤) in product(X, ùíú, ùí™, X)\n",
        "        if !haskey(Œ∑, (x, a, o, x‚Ä≤))\n",
        "            Œ∑[x, a, o, x‚Ä≤] = 0.0\n",
        "        end\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 4\n",
        "function prune!(œÄ::ControllerPolicy, U, prevX)\n",
        "    ùíÆ, ùíú, ùí™, X, œà, Œ∑ = œÄ.ùí´.ùíÆ, œÄ.ùí´.ùíú, œÄ.ùí´.ùí™, œÄ.X, œÄ.œà, œÄ.Œ∑\n",
        "    newX, removeX = setdiff(X, prevX), []\n",
        "    # prune dominated from previous nodes\n",
        "    dominated(x,x‚Ä≤) = all(U[x,s] ‚â§ U[x‚Ä≤,s] for s in ùíÆ)\n",
        "    for (x,x‚Ä≤) in product(prevX, newX)\n",
        "        if x‚Ä≤ ‚àâ removeX && dominated(x, x‚Ä≤)\n",
        "            for s in ùíÆ\n",
        "                U[x,s] = U[x‚Ä≤,s]\n",
        "            end\n",
        "            for a in ùíú\n",
        "                œà[x,a] = œà[x‚Ä≤,a]\n",
        "                for (o,x‚Ä≤‚Ä≤) in product(ùí™, X)\n",
        "                    Œ∑[x,a,o,x‚Ä≤‚Ä≤] = Œ∑[x‚Ä≤,a,o,x‚Ä≤‚Ä≤]\n",
        "                end\n",
        "            end\n",
        "            push!(removeX, x‚Ä≤)\n",
        "        end\n",
        "    end\n",
        "    # prune identical from previous nodes\n",
        "    identical_action(x,x‚Ä≤) = all(œà[x,a] ‚âà œà[x‚Ä≤,a] for a in ùíú)\n",
        "    identical_successor(x,x‚Ä≤) = all(Œ∑[x,a,o,x‚Ä≤‚Ä≤] ‚âà Œ∑[x‚Ä≤,a,o,x‚Ä≤‚Ä≤]\n",
        "            for a in ùíú, o in ùí™, x‚Ä≤‚Ä≤ in X)\n",
        "    identical(x,x‚Ä≤) = identical_action(x,x‚Ä≤) && identical_successor(x,x‚Ä≤)\n",
        "    for (x,x‚Ä≤) in product(prevX, newX)\n",
        "        if x‚Ä≤ ‚àâ removeX && identical(x,x‚Ä≤)\n",
        "            push!(removeX, x‚Ä≤)\n",
        "        end\n",
        "    end\n",
        "    # prune dominated from new nodes\n",
        "    for (x,x‚Ä≤) in product(X, newX)\n",
        "        if x‚Ä≤ ‚àâ removeX && dominated(x‚Ä≤,x) && x ‚â† x‚Ä≤\n",
        "            push!(removeX, x‚Ä≤)\n",
        "        end\n",
        "    end\n",
        "    # update controller\n",
        "    œÄ.X = setdiff(X, removeX)\n",
        "    œÄ.œà = Dict(k => v for (k,v) in œà if k[1] ‚àâ removeX)\n",
        "    œÄ.Œ∑ = Dict(k => v for (k,v) in Œ∑ if k[1] ‚àâ removeX)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 5\n",
        "struct NonlinearProgramming\n",
        "    b # initial belief\n",
        "    ‚Ñì # number of nodes\n",
        "end\n",
        "\n",
        "function tensorform(ùí´::POMDP)\n",
        "    ùíÆ, ùíú, ùí™, R, T, O = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.R, ùí´.T, ùí´.O\n",
        "    ùíÆ‚Ä≤ = eachindex(ùíÆ)\n",
        "    ùíú‚Ä≤ = eachindex(ùíú)\n",
        "    ùí™‚Ä≤ = eachindex(ùí™)\n",
        "    R‚Ä≤ = [R(s,a) for s in ùíÆ, a in ùíú]\n",
        "    T‚Ä≤ = [T(s,a,s‚Ä≤) for s in ùíÆ, a in ùíú, s‚Ä≤ in ùíÆ]\n",
        "    O‚Ä≤ = [O(a,s‚Ä≤,o) for a in ùíú, s‚Ä≤ in ùíÆ, o in ùí™]\n",
        "    return ùíÆ‚Ä≤, ùíú‚Ä≤, ùí™‚Ä≤, R‚Ä≤, T‚Ä≤, O‚Ä≤\n",
        "end\n",
        "\n",
        "function solve(M::NonlinearProgramming, ùí´::POMDP)\n",
        "    x1, X = 1, collect(1:M.‚Ñì)\n",
        "    ùí´, Œ≥, b = ùí´, ùí´.Œ≥, M.b\n",
        "    ùíÆ, ùíú, ùí™, R, T, O = tensorform(ùí´)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[X,ùíÆ])\n",
        "    @variable(model, œà[X,ùíú] ‚â• 0)\n",
        "    @variable(model, Œ∑[X,ùíú,ùí™,X] ‚â• 0)\n",
        "    @objective(model, Max, b‚ãÖU[x1,:])\n",
        "    @NLconstraint(model, [x=X,s=ùíÆ],\n",
        "        U[x,s] == (sum(œà[x,a]*(R[s,a] + Œ≥*sum(T[s,a,s‚Ä≤]*sum(O[a,s‚Ä≤,o]\n",
        "        *sum(Œ∑[x,a,o,x‚Ä≤]*U[x‚Ä≤,s‚Ä≤] for x‚Ä≤ in X)\n",
        "        for o in ùí™) for s‚Ä≤ in ùíÆ)) for a in ùíú)))\n",
        "    @constraint(model, [x=X], sum(œà[x,:]) == 1)\n",
        "    @constraint(model, [x=X,a=ùíú,o=ùí™], sum(Œ∑[x,a,o,:]) == 1)\n",
        "    optimize!(model)\n",
        "    œà‚Ä≤, Œ∑‚Ä≤ = value.(œà), value.(Œ∑)\n",
        "    return ControllerPolicy(ùí´, X,\n",
        "        Dict((x, ùí´.ùíú[a]) => œà‚Ä≤[x, a] for x in X, a in ùíú),\n",
        "        Dict((x, ùí´.ùíú[a], ùí´.ùí™[o], x‚Ä≤) => Œ∑‚Ä≤[x, a, o, x‚Ä≤]\n",
        "             for x in X, a in ùíú, o in ùí™, x‚Ä≤ in X))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 6\n",
        "struct ControllerGradient\n",
        "    b       # initial belief\n",
        "    ‚Ñì       # number of nodes\n",
        "    Œ±       # gradient step\n",
        "    k_max   # maximum iterations\n",
        "end\n",
        "\n",
        "function solve(M::ControllerGradient, ùí´::POMDP)\n",
        "    ùíú, ùí™, ‚Ñì, k_max = ùí´.ùíú, ùí´.ùí™, M.‚Ñì, M.k_max\n",
        "    X = collect(1:‚Ñì)\n",
        "    œà = Dict((x, a) => rand() for x in X, a in ùíú)\n",
        "    Œ∑ = Dict((x, a, o, x‚Ä≤) => rand() for x in X, a in ùíú, o in ùí™, x‚Ä≤ in X)\n",
        "    œÄ = ControllerPolicy(ùí´, X, œà, Œ∑)\n",
        "    for i in 1:k_max\n",
        "        improve!(œÄ, M, ùí´)\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "\n",
        "function improve!(œÄ::ControllerPolicy, M::ControllerGradient, ùí´::POMDP)\n",
        "    ùíÆ, ùíú, ùí™, X, x1, œà, Œ∑ = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, œÄ.X, 1, œÄ.œà, œÄ.Œ∑\n",
        "    n, m, z, b, ‚Ñì, Œ± = length(ùíÆ), length(ùíú), length(ùí™), M.b, M.‚Ñì, M.Œ±\n",
        "    ‚àÇU‚Ä≤‚àÇœà, ‚àÇU‚Ä≤‚àÇŒ∑ = gradient(œÄ, M, ùí´)\n",
        "    UIndex(x, s) = (s - 1) * ‚Ñì + (x - 1) + 1\n",
        "    E(U, x1, b) = sum(b[s]*U[UIndex(x1,s)] for s in 1:n)\n",
        "    œà‚Ä≤ = Dict((x, a) => 0.0 for x in X, a in ùíú)\n",
        "    Œ∑‚Ä≤ = Dict((x, a, o, x‚Ä≤) => 0.0 for x in X, a in ùíú, o in ùí™, x‚Ä≤ in X)\n",
        "    for x in X\n",
        "        œà‚Ä≤x = [œà[x, a] + Œ± * E(‚àÇU‚Ä≤‚àÇœà(x, a), x1, b) for a in ùíú]\n",
        "        œà‚Ä≤x = project_to_simplex(œà‚Ä≤x)\n",
        "        for (aIndex, a) in enumerate(ùíú)\n",
        "            œà‚Ä≤[x, a] = œà‚Ä≤x[aIndex]\n",
        "        end\n",
        "        for (a, o) in product(ùíú, ùí™)\n",
        "            Œ∑‚Ä≤x = [(Œ∑[x, a, o, x‚Ä≤] +\n",
        "                    Œ± * E(‚àÇU‚Ä≤‚àÇŒ∑(x, a, o, x‚Ä≤), x1, b)) for x‚Ä≤ in X]\n",
        "            Œ∑‚Ä≤x = project_to_simplex(Œ∑‚Ä≤x)\n",
        "            for (x‚Ä≤Index, x‚Ä≤) in enumerate(X)\n",
        "                Œ∑‚Ä≤[x, a, o, x‚Ä≤] = Œ∑‚Ä≤x[x‚Ä≤Index]\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    œÄ.œà, œÄ.Œ∑ = œà‚Ä≤, Œ∑‚Ä≤\n",
        "end\n",
        "\n",
        "function project_to_simplex(y)\n",
        "    u = sort(copy(y), rev=true)\n",
        "    i = maximum([j for j in eachindex(u)\n",
        "                 if u[j] + (1 - sum(u[1:j])) / j > 0.0])\n",
        "    Œ¥ = (1 - sum(u[j] for j = 1:i)) / i\n",
        "    return [max(y[j] + Œ¥, 0.0) for j in eachindex(u)]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 7\n",
        "function gradient(œÄ::ControllerPolicy, M::ControllerGradient, ùí´::POMDP)\n",
        "    ùíÆ, ùíú, ùí™, T, O, R, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    X, x1, œà, Œ∑ = œÄ.X, 1, œÄ.œà, œÄ.Œ∑\n",
        "    n, m, z = length(ùíÆ), length(ùíú), length(ùí™)\n",
        "    XùíÆ = vec(collect(product(X, ùíÆ)))\n",
        "    T‚Ä≤ = [sum(œà[x, a] * T(s, a, s‚Ä≤) * sum(O(a, s‚Ä≤, o) * Œ∑[x, a, o, x‚Ä≤]\n",
        "          for o in ùí™) for a in ùíú) for (x, s) in XùíÆ, (x‚Ä≤, s‚Ä≤) in XùíÆ]\n",
        "    R‚Ä≤ = [sum(œà[x, a] * R(s, a) for a in ùíú) for (x, s) in XùíÆ]\n",
        "    Z = 1.0I(length(XùíÆ)) - Œ≥ * T‚Ä≤\n",
        "    invZ = inv(Z)\n",
        "    ‚àÇZ‚àÇœà(hx, ha) = [x == hx ? (-Œ≥ * T(s, ha, s‚Ä≤)\n",
        "                    * sum(O(ha, s‚Ä≤, o) * Œ∑[hx, ha, o, x‚Ä≤]\n",
        "                          for o in ùí™)) : 0.0\n",
        "                    for (x, s) in XùíÆ, (x‚Ä≤, s‚Ä≤) in XùíÆ]\n",
        "    ‚àÇZ‚àÇŒ∑(hx, ha, ho, hx‚Ä≤) = [x == hx && x‚Ä≤ == hx‚Ä≤ ? (-Œ≥ * œà[hx, ha]\n",
        "                    * T(s, ha, s‚Ä≤) * O(ha, s‚Ä≤, ho)) : 0.0\n",
        "                 for (x, s) in XùíÆ, (x‚Ä≤, s‚Ä≤) in XùíÆ]\n",
        "    ‚àÇR‚Ä≤‚àÇœà(hx, ha) = [x == hx ? R(s, ha) : 0.0 for (x, s) in XùíÆ]\n",
        "    ‚àÇR‚Ä≤‚àÇŒ∑(hx, ha, ho, hx‚Ä≤) = [0.0 for (x, s) in XùíÆ]\n",
        "    ‚àÇU‚Ä≤‚àÇœà(hx, ha) = invZ * (‚àÇR‚Ä≤‚àÇœà(hx, ha) - ‚àÇZ‚àÇœà(hx, ha) * invZ * R‚Ä≤)\n",
        "    ‚àÇU‚Ä≤‚àÇŒ∑(hx, ha, ho, hx‚Ä≤) = invZ * (‚àÇR‚Ä≤‚àÇŒ∑(hx, ha, ho, hx‚Ä≤)\n",
        "                                - ‚àÇZ‚àÇŒ∑(hx, ha, ho, hx‚Ä≤) * invZ * R‚Ä≤)\n",
        "    return ‚àÇU‚Ä≤‚àÇœà, ‚àÇU‚Ä≤‚àÇŒ∑\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 1\n",
        "struct SimpleGame\n",
        "    Œ≥  # discount factor\n",
        "    ‚Ñê  # agents\n",
        "    ùíú  # joint action space\n",
        "    R  # joint reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 2\n",
        "struct SimpleGamePolicy\n",
        "    p # dictionary mapping actions to probabilities\n",
        "\n",
        "    function SimpleGamePolicy(p::Base.Generator)\n",
        "        return SimpleGamePolicy(Dict(p))\n",
        "    end\n",
        "\n",
        "    function SimpleGamePolicy(p::Dict)\n",
        "        vs = collect(values(p))\n",
        "        vs ./= sum(vs)\n",
        "        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))\n",
        "    end\n",
        "\n",
        "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
        "end\n",
        "\n",
        "(œÄi::SimpleGamePolicy)(ai) = get(œÄi.p, ai, 0.0)\n",
        "\n",
        "function (œÄi::SimpleGamePolicy)()\n",
        "    D = SetCategorical(collect(keys(œÄi.p)), collect(values(œÄi.p)))\n",
        "    return rand(D)\n",
        "end\n",
        "\n",
        "joint(X) = vec(collect(product(X...)))\n",
        "\n",
        "joint(œÄ, œÄi, i) = [i == j ? œÄi : œÄj for (j, œÄj) in enumerate(œÄ)]\n",
        "\n",
        "function utility(ùí´::SimpleGame, œÄ, i)\n",
        "    ùíú, R = ùí´.ùíú, ùí´.R\n",
        "    p(a) = prod(œÄj(aj) for (œÄj, aj) in zip(œÄ, a))\n",
        "    return sum(R(a)[i]*p(a) for a in joint(ùíú))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 3\n",
        "function best_response(ùí´::SimpleGame, œÄ, i)\n",
        "    U(ai) = utility(ùí´, joint(œÄ, SimpleGamePolicy(ai), i), i)\n",
        "    ai = argmax(U, ùí´.ùíú[i])\n",
        "    return SimpleGamePolicy(ai)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 4\n",
        "function softmax_response(ùí´::SimpleGame, œÄ, i, Œª)\n",
        "    ùíúi = ùí´.ùíú[i]\n",
        "    U(ai) = utility(ùí´, joint(œÄ, SimpleGamePolicy(ai), i), i)\n",
        "    return SimpleGamePolicy(ai => exp(Œª*U(ai)) for ai in ùíúi)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 5\n",
        "struct NashEquilibrium end\n",
        "\n",
        "function tensorform(ùí´::SimpleGame)\n",
        "    ‚Ñê, ùíú, R = ùí´.‚Ñê, ùí´.ùíú, ùí´.R\n",
        "    ‚Ñê‚Ä≤ = eachindex(‚Ñê)\n",
        "    ùíú‚Ä≤ = [eachindex(ùíú[i]) for i in ‚Ñê]\n",
        "    R‚Ä≤ = [R(a) for a in joint(ùíú)]\n",
        "    return ‚Ñê‚Ä≤, ùíú‚Ä≤, R‚Ä≤\n",
        "end\n",
        "\n",
        "function solve(M::NashEquilibrium, ùí´::SimpleGame)\n",
        "    ‚Ñê, ùíú, R = tensorform(ùí´)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[‚Ñê])\n",
        "    @variable(model, œÄ[i=‚Ñê, ùíú[i]] ‚â• 0)\n",
        "    @NLobjective(model, Min,\n",
        "        sum(U[i] - sum(prod(œÄ[j,a[j]] for j in ‚Ñê) * R[y][i]\n",
        "            for (y,a) in enumerate(joint(ùíú))) for i in ‚Ñê))\n",
        "    @NLconstraint(model, [i=‚Ñê, ai=ùíú[i]],\n",
        "        U[i] ‚â• sum(\n",
        "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : œÄ[j,a[j]] for j in ‚Ñê)\n",
        "            * R[y][i] for (y,a) in enumerate(joint(ùíú))))\n",
        "    @constraint(model, [i=‚Ñê], sum(œÄ[i,ai] for ai in ùíú[i]) == 1)\n",
        "    optimize!(model)\n",
        "    œÄi‚Ä≤(i) = SimpleGamePolicy(ùí´.ùíú[i][ai] => value(œÄ[i,ai]) for ai in ùíú[i])\n",
        "    return [œÄi‚Ä≤(i) for i in ‚Ñê]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 6\n",
        "mutable struct JointCorrelatedPolicy\n",
        "    p # dictionary mapping from joint actions to probabilities\n",
        "    JointCorrelatedPolicy(p::Base.Generator) = new(Dict(p))\n",
        "end\n",
        "\n",
        "(œÄ::JointCorrelatedPolicy)(a) = get(œÄ.p, a, 0.0)\n",
        "\n",
        "function (œÄ::JointCorrelatedPolicy)()\n",
        "    D = SetCategorical(collect(keys(œÄ.p)), collect(values(œÄ.p)))\n",
        "    return rand(D)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 7\n",
        "struct CorrelatedEquilibrium end\n",
        "\n",
        "function solve(M::CorrelatedEquilibrium, ùí´::SimpleGame)\n",
        "    ‚Ñê, ùíú, R = ùí´.‚Ñê, ùí´.ùíú, ùí´.R\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, œÄ[joint(ùíú)] ‚â• 0)\n",
        "    @objective(model, Max, sum(sum(œÄ[a]*R(a) for a in joint(ùíú))))\n",
        "    @constraint(model, [i=‚Ñê, ai=ùíú[i], ai‚Ä≤=ùíú[i]],\n",
        "        sum(R(a)[i]*œÄ[a] for a in joint(ùíú) if a[i]==ai)\n",
        "        ‚â• sum(R(joint(a,ai‚Ä≤,i))[i]*œÄ[a] for a in joint(ùíú) if a[i]==ai))\n",
        "    @constraint(model, sum(œÄ) == 1)\n",
        "    optimize!(model)\n",
        "    return JointCorrelatedPolicy(a => value(œÄ[a]) for a in joint(ùíú))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 8\n",
        "struct IteratedBestResponse\n",
        "    k_max # number of iterations\n",
        "    œÄ     # initial policy\n",
        "end\n",
        "\n",
        "function IteratedBestResponse(ùí´::SimpleGame, k_max)\n",
        "    œÄ = [SimpleGamePolicy(ai => 1.0 for ai in ùíúi) for ùíúi in ùí´.ùíú]\n",
        "    return IteratedBestResponse(k_max, œÄ)\n",
        "end\n",
        "\n",
        "function solve(M::IteratedBestResponse, ùí´)\n",
        "    œÄ = M.œÄ\n",
        "    for k in 1:M.k_max\n",
        "        œÄ = [best_response(ùí´, œÄ, i) for i in ùí´.‚Ñê]\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 9\n",
        "struct HierarchicalSoftmax\n",
        "    Œª # precision parameter\n",
        "    k # level\n",
        "    œÄ # initial policy\n",
        "end\n",
        "\n",
        "function HierarchicalSoftmax(ùí´::SimpleGame, Œª, k)\n",
        "    œÄ = [SimpleGamePolicy(ai => 1.0 for ai in ùíúi) for ùíúi in ùí´.ùíú]\n",
        "    return HierarchicalSoftmax(Œª, k, œÄ)\n",
        "end\n",
        "\n",
        "function solve(M::HierarchicalSoftmax, ùí´)\n",
        "    œÄ = M.œÄ\n",
        "    for k in 1:M.k\n",
        "        œÄ = [softmax_response(ùí´, œÄ, i, M.Œª) for i in ùí´.‚Ñê]\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 10\n",
        "function simulate(ùí´::SimpleGame, œÄ, k_max)\n",
        "    for k = 1:k_max\n",
        "        a = [œÄi() for œÄi in œÄ]\n",
        "        for œÄi in œÄ\n",
        "            update!(œÄi, a)\n",
        "        end\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 11\n",
        "mutable struct FictitiousPlay\n",
        "    ùí´  # simple game\n",
        "    i  # agent index\n",
        "    N  # array of action count dictionaries\n",
        "    œÄi # current policy\n",
        "end\n",
        "\n",
        "function FictitiousPlay(ùí´::SimpleGame, i)\n",
        "    N = [Dict(aj => 1 for aj in ùí´.ùíú[j]) for j in ùí´.‚Ñê]\n",
        "    œÄi = SimpleGamePolicy(ai => 1.0 for ai in ùí´.ùíú[i])\n",
        "    return FictitiousPlay(ùí´, i, N, œÄi)\n",
        "end\n",
        "\n",
        "(œÄi::FictitiousPlay)() = œÄi.œÄi()\n",
        "\n",
        "(œÄi::FictitiousPlay)(ai) = œÄi.œÄi(ai)\n",
        "\n",
        "function update!(œÄi::FictitiousPlay, a)\n",
        "    N, ùí´, ‚Ñê, i = œÄi.N, œÄi.ùí´, œÄi.ùí´.‚Ñê, œÄi.i\n",
        "    for (j, aj) in enumerate(a)\n",
        "        N[j][aj] += 1\n",
        "    end\n",
        "    p(j) = SimpleGamePolicy(aj => u/sum(values(N[j])) for (aj, u) in N[j])\n",
        "    œÄ = [p(j) for j in ‚Ñê]\n",
        "    œÄi.œÄi = best_response(ùí´, œÄ, i)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 12\n",
        "mutable struct GradientAscent\n",
        "    ùí´  # simple game\n",
        "    i  # agent index\n",
        "    t  # time step\n",
        "    œÄi # current policy\n",
        "end\n",
        "\n",
        "function GradientAscent(ùí´::SimpleGame, i)\n",
        "    uniform() = SimpleGamePolicy(ai => 1.0 for ai in ùí´.ùíú[i])\n",
        "    return GradientAscent(ùí´, i, 1, uniform())\n",
        "end\n",
        "\n",
        "(œÄi::GradientAscent)() = œÄi.œÄi()\n",
        "\n",
        "(œÄi::GradientAscent)(ai) = œÄi.œÄi(ai)\n",
        "\n",
        "function update!(œÄi::GradientAscent, a)\n",
        "    ùí´, ‚Ñê, ùíúi, i, t = œÄi.ùí´, œÄi.ùí´.‚Ñê, œÄi.ùí´.ùíú[œÄi.i], œÄi.i, œÄi.t\n",
        "    jointœÄ(ai) = [SimpleGamePolicy(j == i ? ai : a[j]) for j in ‚Ñê]\n",
        "    r = [utility(ùí´, jointœÄ(ai), i) for ai in ùíúi]\n",
        "    œÄ‚Ä≤ = [œÄi.œÄi(ai) for ai in ùíúi]\n",
        "    œÄ = project_to_simplex(œÄ‚Ä≤ + r / sqrt(t))\n",
        "    œÄi.t = t + 1\n",
        "    œÄi.œÄi = SimpleGamePolicy(ai => p for (ai, p) in zip(ùíúi, œÄ))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 1\n",
        "struct MG\n",
        "    Œ≥  # discount factor\n",
        "    ‚Ñê  # agents\n",
        "    ùíÆ  # state space\n",
        "    ùíú  # joint action space\n",
        "    T  # transition function\n",
        "    R  # joint reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 2\n",
        "struct MGPolicy\n",
        "    p # dictionary mapping states to simple game policies\n",
        "    MGPolicy(p::Base.Generator) = new(Dict(p))\n",
        "end\n",
        "\n",
        "(œÄi::MGPolicy)(s, ai) = œÄi.p[s](ai)\n",
        "(œÄi::SimpleGamePolicy)(s, ai) = œÄi(ai)\n",
        "\n",
        "probability(ùí´::MG, s, œÄ, a) = prod(œÄj(s, aj) for (œÄj, aj) in zip(œÄ, a))\n",
        "reward(ùí´::MG, s, œÄ, i) =\n",
        "    sum(ùí´.R(s,a)[i]*probability(ùí´,s,œÄ,a) for a in joint(ùí´.ùíú))\n",
        "transition(ùí´::MG, s, œÄ, s‚Ä≤) =\n",
        "    sum(ùí´.T(s,a,s‚Ä≤)*probability(ùí´,s,œÄ,a) for a in joint(ùí´.ùíú))\n",
        "\n",
        "function policy_evaluation(ùí´::MG, œÄ, i)\n",
        "    ùíÆ, ùíú, R, T, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T, ùí´.Œ≥\n",
        "    p(s,a) = prod(œÄj(s, aj) for (œÄj, aj) in zip(œÄ, a))\n",
        "    R‚Ä≤ = [sum(R(s,a)[i]*p(s,a) for a in joint(ùíú)) for s in ùíÆ]\n",
        "    T‚Ä≤ = [sum(T(s,a,s‚Ä≤)*p(s,a) for a in joint(ùíú)) for s in ùíÆ, s‚Ä≤ in ùíÆ]\n",
        "    return (I - Œ≥*T‚Ä≤)\\R‚Ä≤\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 3\n",
        "function best_response(ùí´::MG, œÄ, i)\n",
        "    ùíÆ, ùíú, R, T, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T, ùí´.Œ≥\n",
        "    T‚Ä≤(s,ai,s‚Ä≤) = transition(ùí´, s, joint(œÄ, SimpleGamePolicy(ai), i), s‚Ä≤)\n",
        "    R‚Ä≤(s,ai) = reward(ùí´, s, joint(œÄ, SimpleGamePolicy(ai), i), i)\n",
        "    œÄi = solve(MDP(Œ≥, ùíÆ, ùíú[i], T‚Ä≤, R‚Ä≤))\n",
        "    return MGPolicy(s => SimpleGamePolicy(œÄi(s)) for s in ùíÆ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 4\n",
        "function softmax_response(ùí´::MG, œÄ, i, Œª)\n",
        "    ùíÆ, ùíú, R, T, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T, ùí´.Œ≥\n",
        "    T‚Ä≤(s,ai,s‚Ä≤) = transition(ùí´, s, joint(œÄ, SimpleGamePolicy(ai), i), s‚Ä≤)\n",
        "    R‚Ä≤(s,ai) = reward(ùí´, s, joint(œÄ, SimpleGamePolicy(ai), i), i)\n",
        "    mdp = MDP(Œ≥, ùíÆ, joint(ùíú), T‚Ä≤, R‚Ä≤)\n",
        "    œÄi = solve(mdp)\n",
        "    Q(s,a) = lookahead(mdp, œÄi.U, s, a)\n",
        "    p(s) = SimpleGamePolicy(a => exp(Œª*Q(s,a)) for a in ùíú[i])\n",
        "    return MGPolicy(s => p(s) for s in ùíÆ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 5\n",
        "function tensorform(ùí´::MG)\n",
        "    ‚Ñê, ùíÆ, ùíú, R, T = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T\n",
        "    ‚Ñê‚Ä≤ = eachindex(‚Ñê)\n",
        "    ùíÆ‚Ä≤ = eachindex(ùíÆ)\n",
        "    ùíú‚Ä≤ = [eachindex(ùíú[i]) for i in ‚Ñê]\n",
        "    R‚Ä≤ = [R(s,a) for s in ùíÆ, a in joint(ùíú)]\n",
        "    T‚Ä≤ = [T(s,a,s‚Ä≤) for s in ùíÆ, a in joint(ùíú), s‚Ä≤ in ùíÆ]\n",
        "    return ‚Ñê‚Ä≤, ùíÆ‚Ä≤, ùíú‚Ä≤, R‚Ä≤, T‚Ä≤\n",
        "end\n",
        "\n",
        "function solve(M::NashEquilibrium, ùí´::MG)\n",
        "    ‚Ñê, ùíÆ, ùíú, R, T = tensorform(ùí´)\n",
        "    ùíÆ‚Ä≤, ùíú‚Ä≤, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.Œ≥\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[‚Ñê, ùíÆ])\n",
        "    @variable(model, œÄ[i=‚Ñê, ùíÆ, ai=ùíú[i]] ‚â• 0)\n",
        "    @NLobjective(model, Min,\n",
        "        sum(U[i,s] - sum(prod(œÄ[j,s,a[j]] for j in ‚Ñê)\n",
        "            * (R[s,y][i] + Œ≥*sum(T[s,y,s‚Ä≤]*U[i,s‚Ä≤] for s‚Ä≤ in ùíÆ))\n",
        "            for (y,a) in enumerate(joint(ùíú))) for i in ‚Ñê, s in ùíÆ))\n",
        "    @NLconstraint(model, [i=‚Ñê, s=ùíÆ, ai=ùíú[i]],\n",
        "        U[i,s] ‚â• sum(\n",
        "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : œÄ[j,s,a[j]] for j in ‚Ñê)\n",
        "            * (R[s,y][i] + Œ≥*sum(T[s,y,s‚Ä≤]*U[i,s‚Ä≤] for s‚Ä≤ in ùíÆ))\n",
        "            for (y,a) in enumerate(joint(ùíú))))\n",
        "    @constraint(model, [i=‚Ñê, s=ùíÆ], sum(œÄ[i,s,ai] for ai in ùíú[i]) == 1)\n",
        "    optimize!(model)\n",
        "    œÄ‚Ä≤ = value.(œÄ)\n",
        "    œÄi‚Ä≤(i,s) = SimpleGamePolicy(ùíú‚Ä≤[i][ai] => œÄ‚Ä≤[i,s,ai] for ai in ùíú[i])\n",
        "    œÄi‚Ä≤(i) = MGPolicy(ùíÆ‚Ä≤[s] => œÄi‚Ä≤(i,s) for s in ùíÆ)\n",
        "    return [œÄi‚Ä≤(i) for i in ‚Ñê]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 6\n",
        "function randstep(ùí´::MG, s, a)\n",
        "    s‚Ä≤ = rand(SetCategorical(ùí´.ùíÆ, [ùí´.T(s, a, s‚Ä≤) for s‚Ä≤ in ùí´.ùíÆ]))\n",
        "    r = ùí´.R(s,a)\n",
        "    return s‚Ä≤, r\n",
        "end\n",
        "\n",
        "function simulate(ùí´::MG, œÄ, k_max, b)\n",
        "    s = rand(b)\n",
        "    for k = 1:k_max\n",
        "        a = Tuple(œÄi(s)() for œÄi in œÄ)\n",
        "        s‚Ä≤, r = randstep(ùí´, s, a)\n",
        "        for œÄi in œÄ\n",
        "            update!(œÄi, s, a, s‚Ä≤)\n",
        "        end\n",
        "        s = s‚Ä≤\n",
        "    end\n",
        "    return œÄ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 7\n",
        "mutable struct MGFictitiousPlay\n",
        "    ùí´  # Markov game\n",
        "    i  # agent index\n",
        "    Qi # state-action value estimates\n",
        "    Ni # state-action counts\n",
        "end\n",
        "\n",
        "function MGFictitiousPlay(ùí´::MG, i)\n",
        "    ‚Ñê, ùíÆ, ùíú, R = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.R\n",
        "    Qi = Dict((s, a) => R(s, a)[i] for s in ùíÆ for a in joint(ùíú))\n",
        "    Ni = Dict((j, s, aj) => 1.0 for j in ‚Ñê for s in ùíÆ for aj in ùíú[j])\n",
        "    return MGFictitiousPlay(ùí´, i, Qi, Ni)\n",
        "end\n",
        "\n",
        "function (œÄi::MGFictitiousPlay)(s)\n",
        "    ùí´, i, Qi = œÄi.ùí´, œÄi.i, œÄi.Qi\n",
        "    ‚Ñê, ùíÆ, ùíú, T, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.T, ùí´.R, ùí´.Œ≥\n",
        "    œÄi‚Ä≤(i,s) = SimpleGamePolicy(ai => œÄi.Ni[i,s,ai] for ai in ùíú[i])\n",
        "    œÄi‚Ä≤(i) = MGPolicy(s => œÄi‚Ä≤(i,s) for s in ùíÆ)\n",
        "    œÄ = [œÄi‚Ä≤(i) for i in ‚Ñê]\n",
        "    U(s,œÄ) = sum(œÄi.Qi[s,a]*probability(ùí´,s,œÄ,a) for a in joint(ùíú))\n",
        "    Q(s,œÄ) = reward(ùí´,s,œÄ,i) + Œ≥*sum(transition(ùí´,s,œÄ,s‚Ä≤)*U(s‚Ä≤,œÄ)\n",
        "                                     for s‚Ä≤ in ùíÆ)\n",
        "    Q(ai) = Q(s, joint(œÄ, SimpleGamePolicy(ai), i))\n",
        "    ai = argmax(Q, ùí´.ùíú[œÄi.i])\n",
        "    return SimpleGamePolicy(ai)\n",
        "end\n",
        "\n",
        "function update!(œÄi::MGFictitiousPlay, s, a, s‚Ä≤)\n",
        "    ùí´, i, Qi = œÄi.ùí´, œÄi.i, œÄi.Qi\n",
        "    ‚Ñê, ùíÆ, ùíú, T, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.T, ùí´.R, ùí´.Œ≥\n",
        "    for (j,aj) in enumerate(a)\n",
        "        œÄi.Ni[j,s,aj] += 1\n",
        "    end\n",
        "    œÄi‚Ä≤(i,s) = SimpleGamePolicy(ai => œÄi.Ni[i,s,ai] for ai in ùíú[i])\n",
        "    œÄi‚Ä≤(i) = MGPolicy(s => œÄi‚Ä≤(i,s) for s in ùíÆ)\n",
        "    œÄ = [œÄi‚Ä≤(i) for i in ‚Ñê]\n",
        "    U(œÄ,s) = sum(œÄi.Qi[s,a]*probability(ùí´,s,œÄ,a) for a in joint(ùíú))\n",
        "    Q(s,a) = R(s,a)[i] + Œ≥*sum(T(s,a,s‚Ä≤)*U(œÄ,s‚Ä≤) for s‚Ä≤ in ùíÆ)\n",
        "    for a in joint(ùíú)\n",
        "        œÄi.Qi[s,a] = Q(s,a)\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 8\n",
        "mutable struct MGGradientAscent\n",
        "    ùí´  # Markov game\n",
        "    i  # agent index\n",
        "    t  # time step\n",
        "    Qi # state-action value estimates\n",
        "    œÄi # current policy\n",
        "end\n",
        "\n",
        "function MGGradientAscent(ùí´::MG, i)\n",
        "    ‚Ñê, ùíÆ, ùíú = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú\n",
        "    Qi = Dict((s, a) => 0.0 for s in ùíÆ, a in joint(ùíú))\n",
        "    uniform() = Dict(s => SimpleGamePolicy(ai => 1.0 for ai in ùí´.ùíú[i])\n",
        "                     for s in ùíÆ)\n",
        "    return MGGradientAscent(ùí´, i, 1, Qi, uniform())\n",
        "end\n",
        "\n",
        "function (œÄi::MGGradientAscent)(s)\n",
        "    ùíúi, t = œÄi.ùí´.ùíú[œÄi.i], œÄi.t\n",
        "    œµ = 1 / sqrt(t)\n",
        "    œÄi‚Ä≤(ai) = œµ/length(ùíúi) + (1-œµ)*œÄi.œÄi[s](ai)\n",
        "    return SimpleGamePolicy(ai => œÄi‚Ä≤(ai) for ai in ùíúi)\n",
        "end\n",
        "\n",
        "function update!(œÄi::MGGradientAscent, s, a, s‚Ä≤)\n",
        "    ùí´, i, t, Qi = œÄi.ùí´, œÄi.i, œÄi.t, œÄi.Qi\n",
        "    ‚Ñê, ùíÆ, ùíúi, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú[œÄi.i], ùí´.R, ùí´.Œ≥\n",
        "    jointœÄ(ai) = Tuple(j == i ? ai : a[j] for j in ‚Ñê)\n",
        "    Œ± = 1 / sqrt(t)\n",
        "    Qmax = maximum(Qi[s‚Ä≤, jointœÄ(ai)] for ai in ùíúi)\n",
        "    œÄi.Qi[s, a] += Œ± * (R(s, a)[i] + Œ≥ * Qmax - Qi[s, a])\n",
        "    u = [Qi[s, jointœÄ(ai)] for ai in ùíúi]\n",
        "    œÄ‚Ä≤ = [œÄi.œÄi[s](ai) for ai in ùíúi]\n",
        "    œÄ = project_to_simplex(œÄ‚Ä≤ + u / sqrt(t))\n",
        "    œÄi.t = t + 1\n",
        "    œÄi.œÄi[s] = SimpleGamePolicy(ai => p for (ai, p) in zip(ùíúi, œÄ))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 9\n",
        "mutable struct NashQLearning\n",
        "    ùí´ # Markov game\n",
        "    i # agent index\n",
        "    Q # state-action value estimates\n",
        "    N # history of actions performed\n",
        "end\n",
        "\n",
        "function NashQLearning(ùí´::MG, i)\n",
        "    ‚Ñê, ùíÆ, ùíú = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú\n",
        "    Q = Dict((j, s, a) => 0.0 for j in ‚Ñê, s in ùíÆ, a in joint(ùíú))\n",
        "    N = Dict((s, a) => 1.0 for s in ùíÆ, a in joint(ùíú))\n",
        "    return NashQLearning(ùí´, i, Q, N)\n",
        "end\n",
        "\n",
        "function (œÄi::NashQLearning)(s)\n",
        "    ùí´, i, Q, N = œÄi.ùí´, œÄi.i, œÄi.Q, œÄi.N\n",
        "    ‚Ñê, ùíÆ, ùíú, ùíúi, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.ùíú[œÄi.i], ùí´.Œ≥\n",
        "    M = NashEquilibrium()\n",
        "    ùí¢ = SimpleGame(Œ≥, ‚Ñê, ùíú, a -> [Q[j, s, a] for j in ‚Ñê])\n",
        "    œÄ = solve(M, ùí¢)\n",
        "    œµ = 1 / sum(N[s, a] for a in joint(ùíú))\n",
        "    œÄi‚Ä≤(ai) = œµ/length(ùíúi) + (1-œµ)*œÄ[i](ai)\n",
        "    return SimpleGamePolicy(ai => œÄi‚Ä≤(ai) for ai in ùíúi)\n",
        "end\n",
        "\n",
        "function update!(œÄi::NashQLearning, s, a, s‚Ä≤)\n",
        "    ùí´, ‚Ñê, ùíÆ, ùíú, R, Œ≥ = œÄi.ùí´, œÄi.ùí´.‚Ñê, œÄi.ùí´.ùíÆ, œÄi.ùí´.ùíú, œÄi.ùí´.R, œÄi.ùí´.Œ≥\n",
        "    i, Q, N = œÄi.i, œÄi.Q, œÄi.N\n",
        "    M = NashEquilibrium()\n",
        "    ùí¢ = SimpleGame(Œ≥, ‚Ñê, ùíú, a‚Ä≤ -> [Q[j, s‚Ä≤, a‚Ä≤] for j in ‚Ñê])\n",
        "    œÄ = solve(M, ùí¢)\n",
        "    œÄi.N[s, a] += 1\n",
        "    Œ± = 1 / sqrt(N[s, a])\n",
        "    for j in ‚Ñê\n",
        "        œÄi.Q[j,s,a] += Œ±*(R(s,a)[j] + Œ≥*utility(ùí¢,œÄ,j) - Q[j,s,a])\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 1\n",
        "struct POMG\n",
        "    Œ≥  # discount factor\n",
        "    ‚Ñê  # agents\n",
        "    ùíÆ  # state space\n",
        "    ùíú  # joint action space\n",
        "    ùí™  # joint observation space\n",
        "    T  # transition function\n",
        "    O  # joint observation function\n",
        "    R  # joint reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 2\n",
        "function lookahead(ùí´::POMG, U, s, a)\n",
        "    ùíÆ, ùí™, T, O, R, Œ≥ = ùí´.ùíÆ, joint(ùí´.ùí™), ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    u‚Ä≤ = sum(T(s,a,s‚Ä≤)*sum(O(a,s‚Ä≤,o)*U(o,s‚Ä≤) for o in ùí™) for s‚Ä≤ in ùíÆ)\n",
        "    return R(s,a) + Œ≥*u‚Ä≤\n",
        "end\n",
        "\n",
        "function evaluate_plan(ùí´::POMG, œÄ, s)\n",
        "    a = Tuple(œÄi() for œÄi in œÄ)\n",
        "    U(o,s‚Ä≤) = evaluate_plan(ùí´, [œÄi(oi) for (œÄi, oi) in zip(œÄ,o)], s‚Ä≤)\n",
        "    return isempty(first(œÄ).subplans) ? ùí´.R(s,a) : lookahead(ùí´, U, s, a)\n",
        "end\n",
        "\n",
        "function utility(ùí´::POMG, b, œÄ)\n",
        "    u = [evaluate_plan(ùí´, œÄ, s) for s in ùí´.ùíÆ]\n",
        "    return sum(bs * us for (bs, us) in zip(b, u))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 3\n",
        "struct POMGNashEquilibrium\n",
        "    b # initial belief\n",
        "    d # depth of conditional plans\n",
        "end\n",
        "\n",
        "function create_conditional_plans(ùí´, d)\n",
        "    ‚Ñê, ùíú, ùí™ = ùí´.‚Ñê, ùí´.ùíú, ùí´.ùí™\n",
        "    Œ† = [[ConditionalPlan(ai) for ai in ùíú[i]] for i in ‚Ñê]\n",
        "    for t in 1:d\n",
        "        Œ† = expand_conditional_plans(ùí´, Œ†)\n",
        "    end\n",
        "    return Œ†\n",
        "end\n",
        "\n",
        "function expand_conditional_plans(ùí´, Œ†)\n",
        "    ‚Ñê, ùíú, ùí™ = ùí´.‚Ñê, ùí´.ùíú, ùí´.ùí™\n",
        "    return [[ConditionalPlan(ai, Dict(oi => œÄi for oi in ùí™[i]))\n",
        "        for œÄi in Œ†[i] for ai in ùíú[i]] for i in ‚Ñê]\n",
        "end\n",
        "\n",
        "function solve(M::POMGNashEquilibrium, ùí´::POMG)\n",
        "    ‚Ñê, Œ≥, b, d = ùí´.‚Ñê, ùí´.Œ≥, M.b, M.d\n",
        "    Œ† = create_conditional_plans(ùí´, d)\n",
        "    U = Dict(œÄ => utility(ùí´, b, œÄ) for œÄ in joint(Œ†))\n",
        "    ùí¢ = SimpleGame(Œ≥, ‚Ñê, Œ†, œÄ -> U[œÄ])\n",
        "    œÄ = solve(NashEquilibrium(), ùí¢)\n",
        "    return Tuple(argmax(œÄi.p) for œÄi in œÄ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 4\n",
        "struct POMGDynamicProgramming\n",
        "    b   # initial belief\n",
        "    d   # depth of conditional plans\n",
        "end\n",
        "\n",
        "function solve(M::POMGDynamicProgramming, ùí´::POMG)\n",
        "    ‚Ñê, ùíÆ, ùíú, R, Œ≥, b, d = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.Œ≥, M.b, M.d\n",
        "    Œ† = [[ConditionalPlan(ai) for ai in ùíú[i]] for i in ‚Ñê]\n",
        "    for t in 1:d\n",
        "        Œ† = expand_conditional_plans(ùí´, Œ†)\n",
        "        prune_dominated!(Œ†, ùí´)\n",
        "    end\n",
        "    ùí¢ = SimpleGame(Œ≥, ‚Ñê, Œ†, œÄ -> utility(ùí´, b, œÄ))\n",
        "    œÄ = solve(NashEquilibrium(), ùí¢)\n",
        "    return Tuple(argmax(œÄi.p) for œÄi in œÄ)\n",
        "end\n",
        "\n",
        "function prune_dominated!(Œ†, ùí´::POMG)\n",
        "    done = false\n",
        "    while !done\n",
        "        done = true\n",
        "        for i in shuffle(ùí´.‚Ñê)\n",
        "            for œÄi in shuffle(Œ†[i])\n",
        "                if length(Œ†[i]) > 1 && is_dominated(ùí´, Œ†, i, œÄi)\n",
        "                    filter!(œÄi‚Ä≤ -> œÄi‚Ä≤ ‚â† œÄi, Œ†[i])\n",
        "                    done = false\n",
        "                    break\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "end\n",
        "\n",
        "function is_dominated(ùí´::POMG, Œ†, i, œÄi)\n",
        "    ‚Ñê, ùíÆ = ùí´.‚Ñê, ùí´.ùíÆ\n",
        "    jointŒ†noti = joint([Œ†[j] for j in ‚Ñê if j ‚â† i])\n",
        "    œÄ(œÄi‚Ä≤, œÄnoti) = [j==i ? œÄi‚Ä≤ : œÄnoti[j>i ? j-1 : j] for j in ‚Ñê]\n",
        "    Ui = Dict((œÄi‚Ä≤, œÄnoti, s) => evaluate_plan(ùí´, œÄ(œÄi‚Ä≤, œÄnoti), s)[i]\n",
        "              for œÄi‚Ä≤ in Œ†[i], œÄnoti in jointŒ†noti, s in ùíÆ)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, Œ¥)\n",
        "    @variable(model, b[jointŒ†noti, ùíÆ] ‚â• 0)\n",
        "    @objective(model, Max, Œ¥)\n",
        "    @constraint(model, [œÄi‚Ä≤=Œ†[i]],\n",
        "        sum(b[œÄnoti, s] * (Ui[œÄi‚Ä≤, œÄnoti, s] - Ui[œÄi, œÄnoti, s])\n",
        "        for œÄnoti in jointŒ†noti for s in ùíÆ) ‚â• Œ¥)\n",
        "    @constraint(model, sum(b) == 1)\n",
        "    optimize!(model)\n",
        "    return value(Œ¥) ‚â• 0\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 1\n",
        "struct DecPOMDP\n",
        "    Œ≥  # discount factor\n",
        "    ‚Ñê  # agents\n",
        "    ùíÆ  # state space\n",
        "    ùíú  # joint action space\n",
        "    ùí™  # joint observation space\n",
        "    T  # transition function\n",
        "    O  # joint observation function\n",
        "    R  # reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 2\n",
        "struct DecPOMDPDynamicProgramming\n",
        "    b   # initial belief\n",
        "    d   # depth of conditional plans\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPDynamicProgramming, ùí´::DecPOMDP)\n",
        "    ‚Ñê, ùíÆ, ùíú, ùí™, T, O, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    R‚Ä≤(s, a) = [R(s, a) for i in ‚Ñê]\n",
        "    ùí´‚Ä≤ = POMG(Œ≥, ‚Ñê, ùíÆ, ùíú, ùí™, T, O, R‚Ä≤)\n",
        "    M‚Ä≤ = POMGDynamicProgramming(M.b, M.d)\n",
        "    return solve(M‚Ä≤, ùí´‚Ä≤)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 3\n",
        "struct DecPOMDPIteratedBestResponse\n",
        "    b     # initial belief\n",
        "    d     # depth of conditional plans\n",
        "    k_max # number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPIteratedBestResponse, ùí´::DecPOMDP)\n",
        "    ‚Ñê, ùíÆ, ùíú, ùí™, T, O, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    b, d, k_max = M.b, M.d, M.k_max\n",
        "    R‚Ä≤(s, a) = [R(s, a) for i in ‚Ñê]\n",
        "    ùí´‚Ä≤ = POMG(Œ≥, ‚Ñê, ùíÆ, ùíú, ùí™, T, O, R‚Ä≤)\n",
        "    Œ† = create_conditional_plans(ùí´, d)\n",
        "    œÄ = [rand(Œ†[i]) for i in ‚Ñê]\n",
        "    for k in 1:k_max\n",
        "        for i in shuffle(‚Ñê)\n",
        "            œÄ‚Ä≤(œÄi) = Tuple(j == i ? œÄi : œÄ[j] for j in ‚Ñê)\n",
        "            Ui(œÄi) = utility(ùí´‚Ä≤, b, œÄ‚Ä≤(œÄi))[i]\n",
        "            œÄ[i] = argmax(Ui, Œ†[i])\n",
        "        end\n",
        "    end\n",
        "    return Tuple(œÄ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 4\n",
        "struct DecPOMDPHeuristicSearch\n",
        "    b     # initial belief\n",
        "    d     # depth of conditional plans\n",
        "    œÄ_max # number of policies\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPHeuristicSearch, ùí´::DecPOMDP)\n",
        "    ‚Ñê, ùíÆ, ùíú, ùí™, T, O, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    b, d, œÄ_max = M.b, M.d, M.œÄ_max\n",
        "    R‚Ä≤(s, a) = [R(s, a) for i in ‚Ñê]\n",
        "    ùí´‚Ä≤ = POMG(Œ≥, ‚Ñê, ùíÆ, ùíú, ùí™, T, O, R‚Ä≤)\n",
        "    Œ† = [[ConditionalPlan(ai) for ai in ùíú[i]] for i in ‚Ñê]\n",
        "    for t in 1:d\n",
        "        allŒ† = expand_conditional_plans(ùí´, Œ†)\n",
        "        Œ† = [[] for i in ‚Ñê]\n",
        "        for z in 1:œÄ_max\n",
        "            b‚Ä≤ = explore(M, ùí´, t)\n",
        "            œÄ = argmax(œÄ -> first(utility(ùí´‚Ä≤, b‚Ä≤, œÄ)), joint(allŒ†))\n",
        "            for i in ‚Ñê\n",
        "                push!(Œ†[i], œÄ[i])\n",
        "                filter!(œÄi -> œÄi != œÄ[i], allŒ†[i])\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return argmax(œÄ -> first(utility(ùí´‚Ä≤, b, œÄ)), joint(Œ†))\n",
        "end\n",
        "\n",
        "function explore(M::DecPOMDPHeuristicSearch, ùí´::DecPOMDP, t)\n",
        "    ‚Ñê, ùíÆ, ùíú, ùí™, T, O, R, Œ≥ = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
        "    b = copy(M.b)\n",
        "    b‚Ä≤ = similar(b)\n",
        "    s = rand(SetCategorical(ùíÆ, b))\n",
        "    for œÑ in 1:t\n",
        "        a = Tuple(rand(ùíúi) for ùíúi in ùíú)\n",
        "        s‚Ä≤ = rand(SetCategorical(ùíÆ, [T(s,a,s‚Ä≤) for s‚Ä≤ in ùíÆ]))\n",
        "        o = rand(SetCategorical(joint(ùí™), [O(a,s‚Ä≤,o) for o in joint(ùí™)]))\n",
        "        for (i‚Ä≤, s‚Ä≤) in enumerate(ùíÆ)\n",
        "            po = O(a, s‚Ä≤, o)\n",
        "            b‚Ä≤[i‚Ä≤] = po*sum(T(s,a,s‚Ä≤)*b[i] for (i,s) in enumerate(ùíÆ))\n",
        "        end\n",
        "        normalize!(b‚Ä≤, 1)\n",
        "        b, s = b‚Ä≤, s‚Ä≤\n",
        "    end\n",
        "    return b‚Ä≤\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 5\n",
        "struct DecPOMDPNonlinearProgramming\n",
        "    b # initial belief\n",
        "    ‚Ñì # number of nodes for each agent\n",
        "end\n",
        "\n",
        "function tensorform(ùí´::DecPOMDP)\n",
        "    ‚Ñê, ùíÆ, ùíú, ùí™, R, T, O = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.ùí™, ùí´.R, ùí´.T, ùí´.O\n",
        "    ‚Ñê‚Ä≤ = eachindex(‚Ñê)\n",
        "    ùíÆ‚Ä≤ = eachindex(ùíÆ)\n",
        "    ùíú‚Ä≤ = [eachindex(ùíúi) for ùíúi in ùíú]\n",
        "    ùí™‚Ä≤ = [eachindex(ùí™i) for ùí™i in ùí™]\n",
        "    R‚Ä≤ = [R(s,a) for s in ùíÆ, a in joint(ùíú)]\n",
        "    T‚Ä≤ = [T(s,a,s‚Ä≤) for s in ùíÆ, a in joint(ùíú), s‚Ä≤ in ùíÆ]\n",
        "    O‚Ä≤ = [O(a,s‚Ä≤,o) for a in joint(ùíú), s‚Ä≤ in ùíÆ, o in joint(ùí™)]\n",
        "    return ‚Ñê‚Ä≤, ùíÆ‚Ä≤, ùíú‚Ä≤, ùí™‚Ä≤, R‚Ä≤, T‚Ä≤, O‚Ä≤\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPNonlinearProgramming, ùí´::DecPOMDP)\n",
        "    ùí´, Œ≥, b = ùí´, ùí´.Œ≥, M.b\n",
        "    ‚Ñê, ùíÆ, ùíú, ùí™, R, T, O = tensorform(ùí´)\n",
        "    X = [collect(1:M.‚Ñì) for i in ‚Ñê]\n",
        "    jointX, jointùíú, jointùí™ = joint(X), joint(ùíú), joint(ùí™)\n",
        "    x1 = jointX[1]\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[jointX,ùíÆ])\n",
        "    @variable(model, œà[i=‚Ñê,X[i],ùíú[i]] ‚â• 0)\n",
        "    @variable(model, Œ∑[i=‚Ñê,X[i],ùíú[i],ùí™[i],X[i]] ‚â• 0)\n",
        "    @objective(model, Max, b‚ãÖU[x1,:])\n",
        "    @NLconstraint(model, [x=jointX,s=ùíÆ],\n",
        "        U[x,s] == (sum(prod(œà[i,x[i],a[i]] for i in ‚Ñê)\n",
        "                   *(R[s,y] + Œ≥*sum(T[s,y,s‚Ä≤]*sum(O[y,s‚Ä≤,z]\n",
        "                       *sum(prod(Œ∑[i,x[i],a[i],o[i],x‚Ä≤[i]] for i in ‚Ñê)\n",
        "                               *U[x‚Ä≤,s‚Ä≤] for x‚Ä≤ in jointX)\n",
        "                       for (z, o) in enumerate(jointùí™)) for s‚Ä≤ in ùíÆ))\n",
        "                   for (y, a) in enumerate(jointùíú))))\n",
        "    @constraint(model, [i=‚Ñê,xi=X[i]],\n",
        "                sum(œà[i,xi,ai] for ai in ùíú[i]) == 1)\n",
        "    @constraint(model, [i=‚Ñê,xi=X[i],ai=ùíú[i],oi=ùí™[i]],\n",
        "                sum(Œ∑[i,xi,ai,oi,xi‚Ä≤] for xi‚Ä≤ in X[i]) == 1)\n",
        "    optimize!(model)\n",
        "    œà‚Ä≤, Œ∑‚Ä≤ = value.(œà), value.(Œ∑)\n",
        "    return [ControllerPolicy(ùí´, X[i],\n",
        "            Dict((xi,ùí´.ùíú[i][ai]) => œà‚Ä≤[i,xi,ai]\n",
        "                 for xi in X[i], ai in ùíú[i]),\n",
        "            Dict((xi,ùí´.ùíú[i][ai],ùí´.ùí™[i][oi],xi‚Ä≤) => Œ∑‚Ä≤[i,xi,ai,oi,xi‚Ä≤]\n",
        "                 for xi in X[i], ai in ùíú[i], oi in ùí™[i], xi‚Ä≤ in X[i]))\n",
        "        for i in ‚Ñê]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 1\n",
        "struct Search\n",
        "    ùíÆ  # state space\n",
        "    ùíú  # valid action function\n",
        "    T  # transition function\n",
        "    R  # reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 2\n",
        "function forward_search(ùí´::Search, s, d, U)\n",
        "\tùíú, T, R = ùí´.ùíú(s), ùí´.T, ùí´.R\n",
        "\tif isempty(ùíú) || d ‚â§ 0\n",
        "\t\treturn (a=nothing, u=U(s))\n",
        "\tend\n",
        "\tbest = (a=nothing, u=-Inf)\n",
        "\tfor a in ùíú\n",
        "\t\ts‚Ä≤ = T(s,a)\n",
        "\t\tu = R(s,a) + forward_search(ùí´, s‚Ä≤, d-1, U).u\n",
        "\t\tif u > best.u\n",
        "\t\t\tbest = (a=a, u=u)\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 3\n",
        "function branch_and_bound(ùí´::Search, s, d, Ulo, Qhi)\n",
        "\tùíú, T, R = ùí´.ùíú(s), ùí´.T, ùí´.R\n",
        "\tif isempty(ùíú) || d ‚â§ 0\n",
        "\t\treturn (a=nothing, u=Ulo(s))\n",
        "\tend\n",
        "\tbest = (a=nothing, u=-Inf)\n",
        "\tfor a in sort(ùíú, by=a->Qhi(s,a), rev=true)\n",
        "\t\tif Qhi(s,a) ‚â§ best.u\n",
        "\t\t\treturn best # safe to prune\n",
        "\t\tend\n",
        "\t\tu = R(s,a) + branch_and_bound(ùí´,T(s,a),d-1,Ulo,Qhi).u\n",
        "\t\tif u > best.u\n",
        "\t\t\tbest = (a=a, u=u)\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 4\n",
        "function dynamic_programming(ùí´::Search, s, d, U, M=Dict())\n",
        "\tif haskey(M, (d,s))\n",
        "\t\treturn M[(d,s)]\n",
        "\tend\n",
        "\tùíú, T, R = ùí´.ùíú(s), ùí´.T, ùí´.R\n",
        "\tif isempty(ùíú) || d ‚â§ 0\n",
        "\t\tbest = (a=nothing, u=U(s))\n",
        "\telse\n",
        "\t\tbest = (a=first(ùíú), u=-Inf)\n",
        "\t\tfor a in ùíú\n",
        "\t\t\ts‚Ä≤ = T(s,a)\n",
        "\t\t\tu = R(s,a) + dynamic_programming(ùí´, s‚Ä≤, d-1, U, M).u\n",
        "\t\t\tif u > best.u\n",
        "\t\t\t\tbest = (a=a, u=u)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\tM[(d,s)] = best\n",
        "\treturn best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 5\n",
        "function heuristic_search(ùí´::Search, s, d, Uhi, U, M)\n",
        "\tif haskey(M, (d,s))\n",
        "\t\treturn M[(d,s)]\n",
        "\tend\n",
        "\tùíú, T, R = ùí´.ùíú(s), ùí´.T, ùí´.R\n",
        "\tif isempty(ùíú) || d ‚â§ 0\n",
        "\t\tbest = (a=nothing, u=U(s))\n",
        "\telse\n",
        "\t\tbest = (a=first(ùíú), u=-Inf)\n",
        "\t\tfor a in sort(ùíú, by=a->R(s,a) + Uhi(T(s,a)), rev=true)\n",
        "\t\t\tif R(s,a) + Uhi(T(s,a)) ‚â§ best.u\n",
        "\t\t\t\tbreak\n",
        "\t\t\tend\n",
        "\t\t\ts‚Ä≤ = T(s,a)\n",
        "\t\t\tu = R(s,a) + heuristic_search(ùí´, s‚Ä≤, d-1, Uhi, U, M).u\n",
        "\t\t\tif u > best.u\n",
        "\t\t\t\tbest = (a=a, u=u)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\tM[(d,s)] = best\n",
        "\treturn best\n",
        "end\n",
        "####################"
      ]
    }
  ]
}