{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUR2+KWdULbW4KrMyNvbtP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kessingtonosazee/GCP_Project_1/blob/master/julia2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgdW50taHzQl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This document is an automatically-generated file that contains all typeset code blocks from\n",
        "Algorithms for Decision Making by Mykel Kochenderfer, Tim Wheeler, and Kyle Wray. This book\n",
        "is available from the MIT Press. A PDF version is also available online at algorithmsbook.com.\n",
        "\n",
        "We share this content in the hopes that it helps you and makes the decision making algorithms\n",
        "more approachable and accessible. Thank you for reading!\n",
        "\n",
        "If you encounter any issues or have pressing comments, please file an issue at\n",
        "github.com/algorithmsbooks/decisionmaking\n",
        "\"\"\"\n",
        "\n",
        "#################### representation 1\n",
        "struct Variable\n",
        "\tname::Symbol\n",
        "\tr::Int # number of possible values\n",
        "end\n",
        "\n",
        "const Assignment = Dict{Symbol,Int}\n",
        "const FactorTable = Dict{Assignment,Float64}\n",
        "\n",
        "struct Factor\n",
        "\tvars::Vector{Variable}\n",
        "\ttable::FactorTable\n",
        "end\n",
        "\n",
        "variablenames(ϕ::Factor) = [var.name for var in ϕ.vars]\n",
        "\n",
        "select(a::Assignment, varnames::Vector{Symbol}) =\n",
        "\tAssignment(n=>a[n] for n in varnames)\n",
        "\n",
        "function assignments(vars::AbstractVector{Variable})\n",
        "    names = [var.name for var in vars]\n",
        "    return vec([Assignment(n=>v for (n,v) in zip(names, values))\n",
        "    \t\t\tfor values in product((1:v.r for v in vars)...)])\n",
        "end\n",
        "\n",
        "function normalize!(ϕ::Factor)\n",
        "\tz = sum(p for (a,p) in ϕ.table)\n",
        "\tfor (a,p) in ϕ.table\n",
        "\t\tϕ.table[a] = p/z\n",
        "\tend\n",
        "\treturn ϕ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### representation 2\n",
        "Dict{K,V}(a::NamedTuple) where K where V =\n",
        "    Dict{K,V}(n=>v for (n,v) in zip(keys(a), values(a)))\n",
        "Base.convert(::Type{Dict{K,V}}, a::NamedTuple) where K where V = Dict{K,V}(a)\n",
        "Base.isequal(a::Dict{<:Any,<:Any}, nt::NamedTuple) =\n",
        "    length(a) == length(nt) && all(a[n] == v for (n,v) in zip(keys(nt), values(nt)))\n",
        "####################\n",
        "\n",
        "#################### representation 3\n",
        "struct BayesianNetwork\n",
        "\tvars::Vector{Variable}\n",
        "    factors::Vector{Factor}\n",
        "\tgraph::SimpleDiGraph{Int64}\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### representation 4\n",
        "function probability(bn::BayesianNetwork, assignment)\n",
        "    subassignment(ϕ) = select(assignment, variablenames(ϕ))\n",
        "    probability(ϕ) = get(ϕ.table, subassignment(ϕ), 0.0)\n",
        "    return prod(probability(ϕ) for ϕ in bn.factors)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 1\n",
        "function Base.:*(ϕ::Factor, ψ::Factor)\n",
        "    ϕnames = variablenames(ϕ)\n",
        "    ψnames = variablenames(ψ)\n",
        "    ψonly = setdiff(ψ.vars, ϕ.vars)\n",
        "    table = FactorTable()\n",
        "    for (ϕa,ϕp) in ϕ.table\n",
        "        for a in assignments(ψonly)\n",
        "            a = merge(ϕa, a)\n",
        "            ψa = select(a, ψnames)\n",
        "            table[a] = ϕp * get(ψ.table, ψa, 0.0)\n",
        "        end\n",
        "    end\n",
        "    vars = vcat(ϕ.vars, ψonly)\n",
        "    return Factor(vars, table)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 2\n",
        "function marginalize(ϕ::Factor, name)\n",
        "\ttable = FactorTable()\n",
        "\tfor (a, p) in ϕ.table\n",
        "\t\ta′ = delete!(copy(a), name)\n",
        "\t\ttable[a′] = get(table, a′, 0.0) + p\n",
        "\tend\n",
        "\tvars = filter(v -> v.name != name, ϕ.vars)\n",
        "\treturn Factor(vars, table)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 3\n",
        "in_scope(name, ϕ) = any(name == v.name for v in ϕ.vars)\n",
        "\n",
        "function condition(ϕ::Factor, name, value)\n",
        "\tif !in_scope(name, ϕ)\n",
        "\t\treturn ϕ\n",
        "\tend\n",
        "\ttable = FactorTable()\n",
        "\tfor (a, p) in ϕ.table\n",
        "\t\tif a[name] == value\n",
        "\t\t\ttable[delete!(copy(a), name)] = p\n",
        "\t\tend\n",
        "\tend\n",
        "\tvars = filter(v -> v.name != name, ϕ.vars)\n",
        "\treturn Factor(vars, table)\n",
        "end\n",
        "\n",
        "function condition(ϕ::Factor, evidence)\n",
        "\tfor (name, value) in pairs(evidence)\n",
        "\t\tϕ = condition(ϕ, name, value)\n",
        "\tend\n",
        "\treturn ϕ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 4\n",
        "struct ExactInference end\n",
        "\n",
        "function infer(M::ExactInference, bn, query, evidence)\n",
        "\tϕ = prod(bn.factors)\n",
        "\tϕ = condition(ϕ, evidence)\n",
        "\tfor name in setdiff(variablenames(ϕ), query)\n",
        "\t\tϕ = marginalize(ϕ, name)\n",
        "\tend\n",
        "\treturn normalize!(ϕ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 5\n",
        "struct VariableElimination\n",
        "\tordering # array of variable indices\n",
        "end\n",
        "\n",
        "function infer(M::VariableElimination, bn, query, evidence)\n",
        "\tΦ = [condition(ϕ, evidence) for ϕ in bn.factors]\n",
        "\tfor i in M.ordering\n",
        "\t\tname = bn.vars[i].name\n",
        "\t\tif name ∉ query\n",
        "\t\t\tinds = findall(ϕ->in_scope(name, ϕ), Φ)\n",
        "\t\t\tif !isempty(inds)\n",
        "\t\t\t\tϕ = prod(Φ[inds])\n",
        "\t\t\t\tdeleteat!(Φ, inds)\n",
        "\t\t\t\tϕ = marginalize(ϕ, name)\n",
        "\t\t\t\tpush!(Φ, ϕ)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn normalize!(prod(Φ))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 6\n",
        "function topological_sort(G)\n",
        "\tG = deepcopy(G)\n",
        "\tordering = []\n",
        "\tparentless = filter(i -> isempty(inneighbors(G, i)), 1:nv(G))\n",
        "\twhile !isempty(parentless)\n",
        "\t\ti = pop!(parentless)\n",
        "\t\tpush!(ordering, i)\n",
        "\t\tfor j in copy(outneighbors(G, i))\n",
        "\t\t\trem_edge!(G, i, j)\n",
        "\t\t\tif isempty(inneighbors(G, j))\n",
        "\t\t\t\tpush!(parentless, j)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn ordering\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 7\n",
        "function Base.rand(ϕ::Factor)\n",
        "\ttot, p, w = 0.0, rand(), sum(values(ϕ.table))\n",
        "\tfor (a,v) in ϕ.table\n",
        "\t\ttot += v/w\n",
        "\t\tif tot >= p\n",
        "\t\t\treturn a\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn Assignment()\n",
        "end\n",
        "\n",
        "function Base.rand(bn::BayesianNetwork)\n",
        "\ta = Assignment()\n",
        "\tfor i in topological_sort(bn.graph)\n",
        "\t\tname, ϕ = bn.vars[i].name, bn.factors[i]\n",
        "\t\ta[name] = rand(condition(ϕ, a))[name]\n",
        "\tend\n",
        "\treturn a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 8\n",
        "struct DirectSampling\n",
        "\tm # number of samples\n",
        "end\n",
        "\n",
        "function infer(M::DirectSampling, bn, query, evidence)\n",
        "\ttable = FactorTable()\n",
        "\tfor i in 1:(M.m)\n",
        "\t\ta = rand(bn)\n",
        "\t\tif all(a[k] == v for (k,v) in pairs(evidence))\n",
        "\t\t\tb = select(a, query)\n",
        "\t\t\ttable[b] = get(table, b, 0) + 1\n",
        "\t\tend\n",
        "\tend\n",
        "\tvars = filter(v->v.name ∈ query, bn.vars)\n",
        "\treturn normalize!(Factor(vars, table))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 9\n",
        "struct LikelihoodWeightedSampling\n",
        "\tm # number of samples\n",
        "end\n",
        "\n",
        "function infer(M::LikelihoodWeightedSampling, bn, query, evidence)\n",
        "\ttable = FactorTable()\n",
        "\tordering = topological_sort(bn.graph)\n",
        "\tfor i in 1:(M.m)\n",
        "\t\ta, w = Assignment(), 1.0\n",
        "\t\tfor j in ordering\n",
        "\t\t\tname, ϕ = bn.vars[j].name, bn.factors[j]\n",
        "\t\t\tif haskey(evidence, name)\n",
        "                a[name] = evidence[name]\n",
        "\t\t\t\tw *= ϕ.table[select(a, variablenames(ϕ))]\n",
        "\t\t\telse\n",
        "\t\t\t\ta[name] = rand(condition(ϕ, a))[name]\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "        b = select(a, query)\n",
        "\t\ttable[b] = get(table, b, 0) + w\n",
        "\tend\n",
        "\tvars = filter(v->v.name ∈ query, bn.vars)\n",
        "\treturn normalize!(Factor(vars, table))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 10\n",
        "function blanket(bn, a, i)\n",
        "\tname = bn.vars[i].name\n",
        "\tval = a[name]\n",
        "\ta = delete!(copy(a), name)\n",
        "\tΦ = filter(ϕ -> in_scope(name, ϕ), bn.factors)\n",
        "\tϕ = prod(condition(ϕ, a) for ϕ in Φ)\n",
        "\treturn normalize!(ϕ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 11\n",
        "function update_gibbs_sample!(a, bn, evidence, ordering)\n",
        "    for i in ordering\n",
        "\t\tname = bn.vars[i].name\n",
        "\t\tif !haskey(evidence, name)\n",
        "            b = blanket(bn, a, i)\n",
        "            a[name] = rand(b)[name]\n",
        "\t\tend\n",
        "\tend\n",
        "end\n",
        "\n",
        "function gibbs_sample!(a, bn, evidence, ordering, m)\n",
        "\tfor j in 1:m\n",
        "\t\tupdate_gibbs_sample!(a, bn, evidence, ordering)\n",
        "\tend\n",
        "end\n",
        "\n",
        "struct GibbsSampling\n",
        "\tm_samples # number of samples to use\n",
        "\tm_burnin  # number of samples to discard during burn-in\n",
        "\tm_skip    # number of samples to skip for thinning\n",
        "\tordering  # array of variable indices\n",
        "end\n",
        "\n",
        "function infer(M::GibbsSampling, bn, query, evidence)\n",
        "\ttable = FactorTable()\n",
        "\ta = merge(rand(bn), evidence)\n",
        "\tgibbs_sample!(a, bn, evidence, M.ordering, M.m_burnin)\n",
        "\tfor i in 1:(M.m_samples)\n",
        "\t\tgibbs_sample!(a, bn, evidence, M.ordering, M.m_skip)\n",
        "\t\tb = select(a, query)\n",
        "\t\ttable[b] = get(table, b, 0) + 1\n",
        "\tend\n",
        "\tvars = filter(v->v.name ∈ query, bn.vars)\n",
        "\treturn normalize!(Factor(vars, table))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### inference 12\n",
        "function infer(D::MvNormal, query, evidencevars, evidence)\n",
        "    μ, Σ = D.μ, D.Σ.mat\n",
        "    b, μa, μb = evidence, μ[query], μ[evidencevars]\n",
        "    A = Σ[query,query]\n",
        "    B = Σ[evidencevars,evidencevars]\n",
        "    C = Σ[query,evidencevars]\n",
        "    μ = μ[query] + C * (B\\(b - μb))\n",
        "    Σ = A - C * (B \\ C')\n",
        "    return MvNormal(μ, Σ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### parameter-learning 1\n",
        "function sub2ind(siz, x)\n",
        "    k = vcat(1, cumprod(siz[1:end-1]))\n",
        "    return dot(k, x .- 1) + 1\n",
        "end\n",
        "\n",
        "function statistics(vars, G, D::Matrix{Int})\n",
        "    n = size(D, 1)\n",
        "    r = [vars[i].r for i in 1:n]\n",
        "    q = [prod([r[j] for j in inneighbors(G,i)]) for i in 1:n]\n",
        "    M = [zeros(q[i], r[i]) for i in 1:n]\n",
        "    for o in eachcol(D)\n",
        "        for i in 1:n\n",
        "            k = o[i]\n",
        "            parents = inneighbors(G,i)\n",
        "            j = 1\n",
        "            if !isempty(parents)\n",
        "                 j = sub2ind(r[parents], o[parents])\n",
        "            end\n",
        "            M[i][j,k] += 1.0\n",
        "        end\n",
        "    end\n",
        "    return M\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### parameter-learning 2\n",
        "function prior(vars, G)\n",
        "    n = length(vars)\n",
        "    r = [vars[i].r for i in 1:n]\n",
        "    q = [prod([r[j] for j in inneighbors(G,i)]) for i in 1:n]\n",
        "    return [ones(q[i], r[i]) for i in 1:n]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### parameter-learning 3\n",
        "gaussian_kernel(b) = x->pdf(Normal(0,b), x)\n",
        "\n",
        "function kernel_density_estimate(ϕ, O)\n",
        "\treturn x -> sum([ϕ(x - o) for o in O])/length(O)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 1\n",
        "function bayesian_score_component(M, α)\n",
        "    p =  sum(loggamma.(α + M))\n",
        "    p -= sum(loggamma.(α))\n",
        "    p += sum(loggamma.(sum(α,dims=2)))\n",
        "    p -= sum(loggamma.(sum(α,dims=2) + sum(M,dims=2)))\n",
        "    return p\n",
        "end\n",
        "\n",
        "function bayesian_score(vars, G, D)\n",
        "    n = length(vars)\n",
        "    M = statistics(vars, G, D)\n",
        "    α = prior(vars, G)\n",
        "    return sum(bayesian_score_component(M[i], α[i]) for i in 1:n)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 2\n",
        "struct K2Search\n",
        "    ordering::Vector{Int} # variable ordering\n",
        "end\n",
        "\n",
        "function fit(method::K2Search, vars, D)\n",
        "    G = SimpleDiGraph(length(vars))\n",
        "    for (k,i) in enumerate(method.ordering[2:end])\n",
        "        y = bayesian_score(vars, G, D)\n",
        "        while true\n",
        "            y_best, j_best = -Inf, 0\n",
        "            for j in method.ordering[1:k]\n",
        "                if !has_edge(G, j, i)\n",
        "                    add_edge!(G, j, i)\n",
        "                    y′ = bayesian_score(vars, G, D)\n",
        "                    if y′ > y_best\n",
        "                        y_best, j_best = y′, j\n",
        "                    end\n",
        "                    rem_edge!(G, j, i)\n",
        "                end\n",
        "            end\n",
        "            if y_best > y\n",
        "                y = y_best\n",
        "                add_edge!(G, j_best, i)\n",
        "            else\n",
        "                break\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return G\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 3\n",
        "struct LocalDirectedGraphSearch\n",
        "    G     # initial graph\n",
        "    k_max # number of iterations\n",
        "end\n",
        "\n",
        "function rand_graph_neighbor(G)\n",
        "    n = nv(G)\n",
        "    i = rand(1:n)\n",
        "    j = mod1(i + rand(2:n)-1, n)\n",
        "    G′ = copy(G)\n",
        "    has_edge(G, i, j) ? rem_edge!(G′, i, j) : add_edge!(G′, i, j)\n",
        "    return G′\n",
        "end\n",
        "\n",
        "function fit(method::LocalDirectedGraphSearch, vars, D)\n",
        "    G = method.G\n",
        "    y = bayesian_score(vars, G, D)\n",
        "    for k in 1:method.k_max\n",
        "        G′ = rand_graph_neighbor(G)\n",
        "        y′ = is_cyclic(G′) ? -Inf : bayesian_score(vars, G′, D)\n",
        "        if y′ > y\n",
        "            y, G = y′, G′\n",
        "        end\n",
        "    end\n",
        "    return G\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 4\n",
        "function are_markov_equivalent(G, H)\n",
        "\tif nv(G) != nv(H) || ne(G) != ne(H) ||\n",
        "\t\t!all(has_edge(H, e) || has_edge(H, reverse(e))\n",
        "\t\t\t\t\t\t\t\t\t\tfor e in edges(G))\n",
        "\t\treturn false\n",
        "\tend\n",
        "\tfor (I, J) in [(G,H), (H,G)]\n",
        "\t\tfor c in 1:nv(I)\n",
        "\t\t\tparents = inneighbors(I, c)\n",
        "\t\t \tfor (a, b) in subsets(parents, 2)\n",
        "\t\t \t\tif !has_edge(I, a, b) && !has_edge(I, b, a) &&\n",
        "\t\t \t\t   !(has_edge(J, a, c) && has_edge(J, b, c))\n",
        "\t\t \t\t    return false\n",
        "\t\t \t\tend\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\n",
        "\treturn true\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### structure-learning 5\n",
        "is_clique(P, nodes) = all(has_edge(P, a, b) || has_edge(P, b, a)\n",
        "\t\t\t\t\t\t\t\tfor (a, b) in subsets(nodes, 2))\n",
        "\n",
        "function pdag_to_dag_node!(P, G, removed_nodes)\n",
        "\tfor i in 1:nv(P)\n",
        "\t\tif !removed_nodes[i]\n",
        "\t\t\tincoming = Set(inneighbors(P, i))\n",
        "\t\t\toutgoing = Set(outneighbors(P, i))\n",
        "\t\t\tdirected_in = setdiff(incoming, outgoing)\n",
        "\t\t\tundirected = incoming ∩ outgoing\n",
        "\t\t\tdirected_out = setdiff(outgoing, incoming)\n",
        "\t\t\tif isempty(directed_out) && (isempty(undirected)\n",
        "\t\t\t      || is_clique(P, undirected ∪ directed_in))\n",
        "\t\t\t\tfor j in undirected\n",
        "\t\t\t\t\tadd_edge!(G, j, i)\n",
        "\t\t\t\tend\n",
        "\t\t\t\tfor j in incoming\n",
        "\t\t\t\t\trem_edge!(P, j, i)\n",
        "\t\t\t\t\trem_edge!(P, i, j)\n",
        "\t\t\t\tend\n",
        "\t\t\t\tremoved_nodes[i] = true\n",
        "\t\t\t\treturn true\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn false\n",
        "end\n",
        "\n",
        "function pdag_to_dag(P)\n",
        "\tG = SimpleDiGraph(nv(P))\n",
        "\tfor e in edges(P)\n",
        "\t\tif !has_edge(P, reverse(e))\n",
        "\t\t\tadd_edge!(G, e)\n",
        "\t\tend\n",
        "\tend\n",
        "\tremoved_nodes = falses(nv(P))\n",
        "\twhile !all(removed_nodes)\n",
        "\t\tif !pdag_to_dag_node!(P, G, removed_nodes)\n",
        "\t\t\terror(\"Cannot realize DAG for given PDAG\")\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn G\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### simple-decisions 1\n",
        "struct SimpleProblem\n",
        "    bn::BayesianNetwork\n",
        "    chance_vars::Vector{Variable}\n",
        "    decision_vars::Vector{Variable}\n",
        "    utility_vars::Vector{Variable}\n",
        "    utilities::Dict{Symbol, Vector{Float64}}\n",
        "end\n",
        "\n",
        "function solve(𝒫::SimpleProblem, evidence, M)\n",
        "    query = [var.name for var in 𝒫.utility_vars]\n",
        "    U(a) = sum(𝒫.utilities[uname][a[uname]] for uname in query)\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for assignment in assignments(𝒫.decision_vars)\n",
        "        evidence = merge(evidence, assignment)\n",
        "        ϕ = infer(M, 𝒫.bn, query, evidence)\n",
        "        u = sum(p*U(a) for (a, p) in ϕ.table)\n",
        "        if u > best.u\n",
        "            best = (a=assignment, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### simple-decisions 2\n",
        "function value_of_information(𝒫, query, evidence, M)\n",
        "    ϕ = infer(M, 𝒫.bn, query, evidence)\n",
        "    voi = -solve(𝒫, evidence, M).u\n",
        "    query_vars = filter(v->v.name ∈ query, 𝒫.chance_vars)\n",
        "    for o′ in assignments(query_vars)\n",
        "        oo′ = merge(evidence, o′)\n",
        "        p = ϕ.table[o′]\n",
        "        voi += p*solve(𝒫, oo′, M).u\n",
        "    end\n",
        "    return voi\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 1\n",
        "struct MDP\n",
        "    γ  # discount factor\n",
        "    𝒮  # state space\n",
        "    𝒜  # action space\n",
        "    T  # transition function\n",
        "    R  # reward function\n",
        "    TR # sample transition and reward\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 2\n",
        "function lookahead(𝒫::MDP, U, s, a)\n",
        "    𝒮, T, R, γ = 𝒫.𝒮, 𝒫.T, 𝒫.R, 𝒫.γ\n",
        "    return R(s,a) + γ*sum(T(s,a,s′)*U(s′) for s′ in 𝒮)\n",
        "end\n",
        "function lookahead(𝒫::MDP, U::Vector, s, a)\n",
        "    𝒮, T, R, γ = 𝒫.𝒮, 𝒫.T, 𝒫.R, 𝒫.γ\n",
        "    return R(s,a) + γ*sum(T(s,a,s′)*U[i] for (i,s′) in enumerate(𝒮))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 3\n",
        "function iterative_policy_evaluation(𝒫::MDP, π, k_max)\n",
        "    𝒮, T, R, γ = 𝒫.𝒮, 𝒫.T, 𝒫.R, 𝒫.γ\n",
        "    U = [0.0 for s in 𝒮]\n",
        "    for k in 1:k_max\n",
        "        U = [lookahead(𝒫, U, s, π(s)) for s in 𝒮]\n",
        "    end\n",
        "    return U\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 4\n",
        "function policy_evaluation(𝒫::MDP, π)\n",
        "\t𝒮, R, T, γ = 𝒫.𝒮, 𝒫.R, 𝒫.T, 𝒫.γ\n",
        "\tR′ = [R(s, π(s)) for s in 𝒮]\n",
        "\tT′ = [T(s, π(s), s′) for s in 𝒮, s′ in 𝒮]\n",
        "\treturn (I - γ*T′)\\R′\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 5\n",
        "struct ValueFunctionPolicy\n",
        "\t𝒫 # problem\n",
        "\tU # utility function\n",
        "end\n",
        "\n",
        "function greedy(𝒫::MDP, U, s)\n",
        "    u, a = findmax(a->lookahead(𝒫, U, s, a), 𝒫.𝒜)\n",
        "    return (a=a, u=u)\n",
        "end\n",
        "\n",
        "(π::ValueFunctionPolicy)(s) = greedy(π.𝒫, π.U, s).a\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 6\n",
        "struct PolicyIteration\n",
        "    π # initial policy\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::PolicyIteration, 𝒫::MDP)\n",
        "    π, 𝒮 = M.π, 𝒫.𝒮\n",
        "    for k = 1:M.k_max\n",
        "        U = policy_evaluation(𝒫, π)\n",
        "        π′ = ValueFunctionPolicy(𝒫, U)\n",
        "        if all(π(s) == π′(s) for s in 𝒮)\n",
        "            break\n",
        "        end\n",
        "        π = π′\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 7\n",
        "function backup(𝒫::MDP, U, s)\n",
        "\treturn maximum(lookahead(𝒫, U, s, a) for a in 𝒫.𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 8\n",
        "struct ValueIteration\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::ValueIteration, 𝒫::MDP)\n",
        "    U = [0.0 for s in 𝒫.𝒮]\n",
        "    for k = 1:M.k_max\n",
        "        U = [backup(𝒫, U, s) for s in 𝒫.𝒮]\n",
        "    end\n",
        "    return ValueFunctionPolicy(𝒫, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 9\n",
        "struct GaussSeidelValueIteration\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::GaussSeidelValueIteration, 𝒫::MDP)\n",
        "    U = [0.0 for s in 𝒫.𝒮]\n",
        "    for k = 1:M.k_max\n",
        "        for (i, s) in enumerate(𝒫.𝒮)\n",
        "            U[i] = backup(𝒫, U, s)\n",
        "        end\n",
        "    end\n",
        "    return ValueFunctionPolicy(𝒫, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 10\n",
        "struct LinearProgramFormulation end\n",
        "\n",
        "function tensorform(𝒫::MDP)\n",
        "    𝒮, 𝒜, R, T = 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.T\n",
        "    𝒮′ = eachindex(𝒮)\n",
        "    𝒜′ = eachindex(𝒜)\n",
        "    R′ = [R(s,a) for s in 𝒮, a in 𝒜]\n",
        "    T′ = [T(s,a,s′) for s in 𝒮, a in 𝒜, s′ in 𝒮]\n",
        "    return 𝒮′, 𝒜′, R′, T′\n",
        "end\n",
        "\n",
        "solve(𝒫::MDP) = solve(LinearProgramFormulation(), 𝒫)\n",
        "\n",
        "function solve(M::LinearProgramFormulation, 𝒫::MDP)\n",
        "    𝒮, 𝒜, R, T = tensorform(𝒫)\n",
        "    model = Model(GLPK.Optimizer)\n",
        "    @variable(model, U[𝒮])\n",
        "    @objective(model, Min, sum(U))\n",
        "    @constraint(model, [s=𝒮,a=𝒜], U[s] ≥ R[s,a] + 𝒫.γ*T[s,a,:]⋅U)\n",
        "    optimize!(model)\n",
        "    return ValueFunctionPolicy(𝒫, value.(U))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 11\n",
        "struct LinearQuadraticProblem\n",
        "    Ts # transition matrix with respect to state\n",
        "    Ta # transition matrix with respect to action\n",
        "    Rs # reward matrix with respect to state (negative semidefinite)\n",
        "    Ra # reward matrix with respect to action (negative definite)\n",
        "    h_max # horizon\n",
        "end\n",
        "\n",
        "function solve(𝒫::LinearQuadraticProblem)\n",
        "    Ts, Ta, Rs, Ra, h_max = 𝒫.Ts, 𝒫.Ta, 𝒫.Rs, 𝒫.Ra, 𝒫.h_max\n",
        "    V = zeros(size(Rs))\n",
        "    πs = Any[s -> zeros(size(Ta, 2))]\n",
        "    for h in 2:h_max\n",
        "        V = Ts'*(V - V*Ta*((Ta'*V*Ta + Ra) \\ Ta'*V))*Ts + Rs\n",
        "        L = -(Ta'*V*Ta + Ra) \\ Ta' * V * Ts\n",
        "        push!(πs, s -> L*s)\n",
        "    end\n",
        "    return πs\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 1\n",
        "struct ApproximateValueIteration\n",
        "    Uθ    # initial parameterized value function that supports fit!\n",
        "    S     # set of discrete states for performing backups\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::ApproximateValueIteration, 𝒫::MDP)\n",
        "    Uθ, S, k_max = M.Uθ, M.S, M.k_max\n",
        "    for k in 1:k_max\n",
        "        U = [backup(𝒫, Uθ, s) for s in S]\n",
        "        fit!(Uθ, S, U)\n",
        "    end\n",
        "    return ValueFunctionPolicy(𝒫, Uθ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 2\n",
        "mutable struct NearestNeighborValueFunction\n",
        "    k # number of neighbors\n",
        "    d # distance function d(s, s′)\n",
        "    S # set of discrete states\n",
        "    θ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (Uθ::NearestNeighborValueFunction)(s)\n",
        "    dists = [Uθ.d(s,s′) for s′ in Uθ.S]\n",
        "    ind = sortperm(dists)[1:Uθ.k]\n",
        "    return mean(Uθ.θ[i] for i in ind)\n",
        "end\n",
        "\n",
        "function fit!(Uθ::NearestNeighborValueFunction, S, U)\n",
        "    Uθ.θ = U\n",
        "    return Uθ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 3\n",
        "mutable struct LocallyWeightedValueFunction\n",
        "    k # kernel function k(s, s′)\n",
        "    S # set of discrete states\n",
        "    θ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (Uθ::LocallyWeightedValueFunction)(s)\n",
        "    w = normalize([Uθ.k(s,s′) for s′ in Uθ.S], 1)\n",
        "    return Uθ.θ ⋅ w\n",
        "end\n",
        "\n",
        "function fit!(Uθ::LocallyWeightedValueFunction, S, U)\n",
        "    Uθ.θ = U\n",
        "    return Uθ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 4\n",
        "mutable struct MultilinearValueFunction\n",
        "    o # position of lower-left corner\n",
        "    δ # vector of widths\n",
        "    θ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (Uθ::MultilinearValueFunction)(s)\n",
        "\to, δ, θ = Uθ.o, Uθ.δ, Uθ.θ\n",
        "    Δ = (s - o)./δ\n",
        "    # Multidimensional index of lower-left cell\n",
        "    i = min.(floor.(Int, Δ) .+ 1, size(θ) .- 1)\n",
        "    vertex_index = similar(i)\n",
        "    d = length(s)\n",
        "    u = 0.0\n",
        "    for vertex in 0:2^d-1\n",
        "        weight = 1.0\n",
        "        for j in 1:d\n",
        "            # Check whether jth bit is set\n",
        "            if vertex & (1 << (j-1)) > 0\n",
        "                vertex_index[j] = i[j] + 1\n",
        "                weight *= Δ[j] - i[j] + 1\n",
        "            else\n",
        "                vertex_index[j] = i[j]\n",
        "                weight *= i[j] - Δ[j]\n",
        "            end\n",
        "        end\n",
        "        u += θ[vertex_index...]*weight\n",
        "    end\n",
        "    return u\n",
        "end\n",
        "\n",
        "function fit!(Uθ::MultilinearValueFunction, S, U)\n",
        "    Uθ.θ = U\n",
        "    return Uθ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 5\n",
        "mutable struct SimplexValueFunction\n",
        "    o # position of lower-left corner\n",
        "    δ # vector of widths\n",
        "    θ # vector of values at states in S\n",
        "end\n",
        "\n",
        "function (Uθ::SimplexValueFunction)(s)\n",
        "\tΔ = (s - Uθ.o)./Uθ.δ\n",
        "\t# Multidimensional index of upper-right cell\n",
        "\ti = min.(floor.(Int, Δ) .+ 1, size(Uθ.θ) .- 1) .+ 1\n",
        "\tu = 0.0\n",
        "\ts′ = (s - (Uθ.o + Uθ.δ.*(i.-2))) ./ Uθ.δ\n",
        "\tp = sortperm(s′) # increasing order\n",
        "\tw_tot = 0.0\n",
        "\tfor j in p\n",
        "\t\tw = s′[j] - w_tot\n",
        "\t\tu += w*Uθ.θ[i...]\n",
        "\t\ti[j] -= 1\n",
        "\t\tw_tot += w\n",
        "\tend\n",
        "\tu += (1 - w_tot)*Uθ.θ[i...]\n",
        "\treturn u\n",
        "end\n",
        "\n",
        "function fit!(Uθ::SimplexValueFunction, S, U)\n",
        "    Uθ.θ = U\n",
        "    return Uθ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 6\n",
        "\tusing LinearAlgebra\n",
        "\tfunction regression(X, y, bases::Vector)\n",
        "\t    B = [b(x) for x in X, b in bases]\n",
        "\t    return pinv(B)*y\n",
        "\tend\n",
        "\tfunction regression(X, y, bases::Function)\n",
        "\t    B = Array{Float64}(undef, length(X), length(bases(X[1])))\n",
        "\t\tfor (i,x) in enumerate(X)\n",
        "\t\t\tB[i,:] = bases(x)\n",
        "\t\tend\n",
        "\t    return pinv(B)*y\n",
        "\tend\n",
        "\n",
        "\tpolynomial_bases_1d(i, k) = [x->x[i]^p for p in 0:k]\n",
        "\tfunction polynomial_bases(n, k)\n",
        "\t\tbases = [polynomial_bases_1d(i, k) for i in 1:n]\n",
        "\t\tterms = Function[]\n",
        "\t\tfor ks in product([0:k for i in 1:n]...)\n",
        "\t\t\tif sum(ks) ≤ k\n",
        "\t\t\t\tpush!(terms,\n",
        "\t\t\t\t\tx->prod(b[j+1](x) for (j,b) in zip(ks,bases)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\treturn terms\n",
        "\tend\n",
        "\n",
        "\tfunction sinusoidal_bases_1d(j, k, a, b)\n",
        "\t\tT = b[j] - a[j]\n",
        "\t\tbases = Function[x->1/2]\n",
        "\t\tfor i in 1:k\n",
        "\t\t\tpush!(bases, x->sin(2π*i*x[j]/T))\n",
        "\t\t\tpush!(bases, x->cos(2π*i*x[j]/T))\n",
        "\t\tend\n",
        "\t\treturn bases\n",
        "\tend\n",
        "\tfunction sinusoidal_bases(k, a, b)\n",
        "\t\tn = length(a)\n",
        "\t\tbases = [sinusoidal_bases_1d(i, k, a, b) for i in 1:n]\n",
        "\t\tterms = Function[]\n",
        "\t\tfor ks in product([0:2k for i in 1:n]...)\n",
        "\t\t\tpowers = [div(k+1,2) for k in ks]\n",
        "\t\t\tif sum(powers) ≤ k\n",
        "\t\t\t\tpush!(terms,\n",
        "\t\t\t\t\tx->prod(b[j+1](x) for (j,b) in zip(ks,bases)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\treturn terms\n",
        "\tend\n",
        "\n",
        "\tregress(β, ss, u) = regression(ss, u, β)\n",
        "####################\n",
        "\n",
        "#################### value-function-approximations 7\n",
        "mutable struct LinearRegressionValueFunction\n",
        "    β # basis vector function\n",
        "    θ # vector of parameters\n",
        "end\n",
        "\n",
        "function (Uθ::LinearRegressionValueFunction)(s)\n",
        "    return Uθ.β(s) ⋅ Uθ.θ\n",
        "end\n",
        "\n",
        "function fit!(Uθ::LinearRegressionValueFunction, S, U)\n",
        "    X = hcat([Uθ.β(s) for s in S]...)'\n",
        "    Uθ.θ = pinv(X)*U\n",
        "    return Uθ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 1\n",
        "struct RolloutLookahead\n",
        "\t𝒫 # problem\n",
        "\tπ # rollout policy\n",
        "\td # depth\n",
        "end\n",
        "\n",
        "randstep(𝒫::MDP, s, a) = 𝒫.TR(s, a)\n",
        "\n",
        "function rollout(𝒫, s, π, d)\n",
        "    ret = 0.0\n",
        "    for t in 1:d\n",
        "        a = π(s)\n",
        "        s, r = randstep(𝒫, s, a)\n",
        "        ret += 𝒫.γ^(t-1) * r\n",
        "    end\n",
        "    return ret\n",
        "end\n",
        "\n",
        "function (π::RolloutLookahead)(s)\n",
        "\tU(s) = rollout(π.𝒫, s, π.π, π.d)\n",
        "    return greedy(π.𝒫, U, s).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 2\n",
        "struct ForwardSearch\n",
        "    𝒫 # problem\n",
        "    d # depth\n",
        "    U # value function at depth d\n",
        "end\n",
        "\n",
        "function forward_search(𝒫, s, d, U)\n",
        "    if d ≤ 0\n",
        "        return (a=nothing, u=U(s))\n",
        "    end\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    U′(s) = forward_search(𝒫, s, d-1, U).u\n",
        "    for a in 𝒫.𝒜\n",
        "        u = lookahead(𝒫, U′, s, a)\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "(π::ForwardSearch)(s) = forward_search(π.𝒫, s, π.d, π.U).a\n",
        "####################\n",
        "\n",
        "#################### online-approximations 3\n",
        "struct BranchAndBound\n",
        "    𝒫   # problem\n",
        "    d   # depth\n",
        "    Ulo # lower bound on value function at depth d\n",
        "    Qhi # upper bound on action value function\n",
        "end\n",
        "\n",
        "function branch_and_bound(𝒫, s, d, Ulo, Qhi)\n",
        "    if d ≤ 0\n",
        "        return (a=nothing, u=Ulo(s))\n",
        "    end\n",
        "    U′(s) = branch_and_bound(𝒫, s, d-1, Ulo, Qhi).u\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for a in sort(𝒫.𝒜, by=a->Qhi(s,a), rev=true)\n",
        "        if Qhi(s, a) < best.u\n",
        "            return best # safe to prune\n",
        "        end\n",
        "        u = lookahead(𝒫, U′, s, a)\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "(π::BranchAndBound)(s) = branch_and_bound(π.𝒫, s, π.d, π.Ulo, π.Qhi).a\n",
        "####################\n",
        "\n",
        "#################### online-approximations 4\n",
        "struct SparseSampling\n",
        "    𝒫 # problem\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    U # value function at depth d\n",
        "end\n",
        "\n",
        "function sparse_sampling(𝒫, s, d, m, U)\n",
        "    if d ≤ 0\n",
        "        return (a=nothing, u=U(s))\n",
        "    end\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for a in 𝒫.𝒜\n",
        "        u = 0.0\n",
        "        for i in 1:m\n",
        "            s′, r = randstep(𝒫, s, a)\n",
        "            a′, u′ = sparse_sampling(𝒫, s′, d-1, m, U)\n",
        "            u += (r + 𝒫.γ*u′) / m\n",
        "        end\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "(π::SparseSampling)(s) = sparse_sampling(π.𝒫, s, π.d, π.m, π.U).a\n",
        "####################\n",
        "\n",
        "#################### online-approximations 5\n",
        "struct MonteCarloTreeSearch\n",
        "\t𝒫 # problem\n",
        "\tN # visit counts\n",
        "\tQ # action value estimates\n",
        "\td # depth\n",
        "\tm # number of simulations\n",
        "\tc # exploration constant\n",
        "\tU # value function estimate\n",
        "end\n",
        "\n",
        "function (π::MonteCarloTreeSearch)(s)\n",
        "\tfor k in 1:π.m\n",
        "\t\tsimulate!(π, s)\n",
        "\tend\n",
        "\treturn argmax(a->π.Q[(s,a)], π.𝒫.𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 6\n",
        "function simulate!(π::MonteCarloTreeSearch, s, d=π.d)\n",
        "    if d ≤ 0\n",
        "        return π.U(s)\n",
        "    end\n",
        "    𝒫, N, Q, c = π.𝒫, π.N, π.Q, π.c\n",
        "    𝒜, TR, γ = 𝒫.𝒜, 𝒫.TR, 𝒫.γ\n",
        "    if !haskey(N, (s, first(𝒜)))\n",
        "        for a in 𝒜\n",
        "            N[(s,a)] = 0\n",
        "            Q[(s,a)] = 0.0\n",
        "        end\n",
        "        return π.U(s)\n",
        "    end\n",
        "    a = explore(π, s)\n",
        "    s′, r = TR(s,a)\n",
        "    q = r + γ*simulate!(π, s′, d-1)\n",
        "    N[(s,a)] += 1\n",
        "    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]\n",
        "    return q\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 7\n",
        "bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)\n",
        "\n",
        "function explore(π::MonteCarloTreeSearch, s)\n",
        "    𝒜, N, Q, c = π.𝒫.𝒜, π.N, π.Q, π.c\n",
        "    Ns = sum(N[(s,a)] for a in 𝒜)\n",
        "    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), 𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 8\n",
        "struct HeuristicSearch\n",
        "    𝒫   # problem\n",
        "    Uhi # upper bound on value function\n",
        "    d   # depth\n",
        "    m   # number of simulations\n",
        "end\n",
        "\n",
        "function simulate!(π::HeuristicSearch, U, s)\n",
        "    𝒫 = π.𝒫\n",
        "    for d in 1:π.d\n",
        "        a, u = greedy(𝒫, U, s)\n",
        "        U[s] = u\n",
        "        s = rand(𝒫.T(s, a))\n",
        "    end\n",
        "end\n",
        "\n",
        "function (π::HeuristicSearch)(s)\n",
        "    U = [π.Uhi(s) for s in π.𝒫.𝒮]\n",
        "    for i in 1:π.m\n",
        "        simulate!(π, U, s)\n",
        "    end\n",
        "    return greedy(π.𝒫, U, s).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 9\n",
        "function extractpolicy(π::HeuristicSearch, s)\n",
        "    U = [π.Uhi(s) for s in π.𝒫.𝒮]\n",
        "    for i in 1:π.m\n",
        "        simulate!(π, U, s)\n",
        "    end\n",
        "    return ValueFunctionPolicy(π.𝒫, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 10\n",
        "struct LabeledHeuristicSearch\n",
        "    𝒫     # problem\n",
        "    Uhi   # upper bound on value function\n",
        "    d     # depth\n",
        "    δ     # gap threshold\n",
        "end\n",
        "\n",
        "function (π::LabeledHeuristicSearch)(s)\n",
        "    U, solved = [π.Uhi(s) for s in 𝒫.𝒮], Set()\n",
        "    while s ∉ solved\n",
        "        simulate!(π, U, solved, s)\n",
        "    end\n",
        "    return greedy(π.𝒫, U, s).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 11\n",
        "function simulate!(π::LabeledHeuristicSearch, U, solved, s)\n",
        "    visited = []\n",
        "    for d in 1:π.d\n",
        "        if s ∈ solved\n",
        "            break\n",
        "        end\n",
        "        push!(visited, s)\n",
        "        a, u = greedy(π.𝒫, U, s)\n",
        "        U[s] = u\n",
        "        s = rand(π.𝒫.T(s, a))\n",
        "    end\n",
        "    while !isempty(visited)\n",
        "        if label!(π, U, solved, pop!(visited))\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 12\n",
        "function expand(π::LabeledHeuristicSearch, U, solved, s)\n",
        "    𝒫, δ = π.𝒫, π.δ\n",
        "    𝒮, 𝒜, T = 𝒫.𝒮, 𝒫.𝒜, 𝒫.T\n",
        "    found, toexpand, envelope = false, Set(s), []\n",
        "    while !isempty(toexpand)\n",
        "        s = pop!(toexpand)\n",
        "        push!(envelope, s)\n",
        "        a, u = greedy(𝒫, U, s)\n",
        "        if abs(U[s] - u) > δ\n",
        "            found = true\n",
        "        else\n",
        "            for s′ in 𝒮\n",
        "                if T(s,a,s′) > 0 && s′ ∉ (solved ∪ envelope)\n",
        "                    push!(toexpand, s′)\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return (found, envelope)\n",
        "end\n",
        "\n",
        "function label!(π::LabeledHeuristicSearch, U, solved, s)\n",
        "    if s ∈ solved\n",
        "        return false\n",
        "    end\n",
        "    found, envelope = expand(π, U, solved, s)\n",
        "    if found\n",
        "        for s ∈ reverse(envelope)\n",
        "            U[s] = greedy(π.𝒫, U, s).u\n",
        "        end\n",
        "    else\n",
        "        union!(solved, envelope)\n",
        "    end\n",
        "    return found\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 13\n",
        "function extractpolicy(π::LabeledHeuristicSearch, s)\n",
        "    U, solved = [π.Uhi(s) for s ∈ π.𝒫.𝒮], Set()\n",
        "    while s ∉ solved\n",
        "        simulate!(π, U, solved, s)\n",
        "    end\n",
        "    return ValueFunctionPolicy(π.𝒫, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 1\n",
        "struct MonteCarloPolicyEvaluation\n",
        "    𝒫 # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "end\n",
        "\n",
        "function (U::MonteCarloPolicyEvaluation)(π)\n",
        "    R(π) = rollout(U.𝒫, rand(U.b), π, U.d)\n",
        "    return mean(R(π) for i = 1:U.m)\n",
        "end\n",
        "\n",
        "(U::MonteCarloPolicyEvaluation)(π, θ) = U(s->π(θ, s))\n",
        "####################\n",
        "\n",
        "#################### policy-search 2\n",
        "struct HookeJeevesPolicySearch\n",
        "    θ # initial parameterization\n",
        "    α # step size\n",
        "    c # step size reduction factor\n",
        "    ϵ # termination step size\n",
        "end\n",
        "\n",
        "function optimize(M::HookeJeevesPolicySearch, π, U)\n",
        "    θ, θ′, α, c, ϵ = copy(M.θ), similar(M.θ), M.α, M.c, M.ϵ\n",
        "    u, n = U(π, θ), length(θ)\n",
        "    while α > ϵ\n",
        "        copyto!(θ′, θ)\n",
        "        best = (i=0, sgn=0, u=u)\n",
        "        for i in 1:n\n",
        "            for sgn in (-1,1)\n",
        "                θ′[i] = θ[i] + sgn*α\n",
        "                u′ = U(π, θ′)\n",
        "                if u′ > best.u\n",
        "                    best = (i=i, sgn=sgn, u=u′)\n",
        "                end\n",
        "            end\n",
        "            θ′[i] = θ[i]\n",
        "        end\n",
        "        if best.i != 0\n",
        "            θ[best.i] += best.sgn*α\n",
        "            u = best.u\n",
        "        else\n",
        "            α *= c\n",
        "        end\n",
        "    end\n",
        "    return θ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 3\n",
        "struct GeneticPolicySearch\n",
        "    θs      # initial population\n",
        "    σ       # initial standard deviation\n",
        "    m_elite # number of elite samples\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function optimize(M::GeneticPolicySearch, π, U)\n",
        "    θs, σ = M.θs, M.σ\n",
        "    n, m = length(first(θs)), length(θs)\n",
        "    for k in 1:M.k_max\n",
        "        us = [U(π, θ) for θ in θs]\n",
        "        sp = sortperm(us, rev=true)\n",
        "        θ_best = θs[sp[1]]\n",
        "        rand_elite() = θs[sp[rand(1:M.m_elite)]]\n",
        "        θs = [rand_elite() + σ.*randn(n) for i in 1:(m-1)]\n",
        "        push!(θs, θ_best)\n",
        "    end\n",
        "    return last(θs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 4\n",
        "struct CrossEntropyPolicySearch\n",
        "    p       # initial distribution\n",
        "    m       # number of samples\n",
        "    m_elite # number of elite samples\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function optimize_dist(M::CrossEntropyPolicySearch, π, U)\n",
        "    p, m, m_elite, k_max = M.p, M.m, M.m_elite, M.k_max\n",
        "    for k in 1:k_max\n",
        "        θs = rand(p, m)\n",
        "        us = [U(π, θs[:,i]) for i in 1:m]\n",
        "        θ_elite = θs[:,sortperm(us)[(m-m_elite+1):m]]\n",
        "        p = Distributions.fit(typeof(p), θ_elite)\n",
        "    end\n",
        "    return p\n",
        "end\n",
        "\n",
        "function optimize(M, π, U)\n",
        "    return Distributions.mode(optimize_dist(M, π, U))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 5\n",
        "struct EvolutionStrategies\n",
        "    D       # distribution constructor\n",
        "    ψ       # initial distribution parameterization\n",
        "    ∇logp   # log search likelihood gradient\n",
        "    m       # number of samples\n",
        "    α       # step factor\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function evolution_strategy_weights(m)\n",
        "    ws = [max(0, log(m/2+1) - log(i)) for i in 1:m]\n",
        "    ws ./= sum(ws)\n",
        "    ws .-= 1/m\n",
        "    return ws\n",
        "end\n",
        "\n",
        "function optimize_dist(M::EvolutionStrategies, π, U)\n",
        "    D, ψ, m, ∇logp, α = M.D, M.ψ, M.m, M.∇logp, M.α\n",
        "    ws = evolution_strategy_weights(m)\n",
        "    for k in 1:M.k_max\n",
        "        θs = rand(D(ψ), m)\n",
        "        us = [U(π, θs[:,i]) for i in 1:m]\n",
        "        sp = sortperm(us, rev=true)\n",
        "        ∇ = sum(w.*∇logp(ψ, θs[:,i]) for (w,i) in zip(ws,sp))\n",
        "        ψ += α.*∇\n",
        "    end\n",
        "    return D(ψ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-search 6\n",
        "struct IsotropicEvolutionStrategies\n",
        "    ψ       # initial mean\n",
        "    σ       # initial standard deviation\n",
        "    m       # number of samples\n",
        "    α       # step factor\n",
        "    k_max   # number of iterations\n",
        "end\n",
        "\n",
        "function optimize_dist(M::IsotropicEvolutionStrategies, π, U)\n",
        "    ψ, σ, m, α, k_max = M.ψ, M.σ, M.m, M.α, M.k_max\n",
        "    n = length(ψ)\n",
        "    ws = evolution_strategy_weights(2*div(m,2))\n",
        "    for k in 1:k_max\n",
        "        ϵs = [randn(n) for i in 1:div(m,2)]\n",
        "        append!(ϵs, -ϵs) # weight mirroring\n",
        "        us = [U(π, ψ + σ.*ϵ) for ϵ in ϵs]\n",
        "        sp = sortperm(us, rev=true)\n",
        "        ∇ = sum(w.*ϵs[i] for (w,i) in zip(ws,sp)) / σ\n",
        "        ψ += α.*∇\n",
        "    end\n",
        "    return MvNormal(ψ, σ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 1\n",
        "function simulate(𝒫::MDP, s, π, d)\n",
        "\tτ = []\n",
        "\tfor i = 1:d\n",
        "\t    a = π(s)\n",
        "\t\ts′, r = 𝒫.TR(s,a)\n",
        "\t    push!(τ, (s,a,r))\n",
        "\t    s = s′\n",
        "    end\n",
        "    return τ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 2\n",
        "struct FiniteDifferenceGradient\n",
        "    𝒫 # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    δ # step size\n",
        "end\n",
        "\n",
        "function gradient(M::FiniteDifferenceGradient, π, θ)\n",
        "    𝒫, b, d, m, δ, γ, n = M.𝒫, M.b, M.d, M.m, M.δ, M.𝒫.γ, length(θ)\n",
        "    Δθ(i) = [i == k ? δ : 0.0 for k in 1:n]\n",
        "    R(τ) = sum(r*γ^(k-1) for (k, (s,a,r)) in enumerate(τ))\n",
        "    U(θ) = mean(R(simulate(𝒫, rand(b), s->π(θ, s), d)) for i in 1:m)\n",
        "    ΔU = [U(θ + Δθ(i)) - U(θ) for i in 1:n]\n",
        "    return ΔU ./ δ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 3\n",
        "struct RegressionGradient\n",
        "    𝒫 # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    δ # step size\n",
        "end\n",
        "\n",
        "function gradient(M::RegressionGradient, π, θ)\n",
        "    𝒫, b, d, m, δ, γ = M.𝒫, M.b, M.d, M.m, M.δ, M.𝒫.γ\n",
        "    ΔΘ = [δ.*normalize(randn(length(θ)), 2) for i = 1:m]\n",
        "    R(τ) = sum(r*γ^(k-1) for (k, (s,a,r)) in enumerate(τ))\n",
        "    U(θ) = R(simulate(𝒫, rand(b), s->π(θ,s), d))\n",
        "    ΔU = [U(θ + Δθ) - U(θ) for Δθ in ΔΘ]\n",
        "    return pinv(reduce(hcat, ΔΘ)') * ΔU\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 4\n",
        "struct LikelihoodRatioGradient\n",
        "    𝒫 # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    ∇logπ # gradient of log likelihood\n",
        "end\n",
        "\n",
        "function gradient(M::LikelihoodRatioGradient, π, θ)\n",
        "    𝒫, b, d, m, ∇logπ, γ = M.𝒫, M.b, M.d, M.m, M.∇logπ, M.𝒫.γ\n",
        "    πθ(s) = π(θ, s)\n",
        "    R(τ) = sum(r*γ^(k-1) for (k, (s,a,r)) in enumerate(τ))\n",
        "    ∇U(τ) = sum(∇logπ(θ, a, s) for (s,a) in τ)*R(τ)\n",
        "    return mean(∇U(simulate(𝒫, rand(b), πθ, d)) for i in 1:m)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 5\n",
        "struct RewardToGoGradient\n",
        "    𝒫 # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    ∇logπ # gradient of log likelihood\n",
        "end\n",
        "\n",
        "function gradient(M::RewardToGoGradient, π, θ)\n",
        "    𝒫, b, d, m, ∇logπ, γ = M.𝒫, M.b, M.d, M.m, M.∇logπ, M.𝒫.γ\n",
        "    πθ(s) = π(θ, s)\n",
        "    R(τ, j) = sum(r*γ^(k-1) for (k,(s,a,r)) in zip(j:d, τ[j:end]))\n",
        "    ∇U(τ) = sum(∇logπ(θ, a, s)*R(τ,j) for (j, (s,a,r)) in enumerate(τ))\n",
        "    return mean(∇U(simulate(𝒫, rand(b), πθ, d)) for i in 1:m)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-estimation 6\n",
        "struct BaselineSubtractionGradient\n",
        "    𝒫 # problem\n",
        "    b # initial state distribution\n",
        "    d # depth\n",
        "    m # number of samples\n",
        "    ∇logπ # gradient of log likelihood\n",
        "end\n",
        "\n",
        "function gradient(M::BaselineSubtractionGradient, π, θ)\n",
        "    𝒫, b, d, m, ∇logπ, γ = M.𝒫, M.b, M.d, M.m, M.∇logπ, M.𝒫.γ\n",
        "    πθ(s) = π(θ, s)\n",
        "    ℓ(a, s, k) = ∇logπ(θ, a, s)*γ^(k-1)\n",
        "    R(τ, k) = sum(r*γ^(j-1) for (j,(s,a,r)) in enumerate(τ[k:end]))\n",
        "    numer(τ) = sum(ℓ(a,s,k).^2*R(τ,k) for (k,(s,a,r)) in enumerate(τ))\n",
        "    denom(τ) = sum(ℓ(a,s,k).^2 for (k,(s,a)) in enumerate(τ))\n",
        "    base(τ) = numer(τ) ./ denom(τ)\n",
        "    trajs = [simulate(𝒫, rand(b), πθ, d) for i in 1:m]\n",
        "    rbase = mean(base(τ) for τ in trajs)\n",
        "    ∇U(τ) = sum(ℓ(a,s,k).*(R(τ,k).-rbase) for (k,(s,a,r)) in enumerate(τ))\n",
        "    return mean(∇U(τ) for τ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 1\n",
        "struct PolicyGradientUpdate\n",
        "    ∇U # policy gradient estimate\n",
        "    α  # step factor\n",
        "end\n",
        "\n",
        "function update(M::PolicyGradientUpdate, θ)\n",
        "    return θ + M.α * M.∇U(θ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 2\n",
        "scale_gradient(∇, L2_max) = min(L2_max/norm(∇), 1)*∇\n",
        "clip_gradient(∇, a, b) = clamp.(∇, a, b)\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 3\n",
        "struct RestrictedPolicyUpdate\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ∇logπ # gradient of log likelihood\n",
        "    π     # policy\n",
        "    ϵ     # divergence bound\n",
        "end\n",
        "\n",
        "function update(M::RestrictedPolicyUpdate, θ)\n",
        "    𝒫, b, d, m, ∇logπ, π, γ = M.𝒫, M.b, M.d, M.m, M.∇logπ, M.π, M.𝒫.γ\n",
        "    πθ(s) = π(θ, s)\n",
        "    R(τ) = sum(r*γ^(k-1) for (k, (s,a,r)) in enumerate(τ))\n",
        "    τs = [simulate(𝒫, rand(b), πθ, d) for i in 1:m]\n",
        "    ∇log(τ) = sum(∇logπ(θ, a, s) for (s,a) in τ)\n",
        "    ∇U(τ) = ∇log(τ)*R(τ)\n",
        "    u = mean(∇U(τ) for τ in τs)\n",
        "    return θ + u*sqrt(2*M.ϵ/dot(u,u))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 4\n",
        "struct NaturalPolicyUpdate\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ∇logπ # gradient of log likelihood\n",
        "    π     # policy\n",
        "    ϵ     # divergence bound\n",
        "end\n",
        "\n",
        "function natural_update(θ, ∇f, F, ϵ, τs)\n",
        "    ∇fθ = mean(∇f(τ) for τ in τs)\n",
        "    u = mean(F(τ) for τ in τs) \\ ∇fθ\n",
        "    return θ + u*sqrt(2ϵ/dot(∇fθ,u))\n",
        "end\n",
        "\n",
        "function update(M::NaturalPolicyUpdate, θ)\n",
        "    𝒫, b, d, m, ∇logπ, π, γ = M.𝒫, M.b, M.d, M.m, M.∇logπ, M.π, M.𝒫.γ\n",
        "    πθ(s) = π(θ, s)\n",
        "    R(τ) = sum(r*γ^(k-1) for (k, (s,a,r)) in enumerate(τ))\n",
        "    ∇log(τ) = sum(∇logπ(θ, a, s) for (s,a) in τ)\n",
        "    ∇U(τ) = ∇log(τ)*R(τ)\n",
        "    F(τ) = ∇log(τ)*∇log(τ)'\n",
        "    τs = [simulate(𝒫, rand(b), πθ, d) for i in 1:m]\n",
        "    return natural_update(θ, ∇U, F, M.ϵ, τs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 5\n",
        "struct TrustRegionUpdate\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    π     # policy π(s)\n",
        "    p     # policy likelihood p(θ, a, s)\n",
        "    ∇logπ # log likelihood gradient\n",
        "    KL    # KL divergence KL(θ, θ′, s)\n",
        "    ϵ     # divergence bound\n",
        "    α     # line search reduction factor (e.g., 0.5)\n",
        "end\n",
        "\n",
        "function surrogate_objective(M::TrustRegionUpdate, θ, θ′, τs)\n",
        "    d, p, γ = M.d, M.p, M.𝒫.γ\n",
        "    R(τ, j) = sum(r*γ^(k-1) for (k,(s,a,r)) in zip(j:d, τ[j:end]))\n",
        "    w(a,s) = p(θ′,a,s) / p(θ,a,s)\n",
        "    f(τ) = mean(w(a,s)*R(τ,k) for (k,(s,a,r)) in enumerate(τ))\n",
        "    return mean(f(τ) for τ in τs)\n",
        "end\n",
        "\n",
        "function surrogate_constraint(M::TrustRegionUpdate, θ, θ′, τs)\n",
        "    γ = M.𝒫.γ\n",
        "    KL(τ) = mean(M.KL(θ, θ′, s)*γ^(k-1) for (k,(s,a,r)) in enumerate(τ))\n",
        "    return mean(KL(τ) for τ in τs)\n",
        "end\n",
        "\n",
        "function linesearch(M::TrustRegionUpdate, f, g, θ, θ′)\n",
        "    fθ = f(θ)\n",
        "    while g(θ′) > M.ϵ || f(θ′) ≤ fθ\n",
        "        θ′ = θ + M.α*(θ′ - θ)\n",
        "    end\n",
        "    return θ′\n",
        "end\n",
        "\n",
        "function update(M::TrustRegionUpdate, θ)\n",
        "    𝒫, b, d, m, ∇logπ, π, γ = M.𝒫, M.b, M.d, M.m, M.∇logπ, M.π, M.𝒫.γ\n",
        "    πθ(s) = π(θ, s)\n",
        "    R(τ) = sum(r*γ^(k-1) for (k, (s,a,r)) in enumerate(τ))\n",
        "    ∇log(τ) = sum(∇logπ(θ, a, s) for (s,a) in τ)\n",
        "    ∇U(τ) = ∇log(τ)*R(τ)\n",
        "    F(τ) = ∇log(τ)*∇log(τ)'\n",
        "    τs = [simulate(𝒫, rand(b), πθ, d) for i in 1:m]\n",
        "    θ′ = natural_update(θ, ∇U, F, M.ϵ, τs)\n",
        "    f(θ′) = surrogate_objective(M, θ, θ′, τs)\n",
        "    g(θ′) = surrogate_constraint(M, θ, θ′, τs)\n",
        "    return linesearch(M, f, g, θ, θ′)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### policy-gradient-optimization 6\n",
        "struct ClampedSurrogateUpdate\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of trajectories\n",
        "    π     # policy\n",
        "    p     # policy likelihood\n",
        "    ∇π    # policy likelihood gradient\n",
        "    ϵ     # divergence bound\n",
        "    α     # step size\n",
        "    k_max # number of iterations per update\n",
        "end\n",
        "\n",
        "function clamped_gradient(M::ClampedSurrogateUpdate, θ, θ′, τs)\n",
        "    d, p, ∇π, ϵ, γ = M.d, M.p, M.∇π, M.ϵ, M.𝒫.γ\n",
        "    R(τ, j) = sum(r*γ^(k-1) for (k,(s,a,r)) in zip(j:d, τ[j:end]))\n",
        "    ∇f(a,s,r_togo) = begin\n",
        "        P = p(θ, a,s)\n",
        "        w = p(θ′,a,s) / P\n",
        "        if (r_togo > 0 && w > 1+ϵ) || (r_togo < 0 && w < 1-ϵ)\n",
        "            return zeros(length(θ))\n",
        "        end\n",
        "        return ∇π(θ′, a, s) * r_togo / P\n",
        "    end\n",
        "    ∇f(τ) = mean(∇f(a,s,R(τ,k)) for (k,(s,a,r)) in enumerate(τ))\n",
        "    return mean(∇f(τ) for τ in τs)\n",
        "end\n",
        "\n",
        "function update(M::ClampedSurrogateUpdate, θ)\n",
        "    𝒫, b, d, m, π, α, k_max= M.𝒫, M.b, M.d, M.m, M.π, M.α, M.k_max\n",
        "    πθ(s) = π(θ, s)\n",
        "    τs = [simulate(𝒫, rand(b), πθ, d) for i in 1:m]\n",
        "    θ′ = copy(θ)\n",
        "    for k in 1:k_max\n",
        "        θ′ += α*clamped_gradient(M, θ, θ′, τs)\n",
        "    end\n",
        "    return θ′\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### actor-critic 1\n",
        "struct ActorCritic\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ∇logπ # gradient of log likelihood ∇logπ(θ,a,s)\n",
        "    U     # parameterized value function U(ϕ, s)\n",
        "    ∇U    # gradient of value function ∇U(ϕ,s)\n",
        "end\n",
        "\n",
        "function gradient(M::ActorCritic, π, θ, ϕ)\n",
        "    𝒫, b, d, m, ∇logπ = M.𝒫, M.b, M.d, M.m, M.∇logπ\n",
        "    U, ∇U, γ = M.U, M.∇U, M.𝒫.γ\n",
        "    πθ(s) = π(θ, s)\n",
        "    R(τ,j) = sum(r*γ^(k-1) for (k,(s,a,r)) in enumerate(τ[j:end]))\n",
        "    A(τ,j) = τ[j][3] + γ*U(ϕ,τ[j+1][1]) - U(ϕ,τ[j][1])\n",
        "    ∇Uθ(τ) = sum(∇logπ(θ,a,s)*A(τ,j)*γ^(j-1) for (j, (s,a,r))\n",
        "    \t\t\t\tin enumerate(τ[1:end-1]))\n",
        "    ∇ℓϕ(τ) = sum((U(ϕ,s) - R(τ,j))*∇U(ϕ,s) for (j, (s,a,r))\n",
        "    \t\t\t\tin enumerate(τ))\n",
        "    trajs = [simulate(𝒫, rand(b), πθ, d) for i in 1:m]\n",
        "    return mean(∇Uθ(τ) for τ in trajs), mean(∇ℓϕ(τ) for τ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### actor-critic 2\n",
        "struct GeneralizedAdvantageEstimation\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ∇logπ # gradient of log likelihood ∇logπ(θ,a,s)\n",
        "    U     # parameterized value function U(ϕ, s)\n",
        "    ∇U    # gradient of value function ∇U(ϕ,s)\n",
        "    λ     # weight ∈ [0,1]\n",
        "end\n",
        "\n",
        "function gradient(M::GeneralizedAdvantageEstimation, π, θ, ϕ)\n",
        "    𝒫, b, d, m, ∇logπ = M.𝒫, M.b, M.d, M.m, M.∇logπ\n",
        "    U, ∇U, γ, λ = M.U, M.∇U, M.𝒫.γ, M.λ\n",
        "    πθ(s) = π(θ, s)\n",
        "    R(τ,j) = sum(r*γ^(k-1) for (k,(s,a,r)) in enumerate(τ[j:end]))\n",
        "    δ(τ,j) = τ[j][3] + γ*U(ϕ,τ[j+1][1]) - U(ϕ,τ[j][1])\n",
        "    A(τ,j) = sum((γ*λ)^(ℓ-1)*δ(τ, j+ℓ-1) for ℓ in 1:d-j)\n",
        "    ∇Uθ(τ) = sum(∇logπ(θ,a,s)*A(τ,j)*γ^(j-1)\n",
        "                    for (j, (s,a,r)) in enumerate(τ[1:end-1]))\n",
        "    ∇ℓϕ(τ) = sum((U(ϕ,s) - R(τ,j))*∇U(ϕ,s)\n",
        "                    for (j, (s,a,r)) in enumerate(τ))\n",
        "    trajs = [simulate(𝒫, rand(b), πθ, d) for i in 1:m]\n",
        "    return mean(∇Uθ(τ) for τ in trajs), mean(∇ℓϕ(τ) for τ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### actor-critic 3\n",
        "struct DeterministicPolicyGradient\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    m     # number of samples\n",
        "    ∇π    # gradient of deterministic policy π(θ, s)\n",
        "    Q     # parameterized value function Q(ϕ,s,a)\n",
        "    ∇Qϕ   # gradient of value function with respect to ϕ\n",
        "    ∇Qa   # gradient of value function with respect to a\n",
        "    σ     # policy noise\n",
        "end\n",
        "\n",
        "function gradient(M::DeterministicPolicyGradient, π, θ, ϕ)\n",
        "    𝒫, b, d, m, ∇π = M.𝒫, M.b, M.d, M.m, M.∇π\n",
        "    Q, ∇Qϕ, ∇Qa, σ, γ = M.Q, M.∇Qϕ, M.∇Qa, M.σ, M.𝒫.γ\n",
        "    π_rand(s) = π(θ, s) + σ*randn()*I\n",
        "    ∇Uθ(τ) = sum(∇π(θ,s)*∇Qa(ϕ,s,π(θ,s))*γ^(j-1) for (j,(s,a,r))\n",
        "                in enumerate(τ))\n",
        "    ∇ℓϕ(τ,j) = begin\n",
        "        s, a, r = τ[j]\n",
        "        s′ = τ[j+1][1]\n",
        "        a′ = π(θ,s′)\n",
        "        δ = r + γ*Q(ϕ,s′,a′) - Q(ϕ,s,a)\n",
        "        return δ*(γ*∇Qϕ(ϕ,s′,a′) - ∇Qϕ(ϕ,s,a))\n",
        "    end\n",
        "    ∇ℓϕ(τ) = sum(∇ℓϕ(τ,j) for j in 1:length(τ)-1)\n",
        "    trajs = [simulate(𝒫, rand(b), π_rand, d) for i in 1:m]\n",
        "    return mean(∇Uθ(τ) for τ in trajs), mean(∇ℓϕ(τ) for τ in trajs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### validation 1\n",
        "function adversarial(𝒫::MDP, π, λ)\n",
        "    𝒮, 𝒜, T, R, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.T, 𝒫.R, 𝒫.γ\n",
        "    𝒮′ = 𝒜′ = 𝒮\n",
        "    R′ = zeros(length(𝒮′), length(𝒜′))\n",
        "    T′ = zeros(length(𝒮′), length(𝒜′), length(𝒮′))\n",
        "    for s in 𝒮′\n",
        "        for a in 𝒜′\n",
        "            R′[s,a] = -R(s, π(s)) + λ*log(T(s, π(s), a))\n",
        "            T′[s,a,a] = 1\n",
        "        end\n",
        "    end\n",
        "    return MDP(T′, R′, γ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 1\n",
        "struct BanditProblem\n",
        "    θ # vector of payoff probabilities\n",
        "    R # reward sampler\n",
        "end\n",
        "\n",
        "function BanditProblem(θ)\n",
        "    R(a) = rand() < θ[a] ? 1 : 0\n",
        "    return BanditProblem(θ, R)\n",
        "end\n",
        "\n",
        "function simulate(𝒫::BanditProblem, model, π, h)\n",
        "    for i in 1:h\n",
        "        a = π(model)\n",
        "        r = 𝒫.R(a)\n",
        "        update!(model, a, r)\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 2\n",
        "struct BanditModel\n",
        "    B # vector of beta distributions\n",
        "end\n",
        "\n",
        "function update!(model::BanditModel, a, r)\n",
        "    α, β = StatsBase.params(model.B[a])\n",
        "    model.B[a] = Beta(α + r, β + (1-r))\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 3\n",
        "mutable struct EpsilonGreedyExploration\n",
        "    ϵ # probability of random arm\n",
        "end\n",
        "\n",
        "function (π::EpsilonGreedyExploration)(model::BanditModel)\n",
        "    if rand() < π.ϵ\n",
        "        return rand(eachindex(model.B))\n",
        "    else\n",
        "        return argmax(mean.(model.B))\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 4\n",
        "mutable struct ExploreThenCommitExploration\n",
        "    k # pulls remaining until commitment\n",
        "end\n",
        "\n",
        "function (π::ExploreThenCommitExploration)(model::BanditModel)\n",
        "    if π.k > 0\n",
        "        π.k -= 1\n",
        "        return rand(eachindex(model.B))\n",
        "    end\n",
        "    return argmax(mean.(model.B))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 5\n",
        "mutable struct SoftmaxExploration\n",
        "    λ # precision parameter\n",
        "    α # precision factor\n",
        "end\n",
        "\n",
        "function (π::SoftmaxExploration)(model::BanditModel)\n",
        "    weights = exp.(π.λ * mean.(model.B))\n",
        "    π.λ *= π.α\n",
        "    return rand(Categorical(normalize(weights, 1)))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 6\n",
        "mutable struct QuantileExploration\n",
        "    α # quantile (e.g., 0.95)\n",
        "end\n",
        "\n",
        "function (π::QuantileExploration)(model::BanditModel)\n",
        "    return argmax([quantile(B, π.α) for B in model.B])\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 7\n",
        "mutable struct UCB1Exploration\n",
        "    c # exploration constant\n",
        "end\n",
        "\n",
        "function bonus(π::UCB1Exploration, B, a)\n",
        "\tN = sum(b.α + b.β for b in B)\n",
        "\tNa = B[a].α + B[a].β\n",
        "    return π.c * sqrt(log(N)/Na)\n",
        "end\n",
        "\n",
        "function (π::UCB1Exploration)(model::BanditModel)\n",
        "\tB = model.B\n",
        "    ρ = mean.(B)\n",
        "    u = ρ .+ [bonus(π, B, a) for a in eachindex(B)]\n",
        "    return argmax(u)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 8\n",
        "struct PosteriorSamplingExploration end\n",
        "\n",
        "(π::PosteriorSamplingExploration)(model::BanditModel) =\n",
        "    argmax(rand.(model.B))\n",
        "####################\n",
        "\n",
        "#################### exploration-and-exploitation 9\n",
        "function simulate(𝒫::MDP, model, π, h, s)\n",
        "    for i in 1:h\n",
        "        a = π(model, s)\n",
        "        s′, r = 𝒫.TR(s, a)\n",
        "        update!(model, s, a, r, s′)\n",
        "        s = s′\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 1\n",
        "mutable struct MaximumLikelihoodMDP\n",
        "    𝒮 # state space (assumes 1:nstates)\n",
        "    𝒜 # action space (assumes 1:nactions)\n",
        "    N # transition count N(s,a,s′)\n",
        "    ρ # reward sum ρ(s, a)\n",
        "    γ # discount\n",
        "    U # value function\n",
        "    planner\n",
        "end\n",
        "\n",
        "function lookahead(model::MaximumLikelihoodMDP, s, a)\n",
        "    𝒮, U, γ = model.𝒮, model.U, model.γ\n",
        "    n = sum(model.N[s,a,:])\n",
        "    if n == 0\n",
        "        return 0.0\n",
        "    end\n",
        "    r = model.ρ[s, a] / n\n",
        "    T(s,a,s′) = model.N[s,a,s′] / n\n",
        "    return r + γ * sum(T(s,a,s′)*U[s′] for s′ in 𝒮)\n",
        "end\n",
        "\n",
        "function backup(model::MaximumLikelihoodMDP, U, s)\n",
        "    return maximum(lookahead(model, s, a) for a in model.𝒜)\n",
        "end\n",
        "\n",
        "function update!(model::MaximumLikelihoodMDP, s, a, r, s′)\n",
        "    model.N[s,a,s′] += 1\n",
        "    model.ρ[s,a] += r\n",
        "    update!(model.planner, model, s, a, r, s′)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 2\n",
        "function MDP(model::MaximumLikelihoodMDP)\n",
        "    N, ρ, 𝒮, 𝒜, γ = model.N, model.ρ, model.𝒮, model.𝒜, model.γ\n",
        "    T, R = similar(N), similar(ρ)\n",
        "    for s in 𝒮\n",
        "        for a in 𝒜\n",
        "            n = sum(N[s,a,:])\n",
        "            if n == 0\n",
        "                T[s,a,:] .= 0.0\n",
        "                R[s,a] = 0.0\n",
        "            else\n",
        "                T[s,a,:] = N[s,a,:] / n\n",
        "                R[s,a] = ρ[s,a] / n\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return MDP(T, R, γ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 3\n",
        "struct FullUpdate end\n",
        "\n",
        "function update!(planner::FullUpdate, model, s, a, r, s′)\n",
        "    𝒫 = MDP(model)\n",
        "    U = solve(𝒫).U\n",
        "    copy!(model.U, U)\n",
        "    return planner\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 4\n",
        "struct RandomizedUpdate\n",
        "    m # number of updates\n",
        "end\n",
        "\n",
        "function update!(planner::RandomizedUpdate, model, s, a, r, s′)\n",
        "    U = model.U\n",
        "    U[s] = backup(model, U, s)\n",
        "    for i in 1:planner.m\n",
        "        s = rand(model.𝒮)\n",
        "        U[s] = backup(model, U, s)\n",
        "    end\n",
        "    return planner\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 5\n",
        "struct PrioritizedUpdate\n",
        "    m  # number of updates\n",
        "    pq # priority queue\n",
        "end\n",
        "\n",
        "function update!(planner::PrioritizedUpdate, model, s)\n",
        "    N, U, pq = model.N, model.U, planner.pq\n",
        "    𝒮, 𝒜 = model.𝒮, model.𝒜\n",
        "    u = U[s]\n",
        "    U[s] = backup(model, U, s)\n",
        "    for s⁻ in 𝒮\n",
        "        for a⁻ in 𝒜\n",
        "            n_sa = sum(N[s⁻,a⁻,s′] for s′ in 𝒮)\n",
        "            if n_sa > 0\n",
        "                T = N[s⁻,a⁻,s] / n_sa\n",
        "                priority = T * abs(U[s] - u)\n",
        "                if priority > 0\n",
        "                    pq[s⁻] = max(get(pq, s⁻, 0.0), priority)\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return planner\n",
        "end\n",
        "\n",
        "function update!(planner::PrioritizedUpdate, model, s, a, r, s′)\n",
        "    planner.pq[s] = Inf\n",
        "    for i in 1:planner.m\n",
        "        if isempty(planner.pq)\n",
        "            break\n",
        "        end\n",
        "        update!(planner, model, dequeue!(planner.pq))\n",
        "    end\n",
        "    return planner\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 6\n",
        "function (π::EpsilonGreedyExploration)(model, s)\n",
        "    𝒜, ϵ = model.𝒜, π.ϵ\n",
        "    if rand() < ϵ\n",
        "        return rand(𝒜)\n",
        "    end\n",
        "    Q(s,a) = lookahead(model, s, a)\n",
        "    return argmax(a->Q(s,a), 𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 7\n",
        "mutable struct RmaxMDP\n",
        "    𝒮 # state space (assumes 1:nstates)\n",
        "    𝒜 # action space (assumes 1:nactions)\n",
        "    N # transition count N(s,a,s′)\n",
        "    ρ # reward sum ρ(s, a)\n",
        "    γ # discount\n",
        "    U # value function\n",
        "    planner\n",
        "    m    # count threshold\n",
        "    rmax # maximum reward\n",
        "end\n",
        "\n",
        "function lookahead(model::RmaxMDP, s, a)\n",
        "    𝒮, U, γ = model.𝒮, model.U, model.γ\n",
        "    n = sum(model.N[s,a,:])\n",
        "    if n < model.m\n",
        "        return model.rmax / (1-γ)\n",
        "    end\n",
        "    r = model.ρ[s, a] / n\n",
        "    T(s,a,s′) = model.N[s,a,s′] / n\n",
        "    return r + γ * sum(T(s,a,s′)*U[s′] for s′ in 𝒮)\n",
        "end\n",
        "\n",
        "function backup(model::RmaxMDP, U, s)\n",
        "    return maximum(lookahead(model, s, a) for a in model.𝒜)\n",
        "end\n",
        "\n",
        "function update!(model::RmaxMDP, s, a, r, s′)\n",
        "    model.N[s,a,s′] += 1\n",
        "    model.ρ[s,a] += r\n",
        "    update!(model.planner, model, s, a, r, s′)\n",
        "    return model\n",
        "end\n",
        "\n",
        "function MDP(model::RmaxMDP)\n",
        "    N, ρ, 𝒮, 𝒜, γ = model.N, model.ρ, model.𝒮, model.𝒜, model.γ\n",
        "    T, R, m, rmax = similar(N), similar(ρ), model.m, model.rmax\n",
        "    for s in 𝒮\n",
        "        for a in 𝒜\n",
        "            n = sum(N[s,a,:])\n",
        "            if n < m\n",
        "                T[s,a,:] .= 0.0\n",
        "                T[s,a,s] = 1.0\n",
        "                R[s,a] = rmax\n",
        "            else\n",
        "                T[s,a,:] = N[s,a,:] / n\n",
        "                R[s,a] = ρ[s,a] / n\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return MDP(T, R, γ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 8\n",
        "mutable struct BayesianMDP\n",
        "    𝒮 # state space (assumes 1:nstates)\n",
        "    𝒜 # action space (assumes 1:nactions)\n",
        "    D # Dirichlet distributions D[s,a]\n",
        "    R # reward function as matrix (not estimated)\n",
        "    γ # discount\n",
        "    U # value function\n",
        "    planner\n",
        "end\n",
        "\n",
        "function lookahead(model::BayesianMDP, s, a)\n",
        "    𝒮, U, γ = model.𝒮, model.U, model.γ\n",
        "    n = sum(model.D[s,a].alpha)\n",
        "    if n == 0\n",
        "        return 0.0\n",
        "    end\n",
        "    r = model.R(s,a)\n",
        "    T(s,a,s′) = model.D[s,a].alpha[s′] / n\n",
        "    return r + γ * sum(T(s,a,s′)*U[s′] for s′ in 𝒮)\n",
        "end\n",
        "\n",
        "function update!(model::BayesianMDP, s, a, r, s′)\n",
        "    α = model.D[s,a].alpha\n",
        "    α[s′] += 1\n",
        "    model.D[s,a] = Dirichlet(α)\n",
        "    update!(model.planner, model, s, a, r, s′)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-based-methods 9\n",
        "struct PosteriorSamplingUpdate end\n",
        "\n",
        "function Base.rand(model::BayesianMDP)\n",
        "    𝒮, 𝒜 = model.𝒮, model.𝒜\n",
        "    T = zeros(length(𝒮), length(𝒜), length(𝒮))\n",
        "    for s in 𝒮\n",
        "        for a in 𝒜\n",
        "            T[s,a,:] = rand(model.D[s,a])\n",
        "        end\n",
        "    end\n",
        "    return MDP(T, model.R, model.γ)\n",
        "end\n",
        "\n",
        "function update!(planner::PosteriorSamplingUpdate, model, s, a, r, s′)\n",
        "    𝒫 = rand(model)\n",
        "    U = solve(𝒫).U\n",
        "    copy!(model.U, U)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 1\n",
        "mutable struct IncrementalEstimate\n",
        "\tμ # mean estimate\n",
        "\tα # learning rate function\n",
        "\tm # number of updates\n",
        "end\n",
        "\n",
        "function update!(model::IncrementalEstimate, x)\n",
        "\tmodel.m += 1\n",
        "\tmodel.μ += model.α(model.m) * (x - model.μ)\n",
        "\treturn model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 2\n",
        "mutable struct QLearning\n",
        "    𝒮 # state space (assumes 1:nstates)\n",
        "    𝒜 # action space (assumes 1:nactions)\n",
        "    γ # discount\n",
        "    Q # action value function\n",
        "    α # learning rate\n",
        "end\n",
        "\n",
        "lookahead(model::QLearning, s, a) = model.Q[s,a]\n",
        "\n",
        "function update!(model::QLearning, s, a, r, s′)\n",
        "    γ, Q, α = model.γ, model.Q, model.α\n",
        "    Q[s,a] += α*(r + γ*maximum(Q[s′,:]) - Q[s,a])\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 3\n",
        "mutable struct Sarsa\n",
        "    𝒮 # state space (assumes 1:nstates)\n",
        "    𝒜 # action space (assumes 1:nactions)\n",
        "    γ # discount\n",
        "    Q # action value function\n",
        "    α # learning rate\n",
        "    ℓ # most recent experience tuple (s,a,r)\n",
        "end\n",
        "\n",
        "lookahead(model::Sarsa, s, a) = model.Q[s,a]\n",
        "\n",
        "function update!(model::Sarsa, s, a, r, s′)\n",
        "    if model.ℓ != nothing\n",
        "        γ, Q, α, ℓ = model.γ, model.Q, model.α,  model.ℓ\n",
        "        model.Q[ℓ.s,ℓ.a] += α*(ℓ.r + γ*Q[s,a] - Q[ℓ.s,ℓ.a])\n",
        "    end\n",
        "    model.ℓ = (s=s, a=a, r=r)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 4\n",
        "mutable struct SarsaLambda\n",
        "    𝒮 # state space (assumes 1:nstates)\n",
        "    𝒜 # action space (assumes 1:nactions)\n",
        "    γ # discount\n",
        "    Q # action value function\n",
        "    N # trace\n",
        "    α # learning rate\n",
        "    λ # trace decay rate\n",
        "    ℓ # most recent experience tuple (s,a,r)\n",
        "end\n",
        "\n",
        "lookahead(model::SarsaLambda, s, a) = model.Q[s,a]\n",
        "\n",
        "function update!(model::SarsaLambda, s, a, r, s′)\n",
        "    if model.ℓ != nothing\n",
        "        γ, λ, Q, α, ℓ = model.γ, model.λ, model.Q, model.α, model.ℓ\n",
        "        model.N[ℓ.s,ℓ.a] += 1\n",
        "        δ = ℓ.r + γ*Q[s,a] - Q[ℓ.s,ℓ.a]\n",
        "        for s in model.𝒮\n",
        "            for a in model.𝒜\n",
        "                model.Q[s,a] += α*δ*model.N[s,a]\n",
        "                model.N[s,a] *= γ*λ\n",
        "            end\n",
        "        end\n",
        "    else\n",
        "    \tmodel.N[:,:] .= 0.0\n",
        "    end\n",
        "    model.ℓ = (s=s, a=a, r=r)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 5\n",
        "struct GradientQLearning\n",
        "    𝒜  # action space (assumes 1:nactions)\n",
        "    γ  # discount\n",
        "    Q  # parameterized action value function Q(θ,s,a)\n",
        "    ∇Q # gradient of action value function\n",
        "    θ  # action value function parameter\n",
        "    α  # learning rate\n",
        "end\n",
        "\n",
        "function lookahead(model::GradientQLearning, s, a)\n",
        "    return model.Q(model.θ, s,a)\n",
        "end\n",
        "\n",
        "function update!(model::GradientQLearning, s, a, r, s′)\n",
        "    𝒜, γ, Q, θ, α = model.𝒜, model.γ, model.Q, model.θ, model.α\n",
        "    u = maximum(Q(θ,s′,a′) for a′ in 𝒜)\n",
        "    Δ = (r + γ*u - Q(θ,s,a))*model.∇Q(θ,s,a)\n",
        "    θ[:] += α*scale_gradient(Δ, 1)\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### model-free-methods 6\n",
        "struct ReplayGradientQLearning\n",
        "    𝒜      # action space (assumes 1:nactions)\n",
        "    γ      # discount\n",
        "    Q      # parameterized action value function Q(θ,s,a)\n",
        "    ∇Q     # gradient of action value function\n",
        "    θ      # action value function parameter\n",
        "    α      # learning rate\n",
        "    buffer # circular memory buffer\n",
        "    m      # number of steps between gradient updates\n",
        "    m_grad # batch size\n",
        "end\n",
        "\n",
        "function lookahead(model::ReplayGradientQLearning, s, a)\n",
        "    return model.Q(model.θ, s,a)\n",
        "end\n",
        "\n",
        "function update!(model::ReplayGradientQLearning, s, a, r, s′)\n",
        "    𝒜, γ, Q, θ, α = model.𝒜, model.γ, model.Q, model.θ, model.α\n",
        "    buffer, m, m_grad = model.buffer, model.m, model.m_grad\n",
        "    if isfull(buffer)\n",
        "        U(s) = maximum(Q(θ,s,a) for a in 𝒜)\n",
        "        ∇Q(s,a,r,s′) = (r + γ*U(s′) - Q(θ,s,a))*model.∇Q(θ,s,a)\n",
        "        Δ = mean(∇Q(s,a,r,s′) for (s,a,r,s′) in rand(buffer, m_grad))\n",
        "        θ[:] += α*scale_gradient(Δ, 1)\n",
        "        for i in 1:m # discard oldest experiences\n",
        "            popfirst!(buffer)\n",
        "        end\n",
        "    else\n",
        "        push!(buffer, (s,a,r,s′))\n",
        "    end\n",
        "    return model\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 1\n",
        "struct BehavioralCloning\n",
        "    α     # step size\n",
        "    k_max # number of iterations\n",
        "    ∇logπ # log likelihood gradient\n",
        "end\n",
        "\n",
        "function optimize(M::BehavioralCloning, D, θ)\n",
        "\tα, k_max, ∇logπ = M.α, M.k_max, M.∇logπ\n",
        "\tfor k in 1:k_max\n",
        "\t\t∇ = mean(∇logπ(θ, a, s) for (s,a) in D)\n",
        "\t\tθ += α*∇\n",
        "\tend\n",
        "\treturn θ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 2\n",
        "struct CostSensitiveMultiClassifier\n",
        "\t𝒜     # action space\n",
        "    α     # step size\n",
        "    C     # cost function\n",
        "    k_max # number of iterations\n",
        "    ∇π    # policy likelihood gradient\n",
        "end\n",
        "\n",
        "function optimize(M::CostSensitiveMultiClassifier, D, θ)\n",
        "\t𝒜, α, C, k_max, ∇π = M.𝒜, M.α, M.C, M.k_max, M.∇π\n",
        "\tfor k in 1:k_max\n",
        "\t\t∇ = mean(sum(C(s,a,a_pred)*∇π(θ, a_pred, s)\n",
        "\t\t\t\tfor a_pred in 𝒜)\n",
        "\t\t\t\t\tfor (s,a) in D)\n",
        "\t\tθ -= α*∇\n",
        "\tend\n",
        "\treturn θ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 3\n",
        "struct DataSetAggregation\n",
        "\t𝒫     # problem with unknown reward function\n",
        "\tbc    # behavioral cloning struct\n",
        "\tk_max # number of iterations\n",
        "\tm     # number of rollouts per iteration\n",
        "\td     # rollout depth\n",
        "\tb     # initial state distribution\n",
        "\tπE    # expert\n",
        "\tπθ    # parameterized policy\n",
        "end\n",
        "\n",
        "function optimize(M::DataSetAggregation, D, θ)\n",
        "\t𝒫, bc, k_max, m = M.𝒫, M.bc, M.k_max, M.m\n",
        "\td, b, πE, πθ = M.d, M.b, M.πE, M.πθ\n",
        "\tθ = optimize(bc, D, θ)\n",
        "\tfor k in 2:k_max\n",
        "\t\tfor i in 1:m\n",
        "\t\t\ts = rand(b)\n",
        "\t\t\tfor j in 1:d\n",
        "\t\t\t\tpush!(D, (s, πE(s)))\n",
        "\t\t\t\ta = rand(πθ(θ, s))\n",
        "\t\t\t\ts = rand(𝒫.T(s, a))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\tθ = optimize(bc, D, θ)\n",
        "\tend\n",
        "\treturn θ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 4\n",
        "struct SEARN\n",
        "\t𝒫     # problem with unknown reward\n",
        "\tmc    # cost-sensitive multiclass classifier struct\n",
        "\tk_max # number of iterations\n",
        "\tm     # number of rollouts per iteration\n",
        "\td     # rollout depth\n",
        "\tb     # initial state distribution\n",
        "\tβ     # mixing scalar\n",
        "\tπE    # expert policy\n",
        "\tπθ    # parameterized policy\n",
        "end\n",
        "\n",
        "function optimize(M::SEARN, θ)\n",
        "\t𝒫, mc, k_max, m = M.𝒫, M.mc, M.k_max, M.m\n",
        "\td, b, β, πE, πθ = M.d, M.b, M.β, M.πE, M.πθ\n",
        "\tθs, π = Vector{Float64}[], s -> πE(s)\n",
        "    T, 𝒜 = 𝒫.T, 𝒫.𝒜\n",
        "\tfor k in 1:k_max\n",
        "        D = []\n",
        "\t\tfor i in 1:m\n",
        "\t\t\ts = rand(b)\n",
        "\t\t\tfor j in 1:d\n",
        "\t\t\t\tc = [rollout(𝒫, rand(T(s, a)), π, d-j) for a in 𝒜]\n",
        "\t\t\t\tc = maximum(c) .- c\n",
        "\n",
        "\t\t\t\tpush!(D, (s, c))\n",
        "\t\t\t\ts = rand(T(s, π(s)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\n",
        "\t\tθ = optimize(mc, D, θ)\n",
        "\t\tpush!(θs, θ)\n",
        "\n",
        "\t\tπ_hat = s -> rand(Categorical(πθ(θ, s)))\n",
        "\t\tπ = s -> rand() < β ? π_hat(s) : π(s)\n",
        "\tend\n",
        "\n",
        "\t# Compute a policy that does not contain the expert\n",
        "\tPπ = Categorical(normalize([(1-β)^(k_max-i) for i in 1:k_max],1))\n",
        "\treturn π = s -> rand(Categorical(πθ(θs[rand(Pπ)], s)))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 5\n",
        "struct SMILe\n",
        "\t𝒫     # problem with unknown reward\n",
        "\tbc    # Behavioral cloning struct\n",
        "\tk_max # number of iterations\n",
        "\tm     # number of rollouts per iteration\n",
        "\td     # rollout depth\n",
        "\tb     # initial state distribution\n",
        "\tβ     # mixing scalar (e.g., d^-3)\n",
        "\tπE    # expert policy\n",
        "\tπθ    # parameterized policy\n",
        "end\n",
        "\n",
        "function optimize(M::SMILe, θ)\n",
        "    𝒫, bc, k_max, m = M.𝒫, M.bc, M.k_max, M.m\n",
        "    d, b, β, πE, πθ = M.d, M.b, M.β, M.πE, M.πθ\n",
        "    𝒜, T = 𝒫.𝒜, 𝒫.T\n",
        "\tθs = []\n",
        "\tπ = s -> πE(s)\n",
        "\tfor k in 1:k_max\n",
        "\t\t# execute latest π to get new data set D\n",
        "        D = []\n",
        "\t\tfor i in 1:m\n",
        "\t\t\ts = rand(b)\n",
        "\t\t\tfor j in 1:d\n",
        "\t\t\t\tpush!(D, (s, πE(s)))\n",
        "\t\t\t\ta = π(s)\n",
        "\t\t\t\ts = rand(T(s, a))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\t\t# train new policy classifier\n",
        "\t\tθ = optimize(bc, D, θ)\n",
        "\t\tpush!(θs, θ)\n",
        "\t\t# compute a new policy mixture\n",
        "\t\tPπ = Categorical(normalize([(1-β)^(i-1) for i in 1:k],1))\n",
        "\t\tπ = s -> begin\n",
        "\t\t\tif rand() < (1-β)^(k-1)\n",
        "\t\t\t\treturn πE(s)\n",
        "\t\t\telse\n",
        "\t\t\t\treturn rand(Categorical(πθ(θs[rand(Pπ)], s)))\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\tPs = normalize([(1-β)^(i-1) for i in 1:k_max],1)\n",
        "    return Ps, θs\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 6\n",
        "struct InverseReinforcementLearning\n",
        "    𝒫  # problem\n",
        "    b  # initial state distribution\n",
        "    d  # depth\n",
        "    m  # number of samples\n",
        "    π  # parameterized policy\n",
        "    β  # binary feature mapping\n",
        "    μE # expert feature expectations\n",
        "    RL # reinforcement learning method\n",
        "    ϵ  # tolerance\n",
        "end\n",
        "\n",
        "function feature_expectations(M::InverseReinforcementLearning, π)\n",
        "    𝒫, b, m, d, β, γ = M.𝒫, M.b, M.m, M.d, M.β, M.𝒫.γ\n",
        "    μ(τ) = sum(γ^(k-1)*β(s, a) for (k,(s,a)) in enumerate(τ))\n",
        "    τs = [simulate(𝒫, rand(b), π, d) for i in 1:m]\n",
        "    return mean(μ(τ) for τ in τs)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 7\n",
        "function calc_weighting(M::InverseReinforcementLearning, μs)\n",
        "    μE = M.μE\n",
        "    k = length(μE)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, t)\n",
        "    @variable(model, ϕ[1:k] ≥ 0)\n",
        "    @objective(model, Max, t)\n",
        "    for μ in μs\n",
        "        @constraint(model, ϕ⋅μE ≥ ϕ⋅μ + t)\n",
        "    end\n",
        "    @constraint(model, ϕ⋅ϕ ≤ 1)\n",
        "    optimize!(model)\n",
        "    return (value(t), value.(ϕ))\n",
        "end\n",
        "\n",
        "function calc_policy_mixture(M::InverseReinforcementLearning, μs)\n",
        "    μE = M.μE\n",
        "    k = length(μs)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, λ[1:k] ≥ 0)\n",
        "    @objective(model, Min, (μE - sum(λ[i]*μs[i] for i in 1:k))⋅\n",
        "                            (μE - sum(λ[i]*μs[i] for i in 1:k)))\n",
        "    @constraint(model, sum(λ) == 1)\n",
        "    optimize!(model)\n",
        "    return value.(λ)\n",
        "end\n",
        "\n",
        "function optimize(M::InverseReinforcementLearning, θ)\n",
        "    π, ϵ, RL = M.π, M.ϵ, M.RL\n",
        "    θs = [θ]\n",
        "    μs = [feature_expectations(M, s->π(θ,s))]\n",
        "    while true\n",
        "        t, ϕ = calc_weighting(M, μs)\n",
        "        if t ≤ ϵ\n",
        "            break\n",
        "        end\n",
        "        copyto!(RL.ϕ, ϕ) # R(s,a) = ϕ⋅β(s,a)\n",
        "        θ = optimize(RL, π, θ)\n",
        "        push!(θs, θ)\n",
        "        push!(μs, feature_expectations(M, s->π(θ,s)))\n",
        "    end\n",
        "    λ = calc_policy_mixture(M, μs)\n",
        "    return λ, θs\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### imitation-learning 8\n",
        "struct MaximumEntropyIRL\n",
        "    𝒫     # problem\n",
        "    b     # initial state distribution\n",
        "    d     # depth\n",
        "    π     # parameterized policy π(θ,s)\n",
        "    Pπ    # parameterized policy likelihood π(θ, a, s)\n",
        "    ∇R    # reward function gradient\n",
        "    RL    # reinforcement learning method\n",
        "    α     # step size\n",
        "    k_max # number of iterations\n",
        "end\n",
        "\n",
        "function discounted_state_visitations(M::MaximumEntropyIRL, θ)\n",
        "\t𝒫, b, d, Pπ = M.𝒫, M.b, M.d, M.Pπ\n",
        "\t𝒮, 𝒜, T, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.T, 𝒫.γ\n",
        "\tb_sk = zeros(length(𝒫.𝒮), d)\n",
        "\tb_sk[:,1] = [pdf(b, s) for s in 𝒮]\n",
        "\tfor k in 2:d\n",
        "        for (si′, s′) in enumerate(𝒮)\n",
        "            b_sk[si′,k] = γ*sum(sum(b_sk[si,k-1]*Pπ(θ, a, s)*T(s, a, s′)\n",
        "\t\t\t\t\tfor (si,s) in enumerate(𝒮))\n",
        "\t\t\t\tfor a in 𝒜)\n",
        "\t\tend\n",
        "\tend\n",
        "    return normalize!(vec(mean(b_sk, dims=2)),1)\n",
        "end\n",
        "\n",
        "function optimize(M::MaximumEntropyIRL, D, ϕ, θ)\n",
        "\t𝒫, π, Pπ, ∇R, RL, α, k_max = M.𝒫, M.π, M.Pπ, M.∇R, M.RL, M.α, M.k_max\n",
        "    𝒮, 𝒜, γ, nD = 𝒫.𝒮, 𝒫.𝒜, 𝒫.γ, length(D)\n",
        "    for k in 1:k_max\n",
        "    \tcopyto!(RL.ϕ, ϕ) # update parameters\n",
        "\t\tθ = optimize(RL, π, θ)\n",
        "    \tb = discounted_state_visitations(M, θ)\n",
        "    \t∇Rτ = τ -> sum(γ^(i-1)*∇R(ϕ,s,a) for (i,(s,a)) in enumerate(τ))\n",
        "    \t∇f = sum(∇Rτ(τ) for τ in D) - nD*sum(b[si]*sum(Pπ(θ,a,s)*∇R(ϕ,s,a)\n",
        "        \t\t\tfor (ai,a) in enumerate(𝒜))\n",
        "        \t\tfor (si, s) in enumerate(𝒮))\n",
        "        ϕ += α*∇f\n",
        "    end\n",
        "    return ϕ, θ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 1\n",
        "struct POMDP\n",
        "    γ   # discount factor\n",
        "    𝒮   # state space\n",
        "    𝒜   # action space\n",
        "    𝒪   # observation space\n",
        "    T   # transition function\n",
        "    R   # reward function\n",
        "    O   # observation function\n",
        "    TRO # sample transition, reward, and observation\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 2\n",
        "function update(b::Vector{Float64}, 𝒫, a, o)\n",
        "\t𝒮, T, O = 𝒫.𝒮, 𝒫.T, 𝒫.O\n",
        "    b′ = similar(b)\n",
        "    for (i′, s′) in enumerate(𝒮)\n",
        "        po = O(a, s′, o)\n",
        "        b′[i′] = po * sum(T(s, a, s′) * b[i] for (i, s) in enumerate(𝒮))\n",
        "    end\n",
        "    if sum(b′) ≈ 0.0\n",
        "    \tfill!(b′, 1)\n",
        "    end\n",
        "    return normalize!(b′, 1)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 3\n",
        "struct KalmanFilter\n",
        "\tμb # mean vector\n",
        "\tΣb # covariance matrix\n",
        "end\n",
        "\n",
        "function update(b::KalmanFilter, 𝒫, a, o)\n",
        "\tμb, Σb = b.μb, b.Σb\n",
        "\tTs, Ta, Os = 𝒫.Ts, 𝒫.Ta, 𝒫.Os\n",
        "\tΣs, Σo = 𝒫.Σs, 𝒫.Σo\n",
        "\t# predict\n",
        "\tμp = Ts*μb + Ta*a\n",
        "\tΣp = Ts*Σb*Ts' + Σs\n",
        "\t# update\n",
        "\tΣpo = Σp*Os'\n",
        "\tK = Σpo/(Os*Σp*Os' + Σo)\n",
        "\tμb′ = μp + K*(o - Os*μp)\n",
        "\tΣb′ = (I - K*Os)*Σp\n",
        "\treturn KalmanFilter(μb′, Σb′)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 4\n",
        "struct ExtendedKalmanFilter\n",
        "\tμb # mean vector\n",
        "\tΣb # covariance matrix\n",
        "end\n",
        "\n",
        "import ForwardDiff: jacobian\n",
        "function update(b::ExtendedKalmanFilter, 𝒫, a, o)\n",
        "\tμb, Σb = b.μb, b.Σb\n",
        "\tfT, fO = 𝒫.fT, 𝒫.fO\n",
        "\tΣs, Σo = 𝒫.Σs, 𝒫.Σo\n",
        "\t# predict\n",
        "\tμp = fT(μb, a)\n",
        "\tTs = jacobian(s->fT(s, a), μb)\n",
        "\tOs = jacobian(fO, μp)\n",
        "\tΣp = Ts*Σb*Ts' + Σs\n",
        "\t# update\n",
        "\tΣpo = Σp*Os'\n",
        "\tK = Σpo/(Os*Σp*Os' + Σo)\n",
        "\tμb′ = μp + K*(o - fO(μp))\n",
        "\tΣb′ = (I - K*Os)*Σp\n",
        "\treturn ExtendedKalmanFilter(μb′, Σb′)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 5\n",
        "struct UnscentedKalmanFilter\n",
        "\tμb # mean vector\n",
        "\tΣb # covariance matrix\n",
        "\tλ  # spread parameter\n",
        "end\n",
        "\n",
        "function unscented_transform(μ, Σ, f, λ, ws)\n",
        "    n = length(μ)\n",
        "    Δ = cholesky((n + λ) * Σ).L\n",
        "\tS = [μ]\n",
        "\tfor i in 1:n\n",
        "\t\tpush!(S, μ + Δ[:,i])\n",
        "\t\tpush!(S, μ - Δ[:,i])\n",
        "\tend\n",
        "    S′ = f.(S)\n",
        "\tμ′ = sum(w*s for (w,s) in zip(ws, S′))\n",
        "\tΣ′ = sum(w*(s - μ′)*(s - μ′)' for (w,s) in zip(ws, S′))\n",
        "\treturn (μ′, Σ′, S, S′)\n",
        "end\n",
        "\n",
        "function update(b::UnscentedKalmanFilter, 𝒫, a, o)\n",
        "\tμb, Σb, λ = b.μb, b.Σb, b.λ\n",
        "\tfT, fO = 𝒫.fT, 𝒫.fO\n",
        "\tn = length(μb)\n",
        "\tws = [λ / (n + λ); fill(1/(2(n + λ)), 2n)]\n",
        "    # predict\n",
        "    μp, Σp, Sp, Sp′ = unscented_transform(μb, Σb, s->fT(s,a), λ, ws)\n",
        "\tΣp += 𝒫.Σs\n",
        "    # update\n",
        "    μo, Σo, So, So′ = unscented_transform(μp, Σp, fO, λ, ws)\n",
        "\tΣo += 𝒫.Σo\n",
        "\tΣpo = sum(w*(s - μp)*(s′ - μo)' for (w,s,s′) in zip(ws, So, So′))\n",
        "\tK = Σpo / Σo\n",
        "\tμb′ = μp + K*(o - μo)\n",
        "\tΣb′ = Σp - K*Σo*K'\n",
        "\treturn UnscentedKalmanFilter(μb′, Σb′, λ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 6\n",
        "struct ParticleFilter\n",
        "\tstates # vector of state samples\n",
        "end\n",
        "\n",
        "function update(b::ParticleFilter, 𝒫, a, o)\n",
        "\tT, O = 𝒫.T, 𝒫.O\n",
        "\tstates = [rand(T(s, a)) for s in b.states]\n",
        "\tweights = [O(a, s′, o) for s′ in states]\n",
        "\tD = SetCategorical(states, weights)\n",
        "\treturn ParticleFilter(rand(D, length(states)))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 7\n",
        "struct RejectionParticleFilter\n",
        "\tstates # vector of state samples\n",
        "end\n",
        "\n",
        "function update(b::RejectionParticleFilter, 𝒫, a, o)\n",
        "\tT, O = 𝒫.T, 𝒫.O\n",
        "\tstates = similar(b.states)\n",
        "\ti = 1\n",
        "\twhile i ≤ length(states)\n",
        "\t\ts = rand(b.states)\n",
        "\t\ts′ = rand(T(s,a))\n",
        "\t\tif rand(O(a,s′)) == o\n",
        "\t\t\tstates[i] = s′\n",
        "\t\t\ti += 1\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn RejectionParticleFilter(states)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 8\n",
        "struct InjectionParticleFilter\n",
        "\tstates # vector of state samples\n",
        "\tm_inject # number of samples to inject\n",
        "\tD_inject # injection distribution\n",
        "end\n",
        "\n",
        "function update(b::InjectionParticleFilter, 𝒫, a, o)\n",
        "\tT, O, m_inject, D_inject = 𝒫.T, 𝒫.O, b.m_inject, b.D_inject\n",
        "\tstates = [rand(T(s, a)) for s in b.states]\n",
        "\tweights = [O(a, s′, o) for s′ in states]\n",
        "\tD = SetCategorical(states, weights)\n",
        "\tm = length(states)\n",
        "\tstates = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))\n",
        "\treturn InjectionParticleFilter(states, m_inject, D_inject)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### beliefs 9\n",
        "mutable struct AdaptiveInjectionParticleFilter\n",
        "\tstates   # vector of state samples\n",
        "\tw_slow   # slow moving average\n",
        "\tw_fast   # fast moving average\n",
        "\tα_slow   # slow moving average parameter\n",
        "\tα_fast   # fast moving average parameter\n",
        "\tν        # injection parameter\n",
        "\tD_inject # injection distribution\n",
        "end\n",
        "\n",
        "function update(b::AdaptiveInjectionParticleFilter, 𝒫, a, o)\n",
        "\tT, O = 𝒫.T, 𝒫.O\n",
        "\tw_slow, w_fast, α_slow, α_fast, ν, D_inject =\n",
        "\t\tb.w_slow, b.w_fast, b.α_slow, b.α_fast, b.ν, b.D_inject\n",
        "\tstates = [rand(T(s, a)) for s in b.states]\n",
        "\tweights = [O(a, s′, o) for s′ in states]\n",
        "\tw_mean = mean(weights)\n",
        "\tw_slow += α_slow*(w_mean - w_slow)\n",
        "\tw_fast += α_fast*(w_mean - w_fast)\n",
        "\tm = length(states)\n",
        "\tm_inject = round(Int, m * max(0, 1.0 - ν*w_fast / w_slow))\n",
        "\tD = SetCategorical(states, weights)\n",
        "\tstates = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))\n",
        "\tb.w_slow, b.w_fast = w_slow, w_fast\n",
        "\treturn AdaptiveInjectionParticleFilter(states,\n",
        "\t\tw_slow, w_fast, α_slow, α_fast, ν, D_inject)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 1\n",
        "struct ConditionalPlan\n",
        "    a        # action to take at root\n",
        "    subplans # dictionary mapping observations to subplans\n",
        "end\n",
        "\n",
        "ConditionalPlan(a) = ConditionalPlan(a, Dict())\n",
        "\n",
        "(π::ConditionalPlan)() = π.a\n",
        "(π::ConditionalPlan)(o) = π.subplans[o]\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 2\n",
        "ConditionalPlan(π::Tuple) = ConditionalPlan(π[1], Dict(k=>ConditionalPlan(v) for (k,v) in π[2]))\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 3\n",
        "function lookahead(𝒫::POMDP, U, s, a)\n",
        "    𝒮, 𝒪, T, O, R, γ = 𝒫.𝒮, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    u′ = sum(T(s,a,s′)*sum(O(a,s′,o)*U(o,s′) for o in 𝒪) for s′ in 𝒮)\n",
        "    return R(s,a) + γ*u′\n",
        "end\n",
        "\n",
        "function evaluate_plan(𝒫::POMDP, π::ConditionalPlan, s)\n",
        "    U(o,s′) = evaluate_plan(𝒫, π(o), s′)\n",
        "    return isempty(π.subplans) ? 𝒫.R(s,π()) : lookahead(𝒫, U, s, π())\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 4\n",
        "function alphavector(𝒫::POMDP, π::ConditionalPlan)\n",
        "    return [evaluate_plan(𝒫, π, s) for s in 𝒫.𝒮]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 5\n",
        "struct AlphaVectorPolicy\n",
        "    𝒫 # POMDP problem\n",
        "    Γ # alpha vectors\n",
        "    a # actions associated with alpha vectors\n",
        "end\n",
        "\n",
        "function utility(π::AlphaVectorPolicy, b)\n",
        "    return maximum(α⋅b for α in π.Γ)\n",
        "end\n",
        "\n",
        "function (π::AlphaVectorPolicy)(b)\n",
        "    i = argmax([α⋅b for α in π.Γ])\n",
        "    return π.a[i]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 6\n",
        "function lookahead(𝒫::POMDP, U, b::Vector, a)\n",
        "    𝒮, 𝒪, T, O, R, γ = 𝒫.𝒮, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    r = sum(R(s,a)*b[i] for (i,s) in enumerate(𝒮))\n",
        "    Posa(o,s,a) = sum(O(a,s′,o)*T(s,a,s′) for s′ in 𝒮)\n",
        "    Poba(o,b,a) = sum(b[i]*Posa(o,s,a) for (i,s) in enumerate(𝒮))\n",
        "    return r + γ*sum(Poba(o,b,a)*U(update(b, 𝒫, a, o)) for o in 𝒪)\n",
        "end\n",
        "\n",
        "function greedy(𝒫::POMDP, U, b::Vector)\n",
        "    u, a = findmax(a->lookahead(𝒫, U, b, a), 𝒫.𝒜)\n",
        "    return (a=a, u=u)\n",
        "end\n",
        "\n",
        "struct LookaheadAlphaVectorPolicy\n",
        "    𝒫 # POMDP problem\n",
        "    Γ # alpha vectors\n",
        "end\n",
        "\n",
        "function utility(π::LookaheadAlphaVectorPolicy, b)\n",
        "    return maximum(α⋅b for α in π.Γ)\n",
        "end\n",
        "\n",
        "function greedy(π, b)\n",
        "    U(b) = utility(π, b)\n",
        "    return greedy(π.𝒫, U, b)\n",
        "end\n",
        "\n",
        "(π::LookaheadAlphaVectorPolicy)(b) = greedy(π, b).a\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 7\n",
        "function find_maximal_belief(α, Γ)\n",
        "\tm = length(α)\n",
        "\tif isempty(Γ)\n",
        "\t\treturn fill(1/m, m) # arbitrary belief\n",
        "\tend\n",
        "\tmodel = Model(GLPK.Optimizer)\n",
        "\t@variable(model, δ)\n",
        "\t@variable(model, b[i=1:m] ≥ 0)\n",
        "\t@constraint(model, sum(b) == 1.0)\n",
        "\tfor a in Γ\n",
        "\t\t@constraint(model, (α-a)⋅b ≥ δ)\n",
        "\tend\n",
        "\t@objective(model, Max, δ)\n",
        "\toptimize!(model)\n",
        "\treturn value(δ) > 0 ? value.(b) : nothing\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 8\n",
        "function find_dominating(Γ)\n",
        "    n = length(Γ)\n",
        "    candidates, dominating = trues(n), falses(n)\n",
        "    while any(candidates)\n",
        "        i = findfirst(candidates)\n",
        "        b = find_maximal_belief(Γ[i], Γ[dominating])\n",
        "        if b === nothing\n",
        "            candidates[i] = false\n",
        "        else\n",
        "            k = argmax([candidates[j] ? b⋅Γ[j] : -Inf for j in 1:n])\n",
        "            candidates[k], dominating[k] = false, true\n",
        "        end\n",
        "    end\n",
        "    return dominating\n",
        "end\n",
        "\n",
        "function prune(plans, Γ)\n",
        "    d = find_dominating(Γ)\n",
        "    return (plans[d], Γ[d])\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 9\n",
        "function value_iteration(𝒫::POMDP, k_max)\n",
        "    𝒮, 𝒜, R = 𝒫.𝒮, 𝒫.𝒜, 𝒫.R\n",
        "    plans = [ConditionalPlan(a) for a in 𝒜]\n",
        "    Γ = [[R(s,a) for s in 𝒮] for a in 𝒜]\n",
        "    plans, Γ = prune(plans, Γ)\n",
        "    for k in 2:k_max\n",
        "        plans, Γ = expand(plans, Γ, 𝒫)\n",
        "        plans, Γ = prune(plans, Γ)\n",
        "    end\n",
        "    return (plans, Γ)\n",
        "end\n",
        "\n",
        "function solve(M::ValueIteration, 𝒫::POMDP)\n",
        "    plans, Γ = value_iteration(𝒫, M.k_max)\n",
        "    return LookaheadAlphaVectorPolicy(𝒫, Γ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### exact-solutions 10\n",
        "function ConditionalPlan(𝒫::POMDP, a, plans)\n",
        "    subplans = Dict(o=>π for (o, π) in zip(𝒫.𝒪, plans))\n",
        "    return ConditionalPlan(a, subplans)\n",
        "end\n",
        "\n",
        "function combine_lookahead(𝒫::POMDP, s, a, Γo)\n",
        "    𝒮, 𝒪, T, O, R, γ = 𝒫.𝒮, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    U′(s′,i) = sum(O(a,s′,o)*α[i] for (o,α) in zip(𝒪,Γo))\n",
        "    return R(s,a) + γ*sum(T(s,a,s′)*U′(s′,i) for (i,s′) in enumerate(𝒮))\n",
        "end\n",
        "\n",
        "function combine_alphavector(𝒫::POMDP, a, Γo)\n",
        "    return [combine_lookahead(𝒫, s, a, Γo) for s in 𝒫.𝒮]\n",
        "end\n",
        "\n",
        "function expand(plans, Γ, 𝒫)\n",
        "    𝒮, 𝒜, 𝒪, T, O, R = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R\n",
        "    plans′, Γ′ = [], []\n",
        "    for a in 𝒜\n",
        "        # iterate over all possible mappings from observations to plans\n",
        "        for inds in product([eachindex(plans) for o in 𝒪]...)\n",
        "            πo = plans[[inds...]]\n",
        "            Γo = Γ[[inds...]]\n",
        "            π = ConditionalPlan(𝒫, a, πo)\n",
        "            α = combine_alphavector(𝒫, a, Γo)\n",
        "            push!(plans′, π)\n",
        "            push!(Γ′, α)\n",
        "        end\n",
        "    end\n",
        "    return (plans′, Γ′)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 1\n",
        "function alphavector_iteration(𝒫::POMDP, M, Γ)\n",
        "    for k in 1:M.k_max\n",
        "        Γ = update(𝒫, M, Γ)\n",
        "    end\n",
        "    return Γ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 2\n",
        "struct QMDP\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(𝒫::POMDP, M::QMDP, Γ)\n",
        "    𝒮, 𝒜, R, T, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.T, 𝒫.γ\n",
        "    Γ′ = [[R(s,a) + γ*sum(T(s,a,s′)*maximum(α′[j] for α′ in Γ)\n",
        "        for (j,s′) in enumerate(𝒮)) for s in 𝒮] for a in 𝒜]\n",
        "    return Γ′\n",
        "end\n",
        "\n",
        "function solve(M::QMDP, 𝒫::POMDP)\n",
        "    Γ = [zeros(length(𝒫.𝒮)) for a in 𝒫.𝒜]\n",
        "    Γ = alphavector_iteration(𝒫, M, Γ)\n",
        "    return AlphaVectorPolicy(𝒫, Γ, 𝒫.𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 3\n",
        "struct FastInformedBound\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(𝒫::POMDP, M::FastInformedBound, Γ)\n",
        "    𝒮, 𝒜, 𝒪, R, T, O, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.R, 𝒫.T, 𝒫.O, 𝒫.γ\n",
        "    Γ′ = [[R(s, a) + γ*sum(maximum(sum(O(a,s′,o)*T(s,a,s′)*α′[j]\n",
        "        for (j,s′) in enumerate(𝒮)) for α′ in Γ) for o in 𝒪)\n",
        "        for s in 𝒮] for a in 𝒜]\n",
        "    return Γ′\n",
        "end\n",
        "\n",
        "function solve(M::FastInformedBound, 𝒫::POMDP)\n",
        "    Γ = [zeros(length(𝒫.𝒮)) for a in 𝒫.𝒜]\n",
        "    Γ = alphavector_iteration(𝒫, M, Γ)\n",
        "    return AlphaVectorPolicy(𝒫, Γ, 𝒫.𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 4\n",
        "function baws_lowerbound(𝒫::POMDP)\n",
        "    𝒮, 𝒜, R, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.γ\n",
        "    r = maximum(minimum(R(s, a) for s in 𝒮) for a in 𝒜) / (1-γ)\n",
        "    α = fill(r, length(𝒮))\n",
        "    return α\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 5\n",
        "function blind_lowerbound(𝒫, k_max)\n",
        "    𝒮, 𝒜, T, R, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.T, 𝒫.R, 𝒫.γ\n",
        "    Q(s,a,α) = R(s,a) + γ*sum(T(s,a,s′)*α[j] for (j,s′) in enumerate(𝒮))\n",
        "    Γ = [baws_lowerbound(𝒫) for a in 𝒜]\n",
        "    for k in 1:k_max\n",
        "        Γ = [[Q(s,a,α) for s in 𝒮] for (α,a) in zip(Γ, 𝒜)]\n",
        "    end\n",
        "    return Γ\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 6\n",
        "function backup(𝒫::POMDP, Γ, b)\n",
        "    𝒮, 𝒜, 𝒪, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.γ\n",
        "    R, T, O = 𝒫.R, 𝒫.T, 𝒫.O\n",
        "    Γa = []\n",
        "    for a in 𝒜\n",
        "        Γao = []\n",
        "        for o in 𝒪\n",
        "            b′ = update(b, 𝒫, a, o)\n",
        "            push!(Γao, argmax(α->α⋅b′, Γ))\n",
        "        end\n",
        "        α = [R(s, a) + γ*sum(sum(T(s, a, s′)*O(a, s′, o)*Γao[i][j]\n",
        "            for (j,s′) in enumerate(𝒮)) for (i,o) in enumerate(𝒪))\n",
        "            for s in 𝒮]\n",
        "        push!(Γa, α)\n",
        "    end\n",
        "    return argmax(α->α⋅b, Γa)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 7\n",
        "struct PointBasedValueIteration\n",
        "    B     # set of belief points\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(𝒫::POMDP, M::PointBasedValueIteration, Γ)\n",
        "    return [backup(𝒫, Γ, b) for b in M.B]\n",
        "end\n",
        "\n",
        "function solve(M::PointBasedValueIteration, 𝒫)\n",
        "    Γ = fill(baws_lowerbound(𝒫), length(𝒫.𝒜))\n",
        "    Γ = alphavector_iteration(𝒫, M, Γ)\n",
        "    return LookaheadAlphaVectorPolicy(𝒫, Γ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 8\n",
        "struct RandomizedPointBasedValueIteration\n",
        "    B     # set of belief points\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function update(𝒫::POMDP, M::RandomizedPointBasedValueIteration, Γ)\n",
        "    Γ′, B′ = [], copy(M.B)\n",
        "    while !isempty(B′)\n",
        "        b = rand(B′)\n",
        "        α = argmax(α->α⋅b, Γ)\n",
        "        α′ = backup(𝒫, Γ, b)\n",
        "        if α′⋅b ≥ α⋅b\n",
        "            push!(Γ′, α′)\n",
        "        else\n",
        "            push!(Γ′, α)\n",
        "        end\n",
        "        filter!(b->maximum(α⋅b for α in Γ′) <\n",
        "            maximum(α⋅b for α in Γ), B′)\n",
        "    end\n",
        "    return Γ′\n",
        "end\n",
        "\n",
        "function solve(M::RandomizedPointBasedValueIteration, 𝒫)\n",
        "    Γ = [baws_lowerbound(𝒫)]\n",
        "    Γ = alphavector_iteration(𝒫, M, Γ)\n",
        "    return LookaheadAlphaVectorPolicy(𝒫, Γ)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 9\n",
        "struct SawtoothPolicy\n",
        "    𝒫 # POMDP problem\n",
        "    V # dictionary mapping beliefs to utilities\n",
        "end\n",
        "\n",
        "function basis(𝒫)\n",
        "    n = length(𝒫.𝒮)\n",
        "    e(i) = [j == i ? 1.0 : 0.0 for j in 1:n]\n",
        "    return [e(i) for i in 1:n]\n",
        "end\n",
        "\n",
        "function utility(π::SawtoothPolicy, b)\n",
        "    𝒫, V = π.𝒫, π.V\n",
        "    if haskey(V, b)\n",
        "        return V[b]\n",
        "    end\n",
        "    n = length(𝒫.𝒮)\n",
        "    E = basis(𝒫)\n",
        "    u = sum(V[E[i]] * b[i] for i in 1:n)\n",
        "    for (b′, u′) in V\n",
        "        if b′ ∉ E\n",
        "            i = argmax([norm(b-e, 1) - norm(b′-e, 1) for e in E])\n",
        "            w = [norm(b - e, 1) for e in E]\n",
        "            w[i] = norm(b - b′, 1)\n",
        "            w /= sum(w)\n",
        "            w = [1 - wi for wi in w]\n",
        "            α = [V[e] for e in E]\n",
        "            α[i] = u′\n",
        "            u = min(u, w⋅α)\n",
        "        end\n",
        "    end\n",
        "    return u\n",
        "end\n",
        "\n",
        "(π::SawtoothPolicy)(b) = greedy(π, b).a\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 10\n",
        "struct SawtoothIteration\n",
        "    V     # initial mapping from beliefs to utilities\n",
        "    B     # beliefs to compute values including those in V map\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::SawtoothIteration, 𝒫::POMDP)\n",
        "    E = basis(𝒫)\n",
        "    π = SawtoothPolicy(𝒫, M.V)\n",
        "    for k in 1:M.k_max\n",
        "        V = Dict(b => (b ∈ E ? M.V[b] : greedy(π, b).u) for b in M.B)\n",
        "        π = SawtoothPolicy(𝒫, V)\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 11\n",
        "function randstep(𝒫::POMDP, b, a)\n",
        "\ts = rand(SetCategorical(𝒫.𝒮, b))\n",
        "\ts′, r, o = 𝒫.TRO(s, a)\n",
        "\tb′ = update(b, 𝒫, a, o)\n",
        "\treturn b′, r\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 12\n",
        "function random_belief_expansion(𝒫, B)\n",
        "\tB′ = copy(B)\n",
        "\tfor b in B\n",
        "\t\ta = rand(𝒫.𝒜)\n",
        "\t\tb′, r = randstep(𝒫, b, a)\n",
        "\t\tpush!(B′, b′)\n",
        "\tend\n",
        "\treturn unique!(B′)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 13\n",
        "function exploratory_belief_expansion(𝒫, B)\n",
        "    B′ = copy(B)\n",
        "    for b in B\n",
        "        best = (b=copy(b), d=0.0)\n",
        "        for a in 𝒫.𝒜\n",
        "            b′, r = randstep(𝒫, b, a)\n",
        "            d = minimum(norm(b - b′, 1) for b in B′)\n",
        "            if d > best.d\n",
        "                best = (b=b′, d=d)\n",
        "            end\n",
        "        end\n",
        "        push!(B′, best.b)\n",
        "    end\n",
        "    return unique!(B′)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 14\n",
        "function directed_belief_expansion(π, b)\n",
        "    𝒫, 𝒮, 𝒜, 𝒪 = π.𝒫, π.𝒫.𝒮, π.𝒫.𝒜, π.𝒫.𝒪\n",
        "    T, O, R, γ = π.𝒫.T, π.𝒫.O, π.𝒫.R, π.𝒫.γ\n",
        "    n, Γ, α = length(𝒮), π.Γ, argmax(α -> α⋅b, π.Γ)\n",
        "\n",
        "    B = []\n",
        "    for (a, o) in Iterators.product(𝒜, 𝒪)\n",
        "        b′ = update(b, 𝒫, π(b), o)\n",
        "        α′ = argmax(α -> α⋅b′, Γ)\n",
        "        backup(a′) = [sum(T(s, a′, s′) * O(a′, s′, o) * α′[s′Index]\n",
        "                      for (s′Index, s′) in enumerate(𝒮)) for s in 𝒮]\n",
        "        β = backup(a) - backup(π(b′))\n",
        "\n",
        "        model = Model(Ipopt.Optimizer)\n",
        "        @variable(model, belief[1:n] >= 0.0)\n",
        "        @objective(model, Max, belief⋅β)\n",
        "        for α′′ in Γ\n",
        "            @constraint(model, belief⋅α >= belief⋅α′′)\n",
        "        end\n",
        "        @constraint(model, sum(belief[s] for s in 1:n) == 1.0)\n",
        "        optimize!(model)\n",
        "\n",
        "        println(termination_status(model))\n",
        "        println(objective_value(model))\n",
        "        if (termination_status(model) in [MOI.OPTIMAL,MOI.LOCALLY_SOLVED,\n",
        "                    MOI.ALMOST_OPTIMAL, MOI.ALMOST_LOCALLY_SOLVED] &&\n",
        "                objective_value(model) > 0.0)\n",
        "            push!(B, value.(belief))\n",
        "        end\n",
        "    end\n",
        "\n",
        "    return unique(B)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 15\n",
        "struct SawtoothHeuristicSearch\n",
        "    b     # initial belief\n",
        "    δ     # gap threshold\n",
        "    d     # depth\n",
        "    k_max # maximum number of iterations\n",
        "    k_fib # number of iterations for fast informed bound\n",
        "end\n",
        "\n",
        "function explore!(M::SawtoothHeuristicSearch, 𝒫, πhi, πlo, b, d=0)\n",
        "    𝒮, 𝒜, 𝒪, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.γ\n",
        "    ϵ(b′) = utility(πhi, b′) - utility(πlo, b′)\n",
        "    if d ≥ M.d || ϵ(b) ≤ M.δ / γ^d\n",
        "        return\n",
        "    end\n",
        "    a = πhi(b)\n",
        "    o = argmax(o -> ϵ(update(b, 𝒫, a, o)), 𝒪)\n",
        "    b′ = update(b, 𝒫, a, o)\n",
        "    explore!(M, 𝒫, πhi, πlo, b′, d+1)\n",
        "    if b′ ∉ basis(𝒫)\n",
        "        πhi.V[b′] = greedy(πhi, b′).u\n",
        "    end\n",
        "    push!(πlo.Γ, backup(𝒫, πlo.Γ, b′))\n",
        "end\n",
        "\n",
        "function solve(M::SawtoothHeuristicSearch, 𝒫::POMDP)\n",
        "    πfib = solve(FastInformedBound(M.k_fib), 𝒫)\n",
        "    Vhi = Dict(e => utility(πfib, e) for e in basis(𝒫))\n",
        "    πhi = SawtoothPolicy(𝒫, Vhi)\n",
        "    πlo = LookaheadAlphaVectorPolicy(𝒫, [baws_lowerbound(𝒫)])\n",
        "    for i in 1:M.k_max\n",
        "        explore!(M, 𝒫, πhi, πlo, M.b)\n",
        "        if utility(πhi, M.b) - utility(πlo, M.b) < M.δ\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "    return πlo\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 16\n",
        "struct FreudenthalTriangulation\n",
        "    n::Int # dimensionality\n",
        "    m::Int # granularity\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    vertices(T::FreudenthalTriangulation)\n",
        "Construct the list of Freudenthal vertices. The vertices are represented by a list of `n` dimensional vectors.\n",
        "\"\"\"\n",
        "function vertices(T::FreudenthalTriangulation)\n",
        "    V = Vector{Int}[]\n",
        "    v = Vector{Int}(undef, T.n)\n",
        "    v[1] = T.m\n",
        "    _vertices!(V, v, 2)\n",
        "    return V\n",
        "end\n",
        "\n",
        "function _vertices!(V::Vector{Vector{Int}}, v::Vector{Int}, i::Int)\n",
        "    n = length(v)\n",
        "    if i > n\n",
        "        push!(V, copy(v))\n",
        "        return\n",
        "    end\n",
        "    for k in 0 : v[i-1]\n",
        "        v[i] = k\n",
        "        _vertices!(V, v, i+1)\n",
        "    end\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    _freudenthal_simplex(x::Vector{Float64})\n",
        "Returns the list of vertices of the simplex of point `x` in the Freudenthal grid.\n",
        "\"\"\"\n",
        "function _freudenthal_simplex(x::Vector{Float64})\n",
        "    n = length(x)\n",
        "    V = Vector{Vector{Int}}(undef, n+1)\n",
        "    V[1] = floor.(Int, x)\n",
        "    d = x - V[1]\n",
        "    p = sortperm(d, rev=true)\n",
        "    for i in 2 : n+1\n",
        "        V[i] = copy(V[i-1])\n",
        "        V[i][p[i-1]] += 1\n",
        "    end\n",
        "    return V\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    _barycentric_coordinates(x::Vector{Float64}, V::Vector{Vector{Int}})\n",
        "Given a point `x` and its simplex `V` in the Freudenthal grid, returns the barycentric coordinates\n",
        "of `x` in the grid. `V` must be in the same order as provided by the output of `freudenthal_simplex`\n",
        "\"\"\"\n",
        "function _barycentric_coordinates(x::Vector{Float64}, V::Vector{Vector{Int}})\n",
        "    d = x - V[1]\n",
        "    p = sortperm(d, rev=true)\n",
        "    n = length(x)\n",
        "    λ = Vector{Float64}(undef, n+1)\n",
        "    λ[n+1] = d[p[n]]\n",
        "    for i in n:-1:2\n",
        "        λ[i] = d[p[i-1]] - d[p[i]]\n",
        "    end\n",
        "    λ[1] = 1.0 - sum(λ[2:end])\n",
        "    return λ\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    simplex(T::FreudenthalTriangulation, x::Vector{Float64})\n",
        "Given a point `x`, returns the simplex of the point `x` and the barycentric coordinates of `x` in the grid.\n",
        "\"\"\"\n",
        "function simplex(T::FreudenthalTriangulation, x::Vector{Float64})\n",
        "    V = _freudenthal_simplex(x)\n",
        "    return V, _barycentric_coordinates(x, V)\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "    _to_belief(x)\n",
        "Transform a point `x` in the Freudenthal space to a point in the belief space.\n",
        "`m` is the resolution of the Freudenthal grid.\n",
        "\"\"\"\n",
        "_to_belief(x) = (push!(x[1:end-1] - x[2:end], x[end]))./x[1]\n",
        "\n",
        "\"\"\"\n",
        "    _to_freudenthal(b, m::Int64)\n",
        "Transform a point `b` in the belief space to a point in the Freudenthal space.\n",
        "`m` is the resolution of the Freudenthal grid.\n",
        "\"\"\"\n",
        "_to_freudenthal(b, m::Int64) = [sum(b[k] for k in i : length(b))*m for i in 1 : length(b)]\n",
        "\n",
        "belief_vertices(T::FreudenthalTriangulation) = _to_belief.(vertices(T))\n",
        "\n",
        "function belief_simplex(T::FreudenthalTriangulation, b)\n",
        "    x = _to_freudenthal(b, T.m)\n",
        "    V, λ = simplex(T, x)\n",
        "    B = _to_belief.(V)\n",
        "    valid = λ .> √eps()\n",
        "    return B[valid], λ[valid]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 17\n",
        "struct TriangulatedPolicy\n",
        "    𝒫 # POMDP problem\n",
        "    V # dictionary mapping beliefs to utilities\n",
        "    B # beliefs\n",
        "    T # Freudenthal triangulation\n",
        "end\n",
        "\n",
        "function TriangulatedPolicy(𝒫::POMDP, m)\n",
        "    T = FreudenthalTriangulation(length(𝒫.𝒮), m)\n",
        "    B = belief_vertices(T)\n",
        "    V = Dict(b => 0.0 for b in B)\n",
        "    return TriangulatedPolicy(𝒫, V, B, T)\n",
        "end\n",
        "\n",
        "function utility(π::TriangulatedPolicy, b)\n",
        "    B, λ = belief_simplex(π.T, b)\n",
        "    return sum(λi*π.V[b] for (λi, b) in zip(λ, B))\n",
        "end\n",
        "\n",
        "(π::TriangulatedPolicy)(b) = greedy(π, b).a\n",
        "####################\n",
        "\n",
        "#################### offline-approximations 18\n",
        "struct TriangulatedIteration\n",
        "    m     # granularity\n",
        "    k_max # maximum number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::TriangulatedIteration, 𝒫)\n",
        "    π = TriangulatedPolicy(𝒫, M.m)\n",
        "    U(b) = utility(π, b)\n",
        "    for k in 1:M.k_max\n",
        "        U′ = [greedy(𝒫, U, b).u for b in π.B]\n",
        "        for (b, u′) in zip(π.B, U′)\n",
        "            π.V[b] = u′\n",
        "        end\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 1\n",
        "struct BeliefExplorationPolicy\n",
        "\t𝒫\n",
        "\tN::Dict\n",
        "\tQ::Dict\n",
        "\tc::Float64\n",
        "end\n",
        "\n",
        "function (π::BeliefExplorationPolicy)(h)\n",
        "\t𝒜, N, Q, c = π.𝒫.𝒜, π.N, π.Q, π.c\n",
        "\tNh = sum(get(N, (h,a), 0) for a in 𝒜)\n",
        "\tbest = (a=nothing, u=-Inf)\n",
        "\tfor a in 𝒜\n",
        "\t\tu = Q[(h,a)] + c*(N[(h,a)] == 0 ? Inf :\n",
        "\t\t\t\t\t\t\tsqrt(log(Nh)/N[(h,a)]))\n",
        "\t\tif u > best.u\n",
        "\t\t\tbest = (a=a, u=u)\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn best.a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 2\n",
        "struct HistoryMonteCarloTreeSearch\n",
        "\t𝒫 # problem\n",
        "\tN # visit counts\n",
        "\tQ # action value estimates\n",
        "\td # depth\n",
        "\tm # number of simulations\n",
        "\tc # exploration constant\n",
        "\tU # value function estimate\n",
        "end\n",
        "\n",
        "function explore(π::HistoryMonteCarloTreeSearch, h)\n",
        "    𝒜, N, Q, c = π.𝒫.𝒜, π.N, π.Q, π.c\n",
        "    Nh = sum(get(N, (h,a), 0) for a in 𝒜)\n",
        "    return argmax(a->Q[(h,a)] + c*bonus(N[(h,a)], Nh), 𝒜)\n",
        "end\n",
        "\n",
        "function simulate(π::HistoryMonteCarloTreeSearch, s, h, d)\n",
        "    if d ≤ 0\n",
        "        return π.U(s)\n",
        "    end\n",
        "    𝒫, N, Q, c = π.𝒫, π.N, π.Q, π.c\n",
        "    𝒮, 𝒜, TRO, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.TRO, 𝒫.γ\n",
        "    if !haskey(N, (h, first(𝒜)))\n",
        "        for a in 𝒜\n",
        "            N[(h,a)] = 0\n",
        "            Q[(h,a)] = 0.0\n",
        "        end\n",
        "        return π.U(s)\n",
        "    end\n",
        "    a = explore(π, h)\n",
        "    s′, r, o = TRO(s,a)\n",
        "    q = r + γ*simulate(π, s′, vcat(h, (a,o)), d-1)\n",
        "    N[(h,a)] += 1\n",
        "    Q[(h,a)] += (q-Q[(h,a)])/N[(h,a)]\n",
        "    return q\n",
        "end\n",
        "\n",
        "function (π::HistoryMonteCarloTreeSearch)(b, h=[])\n",
        "    for i in 1:π.m\n",
        "        s = rand(SetCategorical(π.𝒫.𝒮, b))\n",
        "        simulate(π, s, h, π.d)\n",
        "    end\n",
        "    return argmax(a->π.Q[(h,a)], π.𝒫.𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 3\n",
        "struct DeterminizedParticle\n",
        "    s # state\n",
        "    i # scenario index\n",
        "    j # depth index\n",
        "end\n",
        "\n",
        "function successor(𝒫, Φ, ϕ, a)\n",
        "    𝒮, 𝒪, T, O = 𝒫.𝒮, 𝒫.𝒪, 𝒫.T, 𝒫.O\n",
        "    p = 0.0\n",
        "    for (s′, o) in product(𝒮, 𝒪)\n",
        "        p += T(ϕ.s, a, s′) * O(a, s′, o)\n",
        "        if p ≥ Φ[ϕ.i, ϕ.j]\n",
        "            return (s′, o)\n",
        "        end\n",
        "    end\n",
        "    return last(𝒮), last(𝒪)\n",
        "end\n",
        "\n",
        "function possible_observations(𝒫, Φ, b, a)\n",
        "    𝒪 = []\n",
        "    for ϕ in b\n",
        "        s′, o = successor(𝒫, Φ, ϕ, a)\n",
        "        push!(𝒪, o)\n",
        "    end\n",
        "    return unique(𝒪)\n",
        "end\n",
        "\n",
        "function update(b, Φ, 𝒫, a, o)\n",
        "    b′ = []\n",
        "    for ϕ in b\n",
        "        s′, o′ = successor(𝒫, Φ, ϕ, a)\n",
        "        if o == o′\n",
        "            push!(b′, DeterminizedParticle(s′, ϕ.i, ϕ.j + 1))\n",
        "        end\n",
        "    end\n",
        "    return b′\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 4\n",
        "struct DeterminizedSparseTreeSearch\n",
        "\t𝒫 # problem\n",
        "    d # depth\n",
        "    Φ # m×d determinizing matrix\n",
        "    U # value function to use at leaf nodes\n",
        "end\n",
        "\n",
        "function determinized_sparse_tree_search(𝒫, b, d, Φ, U)\n",
        "    𝒮, 𝒜, 𝒪, T, R, O, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.T, 𝒫.R, 𝒫.O, 𝒫.γ\n",
        "    if d == 0\n",
        "        return (a=nothing, u=U(b))\n",
        "    end\n",
        "    best = (a=nothing, u=-Inf)\n",
        "    for a in 𝒜\n",
        "        u = sum(R(ϕ.s, a) for ϕ in b) / length(b)\n",
        "        for o in possible_observations(𝒫, Φ, b, a)\n",
        "            Poba = sum(sum(O(a,s′,o)*T(ϕ.s,a,s′) for s′ in 𝒮)\n",
        "                       for ϕ in b) / length(b)\n",
        "            b′ = update(b, Φ, 𝒫, a, o)\n",
        "            u′ = determinized_sparse_tree_search(𝒫,b′,d-1,Φ,U).u\n",
        "            u += γ*Poba*u′\n",
        "        end\n",
        "        if u > best.u\n",
        "            best = (a=a, u=u)\n",
        "        end\n",
        "    end\n",
        "    return best\n",
        "end\n",
        "\n",
        "function determinized_belief(b, 𝒫, m)\n",
        "    particles = []\n",
        "    for i in 1:m\n",
        "        s = rand(SetCategorical(𝒫.𝒮, b))\n",
        "\t\tpush!(particles, DeterminizedParticle(s, i, 1))\n",
        "    end\n",
        "    return particles\n",
        "end\n",
        "\n",
        "function (π::DeterminizedSparseTreeSearch)(b)\n",
        "\tparticles = determinized_belief(b, π.𝒫, size(π.Φ,1))\n",
        "    return determinized_sparse_tree_search(π.𝒫,particles,π.d,π.Φ,π.U).a\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### online-approximations 5\n",
        "struct GapHeuristicSearch\n",
        "    𝒫     # problem\n",
        "    Ulo   # lower bound on value function\n",
        "    Uhi   # upper bound on value function\n",
        "    δ     # gap threshold\n",
        "    k_max # maximum number of simulations\n",
        "    d_max # maximum depth\n",
        "end\n",
        "\n",
        "function heuristic_search(π::GapHeuristicSearch, Ulo, Uhi, b, d)\n",
        "    𝒫, δ = π.𝒫, π.δ\n",
        "    𝒮, 𝒜, 𝒪, R, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.R, 𝒫.γ\n",
        "    B = Dict((a,o)=>update(b,𝒫,a,o) for (a,o) in product(𝒜,𝒪))\n",
        "    B = merge(B, Dict(()=>copy(b)))\n",
        "    for (ao, b′) in B\n",
        "        if !haskey(Uhi, b′)\n",
        "            Ulo[b′], Uhi[b′] = π.Ulo(b′), π.Uhi(b′)\n",
        "        end\n",
        "    end\n",
        "    if d == 0 || Uhi[b] - Ulo[b] ≤ δ\n",
        "        return\n",
        "    end\n",
        "    a = argmax(a -> lookahead(𝒫,b′->Uhi[b′],b,a), 𝒜)\n",
        "    o = argmax(o -> Uhi[B[(a, o)]] - Ulo[B[(a, o)]], 𝒪)\n",
        "    b′ = update(b,𝒫,a,o)\n",
        "    heuristic_search(π,Ulo,Uhi,b′,d-1)\n",
        "    Ulo[b] = maximum(lookahead(𝒫,b′->Ulo[b′],b,a) for a in 𝒜)\n",
        "    Uhi[b] = maximum(lookahead(𝒫,b′->Uhi[b′],b,a) for a in 𝒜)\n",
        "end\n",
        "\n",
        "function (π::GapHeuristicSearch)(b)\n",
        "    𝒫, k_max, d_max, δ = π.𝒫, π.k_max, π.d_max, π.δ\n",
        "    Ulo = Dict{Vector{Float64}, Float64}()\n",
        "    Uhi = Dict{Vector{Float64}, Float64}()\n",
        "    for i in 1:k_max\n",
        "        heuristic_search(π, Ulo, Uhi, b, d_max)\n",
        "        if Uhi[b] - Ulo[b] < δ\n",
        "            break\n",
        "        end\n",
        "    end\n",
        "    return argmax(a -> lookahead(𝒫,b′->Ulo[b′],b,a), 𝒫.𝒜)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 1\n",
        "mutable struct ControllerPolicy\n",
        "    𝒫 # problem\n",
        "    X # set of controller nodes\n",
        "    ψ # action selection distribution\n",
        "    η # successor selection distribution\n",
        "end\n",
        "\n",
        "function (π::ControllerPolicy)(x)\n",
        "    𝒜, ψ = π.𝒫.𝒜, π.ψ\n",
        "    dist = [ψ[x, a] for a in 𝒜]\n",
        "    return rand(SetCategorical(𝒜, dist))\n",
        "end\n",
        "\n",
        "function update(π::ControllerPolicy, x, a, o)\n",
        "    X, η = π.X, π.η\n",
        "    dist = [η[x, a, o, x′] for x′ in X]\n",
        "    return rand(SetCategorical(X, dist))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 2\n",
        "function utility(π::ControllerPolicy, U, x, s)\n",
        "    𝒮, 𝒜, 𝒪 = π.𝒫.𝒮, π.𝒫.𝒜, π.𝒫.𝒪\n",
        "    T, O, R, γ = π.𝒫.T, π.𝒫.O, π.𝒫.R, π.𝒫.γ\n",
        "    X, ψ, η = π.X, π.ψ, π.η\n",
        "    U′(a,s′,o) = sum(η[x,a,o,x′]*U[x′,s′] for x′ in X)\n",
        "    U′(a,s′) = T(s,a,s′)*sum(O(a,s′,o)*U′(a,s′,o) for o in 𝒪)\n",
        "    U′(a) = R(s,a) + γ*sum(U′(a,s′) for s′ in 𝒮)\n",
        "    return sum(ψ[x,a]*U′(a) for a in 𝒜)\n",
        "end\n",
        "\n",
        "function iterative_policy_evaluation(π::ControllerPolicy, k_max)\n",
        "    𝒮, X = π.𝒫.𝒮, π.X\n",
        "    U = Dict((x, s) => 0.0 for x in X, s in 𝒮)\n",
        "    for k in 1:k_max\n",
        "        U = Dict((x, s) => utility(π, U, x, s) for x in X, s in 𝒮)\n",
        "    end\n",
        "    return U\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 3\n",
        "struct ControllerPolicyIteration\n",
        "    k_max    # number of iterations\n",
        "    eval_max # number of evaluation iterations\n",
        "end\n",
        "\n",
        "function solve(M::ControllerPolicyIteration, 𝒫::POMDP)\n",
        "    𝒜, 𝒪, k_max, eval_max = 𝒫.𝒜, 𝒫.𝒪, M.k_max, M.eval_max\n",
        "    X = [1]\n",
        "    ψ = Dict((x, a) => 1.0 / length(𝒜) for x in X, a in 𝒜)\n",
        "    η = Dict((x, a, o, x′) => 1.0 for x in X, a in 𝒜, o in 𝒪, x′ in X)\n",
        "    π = ControllerPolicy(𝒫, X, ψ, η)\n",
        "    for i in 1:k_max\n",
        "        prevX = copy(π.X)\n",
        "        U = iterative_policy_evaluation(π, eval_max)\n",
        "        policy_improvement!(π, U, prevX)\n",
        "        prune!(π, U, prevX)\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "\n",
        "function policy_improvement!(π::ControllerPolicy, U, prevX)\n",
        "    𝒮, 𝒜, 𝒪 = π.𝒫.𝒮, π.𝒫.𝒜, π.𝒫.𝒪\n",
        "    X, ψ, η = π.X, π.ψ, π.η\n",
        "    repeatX𝒪 = fill(X, length(𝒪))\n",
        "    assign𝒜X′ = vec(collect(product(𝒜, repeatX𝒪...)))\n",
        "    for ax′ in assign𝒜X′\n",
        "        x, a = maximum(X) + 1, ax′[1]\n",
        "        push!(X, x)\n",
        "        successor(o) = ax′[findfirst(isequal(o), 𝒪) + 1]\n",
        "        U′(o,s′) = U[successor(o), s′]\n",
        "        for s in 𝒮\n",
        "            U[x, s] = lookahead(π.𝒫, U′, s, a)\n",
        "        end\n",
        "        for a′ in 𝒜\n",
        "            ψ[x, a′] = a′ == a ? 1.0 : 0.0\n",
        "            for (o, x′) in product(𝒪, prevX)\n",
        "                η[x, a′, o, x′] = x′ == successor(o) ? 1.0 : 0.0\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    for (x, a, o, x′) in product(X, 𝒜, 𝒪, X)\n",
        "        if !haskey(η, (x, a, o, x′))\n",
        "            η[x, a, o, x′] = 0.0\n",
        "        end\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 4\n",
        "function prune!(π::ControllerPolicy, U, prevX)\n",
        "    𝒮, 𝒜, 𝒪, X, ψ, η = π.𝒫.𝒮, π.𝒫.𝒜, π.𝒫.𝒪, π.X, π.ψ, π.η\n",
        "    newX, removeX = setdiff(X, prevX), []\n",
        "    # prune dominated from previous nodes\n",
        "    dominated(x,x′) = all(U[x,s] ≤ U[x′,s] for s in 𝒮)\n",
        "    for (x,x′) in product(prevX, newX)\n",
        "        if x′ ∉ removeX && dominated(x, x′)\n",
        "            for s in 𝒮\n",
        "                U[x,s] = U[x′,s]\n",
        "            end\n",
        "            for a in 𝒜\n",
        "                ψ[x,a] = ψ[x′,a]\n",
        "                for (o,x′′) in product(𝒪, X)\n",
        "                    η[x,a,o,x′′] = η[x′,a,o,x′′]\n",
        "                end\n",
        "            end\n",
        "            push!(removeX, x′)\n",
        "        end\n",
        "    end\n",
        "    # prune identical from previous nodes\n",
        "    identical_action(x,x′) = all(ψ[x,a] ≈ ψ[x′,a] for a in 𝒜)\n",
        "    identical_successor(x,x′) = all(η[x,a,o,x′′] ≈ η[x′,a,o,x′′]\n",
        "            for a in 𝒜, o in 𝒪, x′′ in X)\n",
        "    identical(x,x′) = identical_action(x,x′) && identical_successor(x,x′)\n",
        "    for (x,x′) in product(prevX, newX)\n",
        "        if x′ ∉ removeX && identical(x,x′)\n",
        "            push!(removeX, x′)\n",
        "        end\n",
        "    end\n",
        "    # prune dominated from new nodes\n",
        "    for (x,x′) in product(X, newX)\n",
        "        if x′ ∉ removeX && dominated(x′,x) && x ≠ x′\n",
        "            push!(removeX, x′)\n",
        "        end\n",
        "    end\n",
        "    # update controller\n",
        "    π.X = setdiff(X, removeX)\n",
        "    π.ψ = Dict(k => v for (k,v) in ψ if k[1] ∉ removeX)\n",
        "    π.η = Dict(k => v for (k,v) in η if k[1] ∉ removeX)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 5\n",
        "struct NonlinearProgramming\n",
        "    b # initial belief\n",
        "    ℓ # number of nodes\n",
        "end\n",
        "\n",
        "function tensorform(𝒫::POMDP)\n",
        "    𝒮, 𝒜, 𝒪, R, T, O = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.R, 𝒫.T, 𝒫.O\n",
        "    𝒮′ = eachindex(𝒮)\n",
        "    𝒜′ = eachindex(𝒜)\n",
        "    𝒪′ = eachindex(𝒪)\n",
        "    R′ = [R(s,a) for s in 𝒮, a in 𝒜]\n",
        "    T′ = [T(s,a,s′) for s in 𝒮, a in 𝒜, s′ in 𝒮]\n",
        "    O′ = [O(a,s′,o) for a in 𝒜, s′ in 𝒮, o in 𝒪]\n",
        "    return 𝒮′, 𝒜′, 𝒪′, R′, T′, O′\n",
        "end\n",
        "\n",
        "function solve(M::NonlinearProgramming, 𝒫::POMDP)\n",
        "    x1, X = 1, collect(1:M.ℓ)\n",
        "    𝒫, γ, b = 𝒫, 𝒫.γ, M.b\n",
        "    𝒮, 𝒜, 𝒪, R, T, O = tensorform(𝒫)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[X,𝒮])\n",
        "    @variable(model, ψ[X,𝒜] ≥ 0)\n",
        "    @variable(model, η[X,𝒜,𝒪,X] ≥ 0)\n",
        "    @objective(model, Max, b⋅U[x1,:])\n",
        "    @NLconstraint(model, [x=X,s=𝒮],\n",
        "        U[x,s] == (sum(ψ[x,a]*(R[s,a] + γ*sum(T[s,a,s′]*sum(O[a,s′,o]\n",
        "        *sum(η[x,a,o,x′]*U[x′,s′] for x′ in X)\n",
        "        for o in 𝒪) for s′ in 𝒮)) for a in 𝒜)))\n",
        "    @constraint(model, [x=X], sum(ψ[x,:]) == 1)\n",
        "    @constraint(model, [x=X,a=𝒜,o=𝒪], sum(η[x,a,o,:]) == 1)\n",
        "    optimize!(model)\n",
        "    ψ′, η′ = value.(ψ), value.(η)\n",
        "    return ControllerPolicy(𝒫, X,\n",
        "        Dict((x, 𝒫.𝒜[a]) => ψ′[x, a] for x in X, a in 𝒜),\n",
        "        Dict((x, 𝒫.𝒜[a], 𝒫.𝒪[o], x′) => η′[x, a, o, x′]\n",
        "             for x in X, a in 𝒜, o in 𝒪, x′ in X))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 6\n",
        "struct ControllerGradient\n",
        "    b       # initial belief\n",
        "    ℓ       # number of nodes\n",
        "    α       # gradient step\n",
        "    k_max   # maximum iterations\n",
        "end\n",
        "\n",
        "function solve(M::ControllerGradient, 𝒫::POMDP)\n",
        "    𝒜, 𝒪, ℓ, k_max = 𝒫.𝒜, 𝒫.𝒪, M.ℓ, M.k_max\n",
        "    X = collect(1:ℓ)\n",
        "    ψ = Dict((x, a) => rand() for x in X, a in 𝒜)\n",
        "    η = Dict((x, a, o, x′) => rand() for x in X, a in 𝒜, o in 𝒪, x′ in X)\n",
        "    π = ControllerPolicy(𝒫, X, ψ, η)\n",
        "    for i in 1:k_max\n",
        "        improve!(π, M, 𝒫)\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "\n",
        "function improve!(π::ControllerPolicy, M::ControllerGradient, 𝒫::POMDP)\n",
        "    𝒮, 𝒜, 𝒪, X, x1, ψ, η = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, π.X, 1, π.ψ, π.η\n",
        "    n, m, z, b, ℓ, α = length(𝒮), length(𝒜), length(𝒪), M.b, M.ℓ, M.α\n",
        "    ∂U′∂ψ, ∂U′∂η = gradient(π, M, 𝒫)\n",
        "    UIndex(x, s) = (s - 1) * ℓ + (x - 1) + 1\n",
        "    E(U, x1, b) = sum(b[s]*U[UIndex(x1,s)] for s in 1:n)\n",
        "    ψ′ = Dict((x, a) => 0.0 for x in X, a in 𝒜)\n",
        "    η′ = Dict((x, a, o, x′) => 0.0 for x in X, a in 𝒜, o in 𝒪, x′ in X)\n",
        "    for x in X\n",
        "        ψ′x = [ψ[x, a] + α * E(∂U′∂ψ(x, a), x1, b) for a in 𝒜]\n",
        "        ψ′x = project_to_simplex(ψ′x)\n",
        "        for (aIndex, a) in enumerate(𝒜)\n",
        "            ψ′[x, a] = ψ′x[aIndex]\n",
        "        end\n",
        "        for (a, o) in product(𝒜, 𝒪)\n",
        "            η′x = [(η[x, a, o, x′] +\n",
        "                    α * E(∂U′∂η(x, a, o, x′), x1, b)) for x′ in X]\n",
        "            η′x = project_to_simplex(η′x)\n",
        "            for (x′Index, x′) in enumerate(X)\n",
        "                η′[x, a, o, x′] = η′x[x′Index]\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    π.ψ, π.η = ψ′, η′\n",
        "end\n",
        "\n",
        "function project_to_simplex(y)\n",
        "    u = sort(copy(y), rev=true)\n",
        "    i = maximum([j for j in eachindex(u)\n",
        "                 if u[j] + (1 - sum(u[1:j])) / j > 0.0])\n",
        "    δ = (1 - sum(u[j] for j = 1:i)) / i\n",
        "    return [max(y[j] + δ, 0.0) for j in eachindex(u)]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### controller-abstractions 7\n",
        "function gradient(π::ControllerPolicy, M::ControllerGradient, 𝒫::POMDP)\n",
        "    𝒮, 𝒜, 𝒪, T, O, R, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    X, x1, ψ, η = π.X, 1, π.ψ, π.η\n",
        "    n, m, z = length(𝒮), length(𝒜), length(𝒪)\n",
        "    X𝒮 = vec(collect(product(X, 𝒮)))\n",
        "    T′ = [sum(ψ[x, a] * T(s, a, s′) * sum(O(a, s′, o) * η[x, a, o, x′]\n",
        "          for o in 𝒪) for a in 𝒜) for (x, s) in X𝒮, (x′, s′) in X𝒮]\n",
        "    R′ = [sum(ψ[x, a] * R(s, a) for a in 𝒜) for (x, s) in X𝒮]\n",
        "    Z = 1.0I(length(X𝒮)) - γ * T′\n",
        "    invZ = inv(Z)\n",
        "    ∂Z∂ψ(hx, ha) = [x == hx ? (-γ * T(s, ha, s′)\n",
        "                    * sum(O(ha, s′, o) * η[hx, ha, o, x′]\n",
        "                          for o in 𝒪)) : 0.0\n",
        "                    for (x, s) in X𝒮, (x′, s′) in X𝒮]\n",
        "    ∂Z∂η(hx, ha, ho, hx′) = [x == hx && x′ == hx′ ? (-γ * ψ[hx, ha]\n",
        "                    * T(s, ha, s′) * O(ha, s′, ho)) : 0.0\n",
        "                 for (x, s) in X𝒮, (x′, s′) in X𝒮]\n",
        "    ∂R′∂ψ(hx, ha) = [x == hx ? R(s, ha) : 0.0 for (x, s) in X𝒮]\n",
        "    ∂R′∂η(hx, ha, ho, hx′) = [0.0 for (x, s) in X𝒮]\n",
        "    ∂U′∂ψ(hx, ha) = invZ * (∂R′∂ψ(hx, ha) - ∂Z∂ψ(hx, ha) * invZ * R′)\n",
        "    ∂U′∂η(hx, ha, ho, hx′) = invZ * (∂R′∂η(hx, ha, ho, hx′)\n",
        "                                - ∂Z∂η(hx, ha, ho, hx′) * invZ * R′)\n",
        "    return ∂U′∂ψ, ∂U′∂η\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 1\n",
        "struct SimpleGame\n",
        "    γ  # discount factor\n",
        "    ℐ  # agents\n",
        "    𝒜  # joint action space\n",
        "    R  # joint reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 2\n",
        "struct SimpleGamePolicy\n",
        "    p # dictionary mapping actions to probabilities\n",
        "\n",
        "    function SimpleGamePolicy(p::Base.Generator)\n",
        "        return SimpleGamePolicy(Dict(p))\n",
        "    end\n",
        "\n",
        "    function SimpleGamePolicy(p::Dict)\n",
        "        vs = collect(values(p))\n",
        "        vs ./= sum(vs)\n",
        "        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))\n",
        "    end\n",
        "\n",
        "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
        "end\n",
        "\n",
        "(πi::SimpleGamePolicy)(ai) = get(πi.p, ai, 0.0)\n",
        "\n",
        "function (πi::SimpleGamePolicy)()\n",
        "    D = SetCategorical(collect(keys(πi.p)), collect(values(πi.p)))\n",
        "    return rand(D)\n",
        "end\n",
        "\n",
        "joint(X) = vec(collect(product(X...)))\n",
        "\n",
        "joint(π, πi, i) = [i == j ? πi : πj for (j, πj) in enumerate(π)]\n",
        "\n",
        "function utility(𝒫::SimpleGame, π, i)\n",
        "    𝒜, R = 𝒫.𝒜, 𝒫.R\n",
        "    p(a) = prod(πj(aj) for (πj, aj) in zip(π, a))\n",
        "    return sum(R(a)[i]*p(a) for a in joint(𝒜))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 3\n",
        "function best_response(𝒫::SimpleGame, π, i)\n",
        "    U(ai) = utility(𝒫, joint(π, SimpleGamePolicy(ai), i), i)\n",
        "    ai = argmax(U, 𝒫.𝒜[i])\n",
        "    return SimpleGamePolicy(ai)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 4\n",
        "function softmax_response(𝒫::SimpleGame, π, i, λ)\n",
        "    𝒜i = 𝒫.𝒜[i]\n",
        "    U(ai) = utility(𝒫, joint(π, SimpleGamePolicy(ai), i), i)\n",
        "    return SimpleGamePolicy(ai => exp(λ*U(ai)) for ai in 𝒜i)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 5\n",
        "struct NashEquilibrium end\n",
        "\n",
        "function tensorform(𝒫::SimpleGame)\n",
        "    ℐ, 𝒜, R = 𝒫.ℐ, 𝒫.𝒜, 𝒫.R\n",
        "    ℐ′ = eachindex(ℐ)\n",
        "    𝒜′ = [eachindex(𝒜[i]) for i in ℐ]\n",
        "    R′ = [R(a) for a in joint(𝒜)]\n",
        "    return ℐ′, 𝒜′, R′\n",
        "end\n",
        "\n",
        "function solve(M::NashEquilibrium, 𝒫::SimpleGame)\n",
        "    ℐ, 𝒜, R = tensorform(𝒫)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[ℐ])\n",
        "    @variable(model, π[i=ℐ, 𝒜[i]] ≥ 0)\n",
        "    @NLobjective(model, Min,\n",
        "        sum(U[i] - sum(prod(π[j,a[j]] for j in ℐ) * R[y][i]\n",
        "            for (y,a) in enumerate(joint(𝒜))) for i in ℐ))\n",
        "    @NLconstraint(model, [i=ℐ, ai=𝒜[i]],\n",
        "        U[i] ≥ sum(\n",
        "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : π[j,a[j]] for j in ℐ)\n",
        "            * R[y][i] for (y,a) in enumerate(joint(𝒜))))\n",
        "    @constraint(model, [i=ℐ], sum(π[i,ai] for ai in 𝒜[i]) == 1)\n",
        "    optimize!(model)\n",
        "    πi′(i) = SimpleGamePolicy(𝒫.𝒜[i][ai] => value(π[i,ai]) for ai in 𝒜[i])\n",
        "    return [πi′(i) for i in ℐ]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 6\n",
        "mutable struct JointCorrelatedPolicy\n",
        "    p # dictionary mapping from joint actions to probabilities\n",
        "    JointCorrelatedPolicy(p::Base.Generator) = new(Dict(p))\n",
        "end\n",
        "\n",
        "(π::JointCorrelatedPolicy)(a) = get(π.p, a, 0.0)\n",
        "\n",
        "function (π::JointCorrelatedPolicy)()\n",
        "    D = SetCategorical(collect(keys(π.p)), collect(values(π.p)))\n",
        "    return rand(D)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 7\n",
        "struct CorrelatedEquilibrium end\n",
        "\n",
        "function solve(M::CorrelatedEquilibrium, 𝒫::SimpleGame)\n",
        "    ℐ, 𝒜, R = 𝒫.ℐ, 𝒫.𝒜, 𝒫.R\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, π[joint(𝒜)] ≥ 0)\n",
        "    @objective(model, Max, sum(sum(π[a]*R(a) for a in joint(𝒜))))\n",
        "    @constraint(model, [i=ℐ, ai=𝒜[i], ai′=𝒜[i]],\n",
        "        sum(R(a)[i]*π[a] for a in joint(𝒜) if a[i]==ai)\n",
        "        ≥ sum(R(joint(a,ai′,i))[i]*π[a] for a in joint(𝒜) if a[i]==ai))\n",
        "    @constraint(model, sum(π) == 1)\n",
        "    optimize!(model)\n",
        "    return JointCorrelatedPolicy(a => value(π[a]) for a in joint(𝒜))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 8\n",
        "struct IteratedBestResponse\n",
        "    k_max # number of iterations\n",
        "    π     # initial policy\n",
        "end\n",
        "\n",
        "function IteratedBestResponse(𝒫::SimpleGame, k_max)\n",
        "    π = [SimpleGamePolicy(ai => 1.0 for ai in 𝒜i) for 𝒜i in 𝒫.𝒜]\n",
        "    return IteratedBestResponse(k_max, π)\n",
        "end\n",
        "\n",
        "function solve(M::IteratedBestResponse, 𝒫)\n",
        "    π = M.π\n",
        "    for k in 1:M.k_max\n",
        "        π = [best_response(𝒫, π, i) for i in 𝒫.ℐ]\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 9\n",
        "struct HierarchicalSoftmax\n",
        "    λ # precision parameter\n",
        "    k # level\n",
        "    π # initial policy\n",
        "end\n",
        "\n",
        "function HierarchicalSoftmax(𝒫::SimpleGame, λ, k)\n",
        "    π = [SimpleGamePolicy(ai => 1.0 for ai in 𝒜i) for 𝒜i in 𝒫.𝒜]\n",
        "    return HierarchicalSoftmax(λ, k, π)\n",
        "end\n",
        "\n",
        "function solve(M::HierarchicalSoftmax, 𝒫)\n",
        "    π = M.π\n",
        "    for k in 1:M.k\n",
        "        π = [softmax_response(𝒫, π, i, M.λ) for i in 𝒫.ℐ]\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 10\n",
        "function simulate(𝒫::SimpleGame, π, k_max)\n",
        "    for k = 1:k_max\n",
        "        a = [πi() for πi in π]\n",
        "        for πi in π\n",
        "            update!(πi, a)\n",
        "        end\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 11\n",
        "mutable struct FictitiousPlay\n",
        "    𝒫  # simple game\n",
        "    i  # agent index\n",
        "    N  # array of action count dictionaries\n",
        "    πi # current policy\n",
        "end\n",
        "\n",
        "function FictitiousPlay(𝒫::SimpleGame, i)\n",
        "    N = [Dict(aj => 1 for aj in 𝒫.𝒜[j]) for j in 𝒫.ℐ]\n",
        "    πi = SimpleGamePolicy(ai => 1.0 for ai in 𝒫.𝒜[i])\n",
        "    return FictitiousPlay(𝒫, i, N, πi)\n",
        "end\n",
        "\n",
        "(πi::FictitiousPlay)() = πi.πi()\n",
        "\n",
        "(πi::FictitiousPlay)(ai) = πi.πi(ai)\n",
        "\n",
        "function update!(πi::FictitiousPlay, a)\n",
        "    N, 𝒫, ℐ, i = πi.N, πi.𝒫, πi.𝒫.ℐ, πi.i\n",
        "    for (j, aj) in enumerate(a)\n",
        "        N[j][aj] += 1\n",
        "    end\n",
        "    p(j) = SimpleGamePolicy(aj => u/sum(values(N[j])) for (aj, u) in N[j])\n",
        "    π = [p(j) for j in ℐ]\n",
        "    πi.πi = best_response(𝒫, π, i)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### multiagent_reasoning 12\n",
        "mutable struct GradientAscent\n",
        "    𝒫  # simple game\n",
        "    i  # agent index\n",
        "    t  # time step\n",
        "    πi # current policy\n",
        "end\n",
        "\n",
        "function GradientAscent(𝒫::SimpleGame, i)\n",
        "    uniform() = SimpleGamePolicy(ai => 1.0 for ai in 𝒫.𝒜[i])\n",
        "    return GradientAscent(𝒫, i, 1, uniform())\n",
        "end\n",
        "\n",
        "(πi::GradientAscent)() = πi.πi()\n",
        "\n",
        "(πi::GradientAscent)(ai) = πi.πi(ai)\n",
        "\n",
        "function update!(πi::GradientAscent, a)\n",
        "    𝒫, ℐ, 𝒜i, i, t = πi.𝒫, πi.𝒫.ℐ, πi.𝒫.𝒜[πi.i], πi.i, πi.t\n",
        "    jointπ(ai) = [SimpleGamePolicy(j == i ? ai : a[j]) for j in ℐ]\n",
        "    r = [utility(𝒫, jointπ(ai), i) for ai in 𝒜i]\n",
        "    π′ = [πi.πi(ai) for ai in 𝒜i]\n",
        "    π = project_to_simplex(π′ + r / sqrt(t))\n",
        "    πi.t = t + 1\n",
        "    πi.πi = SimpleGamePolicy(ai => p for (ai, p) in zip(𝒜i, π))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 1\n",
        "struct MG\n",
        "    γ  # discount factor\n",
        "    ℐ  # agents\n",
        "    𝒮  # state space\n",
        "    𝒜  # joint action space\n",
        "    T  # transition function\n",
        "    R  # joint reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 2\n",
        "struct MGPolicy\n",
        "    p # dictionary mapping states to simple game policies\n",
        "    MGPolicy(p::Base.Generator) = new(Dict(p))\n",
        "end\n",
        "\n",
        "(πi::MGPolicy)(s, ai) = πi.p[s](ai)\n",
        "(πi::SimpleGamePolicy)(s, ai) = πi(ai)\n",
        "\n",
        "probability(𝒫::MG, s, π, a) = prod(πj(s, aj) for (πj, aj) in zip(π, a))\n",
        "reward(𝒫::MG, s, π, i) =\n",
        "    sum(𝒫.R(s,a)[i]*probability(𝒫,s,π,a) for a in joint(𝒫.𝒜))\n",
        "transition(𝒫::MG, s, π, s′) =\n",
        "    sum(𝒫.T(s,a,s′)*probability(𝒫,s,π,a) for a in joint(𝒫.𝒜))\n",
        "\n",
        "function policy_evaluation(𝒫::MG, π, i)\n",
        "    𝒮, 𝒜, R, T, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.T, 𝒫.γ\n",
        "    p(s,a) = prod(πj(s, aj) for (πj, aj) in zip(π, a))\n",
        "    R′ = [sum(R(s,a)[i]*p(s,a) for a in joint(𝒜)) for s in 𝒮]\n",
        "    T′ = [sum(T(s,a,s′)*p(s,a) for a in joint(𝒜)) for s in 𝒮, s′ in 𝒮]\n",
        "    return (I - γ*T′)\\R′\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 3\n",
        "function best_response(𝒫::MG, π, i)\n",
        "    𝒮, 𝒜, R, T, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.T, 𝒫.γ\n",
        "    T′(s,ai,s′) = transition(𝒫, s, joint(π, SimpleGamePolicy(ai), i), s′)\n",
        "    R′(s,ai) = reward(𝒫, s, joint(π, SimpleGamePolicy(ai), i), i)\n",
        "    πi = solve(MDP(γ, 𝒮, 𝒜[i], T′, R′))\n",
        "    return MGPolicy(s => SimpleGamePolicy(πi(s)) for s in 𝒮)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 4\n",
        "function softmax_response(𝒫::MG, π, i, λ)\n",
        "    𝒮, 𝒜, R, T, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.T, 𝒫.γ\n",
        "    T′(s,ai,s′) = transition(𝒫, s, joint(π, SimpleGamePolicy(ai), i), s′)\n",
        "    R′(s,ai) = reward(𝒫, s, joint(π, SimpleGamePolicy(ai), i), i)\n",
        "    mdp = MDP(γ, 𝒮, joint(𝒜), T′, R′)\n",
        "    πi = solve(mdp)\n",
        "    Q(s,a) = lookahead(mdp, πi.U, s, a)\n",
        "    p(s) = SimpleGamePolicy(a => exp(λ*Q(s,a)) for a in 𝒜[i])\n",
        "    return MGPolicy(s => p(s) for s in 𝒮)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 5\n",
        "function tensorform(𝒫::MG)\n",
        "    ℐ, 𝒮, 𝒜, R, T = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.T\n",
        "    ℐ′ = eachindex(ℐ)\n",
        "    𝒮′ = eachindex(𝒮)\n",
        "    𝒜′ = [eachindex(𝒜[i]) for i in ℐ]\n",
        "    R′ = [R(s,a) for s in 𝒮, a in joint(𝒜)]\n",
        "    T′ = [T(s,a,s′) for s in 𝒮, a in joint(𝒜), s′ in 𝒮]\n",
        "    return ℐ′, 𝒮′, 𝒜′, R′, T′\n",
        "end\n",
        "\n",
        "function solve(M::NashEquilibrium, 𝒫::MG)\n",
        "    ℐ, 𝒮, 𝒜, R, T = tensorform(𝒫)\n",
        "    𝒮′, 𝒜′, γ = 𝒫.𝒮, 𝒫.𝒜, 𝒫.γ\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[ℐ, 𝒮])\n",
        "    @variable(model, π[i=ℐ, 𝒮, ai=𝒜[i]] ≥ 0)\n",
        "    @NLobjective(model, Min,\n",
        "        sum(U[i,s] - sum(prod(π[j,s,a[j]] for j in ℐ)\n",
        "            * (R[s,y][i] + γ*sum(T[s,y,s′]*U[i,s′] for s′ in 𝒮))\n",
        "            for (y,a) in enumerate(joint(𝒜))) for i in ℐ, s in 𝒮))\n",
        "    @NLconstraint(model, [i=ℐ, s=𝒮, ai=𝒜[i]],\n",
        "        U[i,s] ≥ sum(\n",
        "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : π[j,s,a[j]] for j in ℐ)\n",
        "            * (R[s,y][i] + γ*sum(T[s,y,s′]*U[i,s′] for s′ in 𝒮))\n",
        "            for (y,a) in enumerate(joint(𝒜))))\n",
        "    @constraint(model, [i=ℐ, s=𝒮], sum(π[i,s,ai] for ai in 𝒜[i]) == 1)\n",
        "    optimize!(model)\n",
        "    π′ = value.(π)\n",
        "    πi′(i,s) = SimpleGamePolicy(𝒜′[i][ai] => π′[i,s,ai] for ai in 𝒜[i])\n",
        "    πi′(i) = MGPolicy(𝒮′[s] => πi′(i,s) for s in 𝒮)\n",
        "    return [πi′(i) for i in ℐ]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 6\n",
        "function randstep(𝒫::MG, s, a)\n",
        "    s′ = rand(SetCategorical(𝒫.𝒮, [𝒫.T(s, a, s′) for s′ in 𝒫.𝒮]))\n",
        "    r = 𝒫.R(s,a)\n",
        "    return s′, r\n",
        "end\n",
        "\n",
        "function simulate(𝒫::MG, π, k_max, b)\n",
        "    s = rand(b)\n",
        "    for k = 1:k_max\n",
        "        a = Tuple(πi(s)() for πi in π)\n",
        "        s′, r = randstep(𝒫, s, a)\n",
        "        for πi in π\n",
        "            update!(πi, s, a, s′)\n",
        "        end\n",
        "        s = s′\n",
        "    end\n",
        "    return π\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 7\n",
        "mutable struct MGFictitiousPlay\n",
        "    𝒫  # Markov game\n",
        "    i  # agent index\n",
        "    Qi # state-action value estimates\n",
        "    Ni # state-action counts\n",
        "end\n",
        "\n",
        "function MGFictitiousPlay(𝒫::MG, i)\n",
        "    ℐ, 𝒮, 𝒜, R = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.R\n",
        "    Qi = Dict((s, a) => R(s, a)[i] for s in 𝒮 for a in joint(𝒜))\n",
        "    Ni = Dict((j, s, aj) => 1.0 for j in ℐ for s in 𝒮 for aj in 𝒜[j])\n",
        "    return MGFictitiousPlay(𝒫, i, Qi, Ni)\n",
        "end\n",
        "\n",
        "function (πi::MGFictitiousPlay)(s)\n",
        "    𝒫, i, Qi = πi.𝒫, πi.i, πi.Qi\n",
        "    ℐ, 𝒮, 𝒜, T, R, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.T, 𝒫.R, 𝒫.γ\n",
        "    πi′(i,s) = SimpleGamePolicy(ai => πi.Ni[i,s,ai] for ai in 𝒜[i])\n",
        "    πi′(i) = MGPolicy(s => πi′(i,s) for s in 𝒮)\n",
        "    π = [πi′(i) for i in ℐ]\n",
        "    U(s,π) = sum(πi.Qi[s,a]*probability(𝒫,s,π,a) for a in joint(𝒜))\n",
        "    Q(s,π) = reward(𝒫,s,π,i) + γ*sum(transition(𝒫,s,π,s′)*U(s′,π)\n",
        "                                     for s′ in 𝒮)\n",
        "    Q(ai) = Q(s, joint(π, SimpleGamePolicy(ai), i))\n",
        "    ai = argmax(Q, 𝒫.𝒜[πi.i])\n",
        "    return SimpleGamePolicy(ai)\n",
        "end\n",
        "\n",
        "function update!(πi::MGFictitiousPlay, s, a, s′)\n",
        "    𝒫, i, Qi = πi.𝒫, πi.i, πi.Qi\n",
        "    ℐ, 𝒮, 𝒜, T, R, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.T, 𝒫.R, 𝒫.γ\n",
        "    for (j,aj) in enumerate(a)\n",
        "        πi.Ni[j,s,aj] += 1\n",
        "    end\n",
        "    πi′(i,s) = SimpleGamePolicy(ai => πi.Ni[i,s,ai] for ai in 𝒜[i])\n",
        "    πi′(i) = MGPolicy(s => πi′(i,s) for s in 𝒮)\n",
        "    π = [πi′(i) for i in ℐ]\n",
        "    U(π,s) = sum(πi.Qi[s,a]*probability(𝒫,s,π,a) for a in joint(𝒜))\n",
        "    Q(s,a) = R(s,a)[i] + γ*sum(T(s,a,s′)*U(π,s′) for s′ in 𝒮)\n",
        "    for a in joint(𝒜)\n",
        "        πi.Qi[s,a] = Q(s,a)\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 8\n",
        "mutable struct MGGradientAscent\n",
        "    𝒫  # Markov game\n",
        "    i  # agent index\n",
        "    t  # time step\n",
        "    Qi # state-action value estimates\n",
        "    πi # current policy\n",
        "end\n",
        "\n",
        "function MGGradientAscent(𝒫::MG, i)\n",
        "    ℐ, 𝒮, 𝒜 = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜\n",
        "    Qi = Dict((s, a) => 0.0 for s in 𝒮, a in joint(𝒜))\n",
        "    uniform() = Dict(s => SimpleGamePolicy(ai => 1.0 for ai in 𝒫.𝒜[i])\n",
        "                     for s in 𝒮)\n",
        "    return MGGradientAscent(𝒫, i, 1, Qi, uniform())\n",
        "end\n",
        "\n",
        "function (πi::MGGradientAscent)(s)\n",
        "    𝒜i, t = πi.𝒫.𝒜[πi.i], πi.t\n",
        "    ϵ = 1 / sqrt(t)\n",
        "    πi′(ai) = ϵ/length(𝒜i) + (1-ϵ)*πi.πi[s](ai)\n",
        "    return SimpleGamePolicy(ai => πi′(ai) for ai in 𝒜i)\n",
        "end\n",
        "\n",
        "function update!(πi::MGGradientAscent, s, a, s′)\n",
        "    𝒫, i, t, Qi = πi.𝒫, πi.i, πi.t, πi.Qi\n",
        "    ℐ, 𝒮, 𝒜i, R, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜[πi.i], 𝒫.R, 𝒫.γ\n",
        "    jointπ(ai) = Tuple(j == i ? ai : a[j] for j in ℐ)\n",
        "    α = 1 / sqrt(t)\n",
        "    Qmax = maximum(Qi[s′, jointπ(ai)] for ai in 𝒜i)\n",
        "    πi.Qi[s, a] += α * (R(s, a)[i] + γ * Qmax - Qi[s, a])\n",
        "    u = [Qi[s, jointπ(ai)] for ai in 𝒜i]\n",
        "    π′ = [πi.πi[s](ai) for ai in 𝒜i]\n",
        "    π = project_to_simplex(π′ + u / sqrt(t))\n",
        "    πi.t = t + 1\n",
        "    πi.πi[s] = SimpleGamePolicy(ai => p for (ai, p) in zip(𝒜i, π))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### sequential_problems 9\n",
        "mutable struct NashQLearning\n",
        "    𝒫 # Markov game\n",
        "    i # agent index\n",
        "    Q # state-action value estimates\n",
        "    N # history of actions performed\n",
        "end\n",
        "\n",
        "function NashQLearning(𝒫::MG, i)\n",
        "    ℐ, 𝒮, 𝒜 = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜\n",
        "    Q = Dict((j, s, a) => 0.0 for j in ℐ, s in 𝒮, a in joint(𝒜))\n",
        "    N = Dict((s, a) => 1.0 for s in 𝒮, a in joint(𝒜))\n",
        "    return NashQLearning(𝒫, i, Q, N)\n",
        "end\n",
        "\n",
        "function (πi::NashQLearning)(s)\n",
        "    𝒫, i, Q, N = πi.𝒫, πi.i, πi.Q, πi.N\n",
        "    ℐ, 𝒮, 𝒜, 𝒜i, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒜[πi.i], 𝒫.γ\n",
        "    M = NashEquilibrium()\n",
        "    𝒢 = SimpleGame(γ, ℐ, 𝒜, a -> [Q[j, s, a] for j in ℐ])\n",
        "    π = solve(M, 𝒢)\n",
        "    ϵ = 1 / sum(N[s, a] for a in joint(𝒜))\n",
        "    πi′(ai) = ϵ/length(𝒜i) + (1-ϵ)*π[i](ai)\n",
        "    return SimpleGamePolicy(ai => πi′(ai) for ai in 𝒜i)\n",
        "end\n",
        "\n",
        "function update!(πi::NashQLearning, s, a, s′)\n",
        "    𝒫, ℐ, 𝒮, 𝒜, R, γ = πi.𝒫, πi.𝒫.ℐ, πi.𝒫.𝒮, πi.𝒫.𝒜, πi.𝒫.R, πi.𝒫.γ\n",
        "    i, Q, N = πi.i, πi.Q, πi.N\n",
        "    M = NashEquilibrium()\n",
        "    𝒢 = SimpleGame(γ, ℐ, 𝒜, a′ -> [Q[j, s′, a′] for j in ℐ])\n",
        "    π = solve(M, 𝒢)\n",
        "    πi.N[s, a] += 1\n",
        "    α = 1 / sqrt(N[s, a])\n",
        "    for j in ℐ\n",
        "        πi.Q[j,s,a] += α*(R(s,a)[j] + γ*utility(𝒢,π,j) - Q[j,s,a])\n",
        "    end\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 1\n",
        "struct POMG\n",
        "    γ  # discount factor\n",
        "    ℐ  # agents\n",
        "    𝒮  # state space\n",
        "    𝒜  # joint action space\n",
        "    𝒪  # joint observation space\n",
        "    T  # transition function\n",
        "    O  # joint observation function\n",
        "    R  # joint reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 2\n",
        "function lookahead(𝒫::POMG, U, s, a)\n",
        "    𝒮, 𝒪, T, O, R, γ = 𝒫.𝒮, joint(𝒫.𝒪), 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    u′ = sum(T(s,a,s′)*sum(O(a,s′,o)*U(o,s′) for o in 𝒪) for s′ in 𝒮)\n",
        "    return R(s,a) + γ*u′\n",
        "end\n",
        "\n",
        "function evaluate_plan(𝒫::POMG, π, s)\n",
        "    a = Tuple(πi() for πi in π)\n",
        "    U(o,s′) = evaluate_plan(𝒫, [πi(oi) for (πi, oi) in zip(π,o)], s′)\n",
        "    return isempty(first(π).subplans) ? 𝒫.R(s,a) : lookahead(𝒫, U, s, a)\n",
        "end\n",
        "\n",
        "function utility(𝒫::POMG, b, π)\n",
        "    u = [evaluate_plan(𝒫, π, s) for s in 𝒫.𝒮]\n",
        "    return sum(bs * us for (bs, us) in zip(b, u))\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 3\n",
        "struct POMGNashEquilibrium\n",
        "    b # initial belief\n",
        "    d # depth of conditional plans\n",
        "end\n",
        "\n",
        "function create_conditional_plans(𝒫, d)\n",
        "    ℐ, 𝒜, 𝒪 = 𝒫.ℐ, 𝒫.𝒜, 𝒫.𝒪\n",
        "    Π = [[ConditionalPlan(ai) for ai in 𝒜[i]] for i in ℐ]\n",
        "    for t in 1:d\n",
        "        Π = expand_conditional_plans(𝒫, Π)\n",
        "    end\n",
        "    return Π\n",
        "end\n",
        "\n",
        "function expand_conditional_plans(𝒫, Π)\n",
        "    ℐ, 𝒜, 𝒪 = 𝒫.ℐ, 𝒫.𝒜, 𝒫.𝒪\n",
        "    return [[ConditionalPlan(ai, Dict(oi => πi for oi in 𝒪[i]))\n",
        "        for πi in Π[i] for ai in 𝒜[i]] for i in ℐ]\n",
        "end\n",
        "\n",
        "function solve(M::POMGNashEquilibrium, 𝒫::POMG)\n",
        "    ℐ, γ, b, d = 𝒫.ℐ, 𝒫.γ, M.b, M.d\n",
        "    Π = create_conditional_plans(𝒫, d)\n",
        "    U = Dict(π => utility(𝒫, b, π) for π in joint(Π))\n",
        "    𝒢 = SimpleGame(γ, ℐ, Π, π -> U[π])\n",
        "    π = solve(NashEquilibrium(), 𝒢)\n",
        "    return Tuple(argmax(πi.p) for πi in π)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### state_uncertainty 4\n",
        "struct POMGDynamicProgramming\n",
        "    b   # initial belief\n",
        "    d   # depth of conditional plans\n",
        "end\n",
        "\n",
        "function solve(M::POMGDynamicProgramming, 𝒫::POMG)\n",
        "    ℐ, 𝒮, 𝒜, R, γ, b, d = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.γ, M.b, M.d\n",
        "    Π = [[ConditionalPlan(ai) for ai in 𝒜[i]] for i in ℐ]\n",
        "    for t in 1:d\n",
        "        Π = expand_conditional_plans(𝒫, Π)\n",
        "        prune_dominated!(Π, 𝒫)\n",
        "    end\n",
        "    𝒢 = SimpleGame(γ, ℐ, Π, π -> utility(𝒫, b, π))\n",
        "    π = solve(NashEquilibrium(), 𝒢)\n",
        "    return Tuple(argmax(πi.p) for πi in π)\n",
        "end\n",
        "\n",
        "function prune_dominated!(Π, 𝒫::POMG)\n",
        "    done = false\n",
        "    while !done\n",
        "        done = true\n",
        "        for i in shuffle(𝒫.ℐ)\n",
        "            for πi in shuffle(Π[i])\n",
        "                if length(Π[i]) > 1 && is_dominated(𝒫, Π, i, πi)\n",
        "                    filter!(πi′ -> πi′ ≠ πi, Π[i])\n",
        "                    done = false\n",
        "                    break\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "end\n",
        "\n",
        "function is_dominated(𝒫::POMG, Π, i, πi)\n",
        "    ℐ, 𝒮 = 𝒫.ℐ, 𝒫.𝒮\n",
        "    jointΠnoti = joint([Π[j] for j in ℐ if j ≠ i])\n",
        "    π(πi′, πnoti) = [j==i ? πi′ : πnoti[j>i ? j-1 : j] for j in ℐ]\n",
        "    Ui = Dict((πi′, πnoti, s) => evaluate_plan(𝒫, π(πi′, πnoti), s)[i]\n",
        "              for πi′ in Π[i], πnoti in jointΠnoti, s in 𝒮)\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, δ)\n",
        "    @variable(model, b[jointΠnoti, 𝒮] ≥ 0)\n",
        "    @objective(model, Max, δ)\n",
        "    @constraint(model, [πi′=Π[i]],\n",
        "        sum(b[πnoti, s] * (Ui[πi′, πnoti, s] - Ui[πi, πnoti, s])\n",
        "        for πnoti in jointΠnoti for s in 𝒮) ≥ δ)\n",
        "    @constraint(model, sum(b) == 1)\n",
        "    optimize!(model)\n",
        "    return value(δ) ≥ 0\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 1\n",
        "struct DecPOMDP\n",
        "    γ  # discount factor\n",
        "    ℐ  # agents\n",
        "    𝒮  # state space\n",
        "    𝒜  # joint action space\n",
        "    𝒪  # joint observation space\n",
        "    T  # transition function\n",
        "    O  # joint observation function\n",
        "    R  # reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 2\n",
        "struct DecPOMDPDynamicProgramming\n",
        "    b   # initial belief\n",
        "    d   # depth of conditional plans\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPDynamicProgramming, 𝒫::DecPOMDP)\n",
        "    ℐ, 𝒮, 𝒜, 𝒪, T, O, R, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    R′(s, a) = [R(s, a) for i in ℐ]\n",
        "    𝒫′ = POMG(γ, ℐ, 𝒮, 𝒜, 𝒪, T, O, R′)\n",
        "    M′ = POMGDynamicProgramming(M.b, M.d)\n",
        "    return solve(M′, 𝒫′)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 3\n",
        "struct DecPOMDPIteratedBestResponse\n",
        "    b     # initial belief\n",
        "    d     # depth of conditional plans\n",
        "    k_max # number of iterations\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPIteratedBestResponse, 𝒫::DecPOMDP)\n",
        "    ℐ, 𝒮, 𝒜, 𝒪, T, O, R, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    b, d, k_max = M.b, M.d, M.k_max\n",
        "    R′(s, a) = [R(s, a) for i in ℐ]\n",
        "    𝒫′ = POMG(γ, ℐ, 𝒮, 𝒜, 𝒪, T, O, R′)\n",
        "    Π = create_conditional_plans(𝒫, d)\n",
        "    π = [rand(Π[i]) for i in ℐ]\n",
        "    for k in 1:k_max\n",
        "        for i in shuffle(ℐ)\n",
        "            π′(πi) = Tuple(j == i ? πi : π[j] for j in ℐ)\n",
        "            Ui(πi) = utility(𝒫′, b, π′(πi))[i]\n",
        "            π[i] = argmax(Ui, Π[i])\n",
        "        end\n",
        "    end\n",
        "    return Tuple(π)\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 4\n",
        "struct DecPOMDPHeuristicSearch\n",
        "    b     # initial belief\n",
        "    d     # depth of conditional plans\n",
        "    π_max # number of policies\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPHeuristicSearch, 𝒫::DecPOMDP)\n",
        "    ℐ, 𝒮, 𝒜, 𝒪, T, O, R, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    b, d, π_max = M.b, M.d, M.π_max\n",
        "    R′(s, a) = [R(s, a) for i in ℐ]\n",
        "    𝒫′ = POMG(γ, ℐ, 𝒮, 𝒜, 𝒪, T, O, R′)\n",
        "    Π = [[ConditionalPlan(ai) for ai in 𝒜[i]] for i in ℐ]\n",
        "    for t in 1:d\n",
        "        allΠ = expand_conditional_plans(𝒫, Π)\n",
        "        Π = [[] for i in ℐ]\n",
        "        for z in 1:π_max\n",
        "            b′ = explore(M, 𝒫, t)\n",
        "            π = argmax(π -> first(utility(𝒫′, b′, π)), joint(allΠ))\n",
        "            for i in ℐ\n",
        "                push!(Π[i], π[i])\n",
        "                filter!(πi -> πi != π[i], allΠ[i])\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    return argmax(π -> first(utility(𝒫′, b, π)), joint(Π))\n",
        "end\n",
        "\n",
        "function explore(M::DecPOMDPHeuristicSearch, 𝒫::DecPOMDP, t)\n",
        "    ℐ, 𝒮, 𝒜, 𝒪, T, O, R, γ = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
        "    b = copy(M.b)\n",
        "    b′ = similar(b)\n",
        "    s = rand(SetCategorical(𝒮, b))\n",
        "    for τ in 1:t\n",
        "        a = Tuple(rand(𝒜i) for 𝒜i in 𝒜)\n",
        "        s′ = rand(SetCategorical(𝒮, [T(s,a,s′) for s′ in 𝒮]))\n",
        "        o = rand(SetCategorical(joint(𝒪), [O(a,s′,o) for o in joint(𝒪)]))\n",
        "        for (i′, s′) in enumerate(𝒮)\n",
        "            po = O(a, s′, o)\n",
        "            b′[i′] = po*sum(T(s,a,s′)*b[i] for (i,s) in enumerate(𝒮))\n",
        "        end\n",
        "        normalize!(b′, 1)\n",
        "        b, s = b′, s′\n",
        "    end\n",
        "    return b′\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### collaborative_agents 5\n",
        "struct DecPOMDPNonlinearProgramming\n",
        "    b # initial belief\n",
        "    ℓ # number of nodes for each agent\n",
        "end\n",
        "\n",
        "function tensorform(𝒫::DecPOMDP)\n",
        "    ℐ, 𝒮, 𝒜, 𝒪, R, T, O = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.𝒪, 𝒫.R, 𝒫.T, 𝒫.O\n",
        "    ℐ′ = eachindex(ℐ)\n",
        "    𝒮′ = eachindex(𝒮)\n",
        "    𝒜′ = [eachindex(𝒜i) for 𝒜i in 𝒜]\n",
        "    𝒪′ = [eachindex(𝒪i) for 𝒪i in 𝒪]\n",
        "    R′ = [R(s,a) for s in 𝒮, a in joint(𝒜)]\n",
        "    T′ = [T(s,a,s′) for s in 𝒮, a in joint(𝒜), s′ in 𝒮]\n",
        "    O′ = [O(a,s′,o) for a in joint(𝒜), s′ in 𝒮, o in joint(𝒪)]\n",
        "    return ℐ′, 𝒮′, 𝒜′, 𝒪′, R′, T′, O′\n",
        "end\n",
        "\n",
        "function solve(M::DecPOMDPNonlinearProgramming, 𝒫::DecPOMDP)\n",
        "    𝒫, γ, b = 𝒫, 𝒫.γ, M.b\n",
        "    ℐ, 𝒮, 𝒜, 𝒪, R, T, O = tensorform(𝒫)\n",
        "    X = [collect(1:M.ℓ) for i in ℐ]\n",
        "    jointX, joint𝒜, joint𝒪 = joint(X), joint(𝒜), joint(𝒪)\n",
        "    x1 = jointX[1]\n",
        "    model = Model(Ipopt.Optimizer)\n",
        "    @variable(model, U[jointX,𝒮])\n",
        "    @variable(model, ψ[i=ℐ,X[i],𝒜[i]] ≥ 0)\n",
        "    @variable(model, η[i=ℐ,X[i],𝒜[i],𝒪[i],X[i]] ≥ 0)\n",
        "    @objective(model, Max, b⋅U[x1,:])\n",
        "    @NLconstraint(model, [x=jointX,s=𝒮],\n",
        "        U[x,s] == (sum(prod(ψ[i,x[i],a[i]] for i in ℐ)\n",
        "                   *(R[s,y] + γ*sum(T[s,y,s′]*sum(O[y,s′,z]\n",
        "                       *sum(prod(η[i,x[i],a[i],o[i],x′[i]] for i in ℐ)\n",
        "                               *U[x′,s′] for x′ in jointX)\n",
        "                       for (z, o) in enumerate(joint𝒪)) for s′ in 𝒮))\n",
        "                   for (y, a) in enumerate(joint𝒜))))\n",
        "    @constraint(model, [i=ℐ,xi=X[i]],\n",
        "                sum(ψ[i,xi,ai] for ai in 𝒜[i]) == 1)\n",
        "    @constraint(model, [i=ℐ,xi=X[i],ai=𝒜[i],oi=𝒪[i]],\n",
        "                sum(η[i,xi,ai,oi,xi′] for xi′ in X[i]) == 1)\n",
        "    optimize!(model)\n",
        "    ψ′, η′ = value.(ψ), value.(η)\n",
        "    return [ControllerPolicy(𝒫, X[i],\n",
        "            Dict((xi,𝒫.𝒜[i][ai]) => ψ′[i,xi,ai]\n",
        "                 for xi in X[i], ai in 𝒜[i]),\n",
        "            Dict((xi,𝒫.𝒜[i][ai],𝒫.𝒪[i][oi],xi′) => η′[i,xi,ai,oi,xi′]\n",
        "                 for xi in X[i], ai in 𝒜[i], oi in 𝒪[i], xi′ in X[i]))\n",
        "        for i in ℐ]\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 1\n",
        "struct Search\n",
        "    𝒮  # state space\n",
        "    𝒜  # valid action function\n",
        "    T  # transition function\n",
        "    R  # reward function\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 2\n",
        "function forward_search(𝒫::Search, s, d, U)\n",
        "\t𝒜, T, R = 𝒫.𝒜(s), 𝒫.T, 𝒫.R\n",
        "\tif isempty(𝒜) || d ≤ 0\n",
        "\t\treturn (a=nothing, u=U(s))\n",
        "\tend\n",
        "\tbest = (a=nothing, u=-Inf)\n",
        "\tfor a in 𝒜\n",
        "\t\ts′ = T(s,a)\n",
        "\t\tu = R(s,a) + forward_search(𝒫, s′, d-1, U).u\n",
        "\t\tif u > best.u\n",
        "\t\t\tbest = (a=a, u=u)\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 3\n",
        "function branch_and_bound(𝒫::Search, s, d, Ulo, Qhi)\n",
        "\t𝒜, T, R = 𝒫.𝒜(s), 𝒫.T, 𝒫.R\n",
        "\tif isempty(𝒜) || d ≤ 0\n",
        "\t\treturn (a=nothing, u=Ulo(s))\n",
        "\tend\n",
        "\tbest = (a=nothing, u=-Inf)\n",
        "\tfor a in sort(𝒜, by=a->Qhi(s,a), rev=true)\n",
        "\t\tif Qhi(s,a) ≤ best.u\n",
        "\t\t\treturn best # safe to prune\n",
        "\t\tend\n",
        "\t\tu = R(s,a) + branch_and_bound(𝒫,T(s,a),d-1,Ulo,Qhi).u\n",
        "\t\tif u > best.u\n",
        "\t\t\tbest = (a=a, u=u)\n",
        "\t\tend\n",
        "\tend\n",
        "\treturn best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 4\n",
        "function dynamic_programming(𝒫::Search, s, d, U, M=Dict())\n",
        "\tif haskey(M, (d,s))\n",
        "\t\treturn M[(d,s)]\n",
        "\tend\n",
        "\t𝒜, T, R = 𝒫.𝒜(s), 𝒫.T, 𝒫.R\n",
        "\tif isempty(𝒜) || d ≤ 0\n",
        "\t\tbest = (a=nothing, u=U(s))\n",
        "\telse\n",
        "\t\tbest = (a=first(𝒜), u=-Inf)\n",
        "\t\tfor a in 𝒜\n",
        "\t\t\ts′ = T(s,a)\n",
        "\t\t\tu = R(s,a) + dynamic_programming(𝒫, s′, d-1, U, M).u\n",
        "\t\t\tif u > best.u\n",
        "\t\t\t\tbest = (a=a, u=u)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\tM[(d,s)] = best\n",
        "\treturn best\n",
        "end\n",
        "####################\n",
        "\n",
        "#################### search 5\n",
        "function heuristic_search(𝒫::Search, s, d, Uhi, U, M)\n",
        "\tif haskey(M, (d,s))\n",
        "\t\treturn M[(d,s)]\n",
        "\tend\n",
        "\t𝒜, T, R = 𝒫.𝒜(s), 𝒫.T, 𝒫.R\n",
        "\tif isempty(𝒜) || d ≤ 0\n",
        "\t\tbest = (a=nothing, u=U(s))\n",
        "\telse\n",
        "\t\tbest = (a=first(𝒜), u=-Inf)\n",
        "\t\tfor a in sort(𝒜, by=a->R(s,a) + Uhi(T(s,a)), rev=true)\n",
        "\t\t\tif R(s,a) + Uhi(T(s,a)) ≤ best.u\n",
        "\t\t\t\tbreak\n",
        "\t\t\tend\n",
        "\t\t\ts′ = T(s,a)\n",
        "\t\t\tu = R(s,a) + heuristic_search(𝒫, s′, d-1, Uhi, U, M).u\n",
        "\t\t\tif u > best.u\n",
        "\t\t\t\tbest = (a=a, u=u)\n",
        "\t\t\tend\n",
        "\t\tend\n",
        "\tend\n",
        "\tM[(d,s)] = best\n",
        "\treturn best\n",
        "end\n",
        "####################"
      ]
    }
  ]
}